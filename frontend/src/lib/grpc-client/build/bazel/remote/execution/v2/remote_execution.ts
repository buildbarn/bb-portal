// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.6.1
//   protoc               v3.19.1
// source: build/bazel/remote/execution/v2/remote_execution.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../../../google/longrunning/operations";
import { Any } from "../../../../../google/protobuf/any";
import { Duration } from "../../../../../google/protobuf/duration";
import { Timestamp } from "../../../../../google/protobuf/timestamp";
import { UInt32Value } from "../../../../../google/protobuf/wrappers";
import { Status } from "../../../../../google/rpc/status";
import { SemVer } from "../../../semver/semver";

export const protobufPackage = "build.bazel.remote.execution.v2";

/**
 * An `Action` captures all the information about an execution which is required
 * to reproduce it.
 *
 * `Action`s are the core component of the [Execution] service. A single
 * `Action` represents a repeatable action that can be performed by the
 * execution service. `Action`s can be succinctly identified by the digest of
 * their wire format encoding and, once an `Action` has been executed, will be
 * cached in the action cache. Future requests can then use the cached result
 * rather than needing to run afresh.
 *
 * When a server completes execution of an
 * [Action][build.bazel.remote.execution.v2.Action], it MAY choose to
 * cache the [result][build.bazel.remote.execution.v2.ActionResult] in
 * the [ActionCache][build.bazel.remote.execution.v2.ActionCache] unless
 * `do_not_cache` is `true`. Clients SHOULD expect the server to do so. By
 * default, future calls to
 * [Execute][build.bazel.remote.execution.v2.Execution.Execute] the same
 * `Action` will also serve their results from the cache. Clients must take care
 * to understand the caching behaviour. Ideally, all `Action`s will be
 * reproducible so that serving a result from cache is always desirable and
 * correct.
 */
export interface Action {
  /**
   * The digest of the [Command][build.bazel.remote.execution.v2.Command]
   * to run, which MUST be present in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
   */
  commandDigest:
    | Digest
    | undefined;
  /**
   * The digest of the root
   * [Directory][build.bazel.remote.execution.v2.Directory] for the input
   * files. The files in the directory tree are available in the correct
   * location on the build machine before the command is executed. The root
   * directory, as well as every subdirectory and content blob referred to, MUST
   * be in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
   */
  inputRootDigest:
    | Digest
    | undefined;
  /**
   * A timeout after which the execution should be killed. If the timeout is
   * absent, then the client is specifying that the execution should continue
   * as long as the server will let it. The server SHOULD impose a timeout if
   * the client does not specify one, however, if the client does specify a
   * timeout that is longer than the server's maximum timeout, the server MUST
   * reject the request.
   *
   * The timeout is only intended to cover the "execution" of the specified
   * action and not time in queue nor any overheads before or after execution
   * such as marshalling inputs/outputs. The server SHOULD avoid including time
   * spent the client doesn't have control over, and MAY extend or reduce the
   * timeout to account for delays or speedups that occur during execution
   * itself (e.g., lazily loading data from the Content Addressable Storage,
   * live migration of virtual machines, emulation overhead).
   *
   * The timeout is a part of the
   * [Action][build.bazel.remote.execution.v2.Action] message, and
   * therefore two `Actions` with different timeouts are different, even if they
   * are otherwise identical. This is because, if they were not, running an
   * `Action` with a lower timeout than is required might result in a cache hit
   * from an execution run with a longer timeout, hiding the fact that the
   * timeout is too short. By encoding it directly in the `Action`, a lower
   * timeout will result in a cache miss and the execution timeout will fail
   * immediately, rather than whenever the cache entry gets evicted.
   */
  timeout:
    | Duration
    | undefined;
  /**
   * If true, then the `Action`'s result cannot be cached, and in-flight
   * requests for the same `Action` may not be merged.
   */
  doNotCache: boolean;
  /**
   * An optional additional salt value used to place this `Action` into a
   * separate cache namespace from other instances having the same field
   * contents. This salt typically comes from operational configuration
   * specific to sources such as repo and service configuration,
   * and allows disowning an entire set of ActionResults that might have been
   * poisoned by buggy software or tool failures.
   */
  salt: Uint8Array;
  /**
   * The optional platform requirements for the execution environment. The
   * server MAY choose to execute the action on any worker satisfying the
   * requirements, so the client SHOULD ensure that running the action on any
   * such worker will have the same result.  A detailed lexicon for this can be
   * found in the accompanying platform.md.
   * New in version 2.2: clients SHOULD set these platform properties as well
   * as those in the [Command][build.bazel.remote.execution.v2.Command]. Servers
   * SHOULD prefer those set here.
   */
  platform: Platform | undefined;
}

/**
 * A `Command` is the actual command executed by a worker running an
 * [Action][build.bazel.remote.execution.v2.Action] and specifications of its
 * environment.
 *
 * Except as otherwise required, the environment (such as which system
 * libraries or binaries are available, and what filesystems are mounted where)
 * is defined by and specific to the implementation of the remote execution API.
 */
export interface Command {
  /**
   * The arguments to the command.
   *
   * The first argument specifies the command to run, which may be either an
   * absolute path, a path relative to the working directory, or an unqualified
   * path (without path separators) which will be resolved using the operating
   * system's equivalent of the PATH environment variable. Path separators
   * native to the operating system running on the worker SHOULD be used. If the
   * `environment_variables` list contains an entry for the PATH environment
   * variable, it SHOULD be respected. If not, the resolution process is
   * implementation-defined.
   *
   * Changed in v2.3. v2.2 and older require that no PATH lookups are performed,
   * and that relative paths are resolved relative to the input root. This
   * behavior can, however, not be relied upon, as most implementations already
   * followed the rules described above.
   */
  arguments: string[];
  /**
   * The environment variables to set when running the program. The worker may
   * provide its own default environment variables; these defaults can be
   * overridden using this field. Additional variables can also be specified.
   *
   * In order to ensure that equivalent
   * [Command][build.bazel.remote.execution.v2.Command]s always hash to the same
   * value, the environment variables MUST be lexicographically sorted by name.
   * Sorting of strings is done by code point, equivalently, by the UTF-8 bytes.
   */
  environmentVariables: Command_EnvironmentVariable[];
  /**
   * A list of the output files that the client expects to retrieve from the
   * action. Only the listed files, as well as directories listed in
   * `output_directories`, will be returned to the client as output.
   * Other files or directories that may be created during command execution
   * are discarded.
   *
   * The paths are relative to the working directory of the action execution.
   * The paths are specified using a single forward slash (`/`) as a path
   * separator, even if the execution platform natively uses a different
   * separator. The path MUST NOT include a trailing slash, nor a leading slash,
   * being a relative path.
   *
   * In order to ensure consistent hashing of the same Action, the output paths
   * MUST be sorted lexicographically by code point (or, equivalently, by UTF-8
   * bytes).
   *
   * An output file cannot be duplicated, be a parent of another output file, or
   * have the same path as any of the listed output directories.
   *
   * Directories leading up to the output files are created by the worker prior
   * to execution, even if they are not explicitly part of the input root.
   *
   * DEPRECATED since v2.1: Use `output_paths` instead.
   *
   * @deprecated
   */
  outputFiles: string[];
  /**
   * A list of the output directories that the client expects to retrieve from
   * the action. Only the listed directories will be returned (an entire
   * directory structure will be returned as a
   * [Tree][build.bazel.remote.execution.v2.Tree] message digest, see
   * [OutputDirectory][build.bazel.remote.execution.v2.OutputDirectory]), as
   * well as files listed in `output_files`. Other files or directories that
   * may be created during command execution are discarded.
   *
   * The paths are relative to the working directory of the action execution.
   * The paths are specified using a single forward slash (`/`) as a path
   * separator, even if the execution platform natively uses a different
   * separator. The path MUST NOT include a trailing slash, nor a leading slash,
   * being a relative path. The special value of empty string is allowed,
   * although not recommended, and can be used to capture the entire working
   * directory tree, including inputs.
   *
   * In order to ensure consistent hashing of the same Action, the output paths
   * MUST be sorted lexicographically by code point (or, equivalently, by UTF-8
   * bytes).
   *
   * An output directory cannot be duplicated or have the same path as any of
   * the listed output files. An output directory is allowed to be a parent of
   * another output directory.
   *
   * Directories leading up to the output directories (but not the output
   * directories themselves) are created by the worker prior to execution, even
   * if they are not explicitly part of the input root.
   *
   * DEPRECATED since 2.1: Use `output_paths` instead.
   *
   * @deprecated
   */
  outputDirectories: string[];
  /**
   * A list of the output paths that the client expects to retrieve from the
   * action. Only the listed paths will be returned to the client as output.
   * The type of the output (file or directory) is not specified, and will be
   * determined by the server after action execution. If the resulting path is
   * a file, it will be returned in an
   * [OutputFile][build.bazel.remote.execution.v2.OutputFile] typed field.
   * If the path is a directory, the entire directory structure will be returned
   * as a [Tree][build.bazel.remote.execution.v2.Tree] message digest, see
   * [OutputDirectory][build.bazel.remote.execution.v2.OutputDirectory]
   * Other files or directories that may be created during command execution
   * are discarded.
   *
   * The paths are relative to the working directory of the action execution.
   * The paths are specified using a single forward slash (`/`) as a path
   * separator, even if the execution platform natively uses a different
   * separator. The path MUST NOT include a trailing slash, nor a leading slash,
   * being a relative path.
   *
   * In order to ensure consistent hashing of the same Action, the output paths
   * MUST be deduplicated and sorted lexicographically by code point (or,
   * equivalently, by UTF-8 bytes).
   *
   * Directories leading up to the output paths are created by the worker prior
   * to execution, even if they are not explicitly part of the input root.
   *
   * New in v2.1: this field supersedes the DEPRECATED `output_files` and
   * `output_directories` fields. If `output_paths` is used, `output_files` and
   * `output_directories` will be ignored!
   */
  outputPaths: string[];
  /**
   * The platform requirements for the execution environment. The server MAY
   * choose to execute the action on any worker satisfying the requirements, so
   * the client SHOULD ensure that running the action on any such worker will
   * have the same result.  A detailed lexicon for this can be found in the
   * accompanying platform.md.
   * DEPRECATED as of v2.2: platform properties are now specified directly in
   * the action. See documentation note in the
   * [Action][build.bazel.remote.execution.v2.Action] for migration.
   *
   * @deprecated
   */
  platform:
    | Platform
    | undefined;
  /**
   * The working directory, relative to the input root, for the command to run
   * in. It must be a directory which exists in the input tree. If it is left
   * empty, then the action is run in the input root.
   */
  workingDirectory: string;
  /**
   * A list of keys for node properties the client expects to retrieve for
   * output files and directories. Keys are either names of string-based
   * [NodeProperty][build.bazel.remote.execution.v2.NodeProperty] or
   * names of fields in [NodeProperties][build.bazel.remote.execution.v2.NodeProperties].
   * In order to ensure that equivalent `Action`s always hash to the same
   * value, the node properties MUST be lexicographically sorted by name.
   * Sorting of strings is done by code point, equivalently, by the UTF-8 bytes.
   *
   * The interpretation of string-based properties is server-dependent. If a
   * property is not recognized by the server, the server will return an
   * `INVALID_ARGUMENT`.
   */
  outputNodeProperties: string[];
  /**
   * The format that the worker should use to store the contents of
   * output directories.
   *
   * In case this field is set to a value that is not supported by the
   * worker, the worker SHOULD interpret this field as TREE_ONLY. The
   * worker MAY store output directories in formats that are a superset
   * of what was requested (e.g., interpreting DIRECTORY_ONLY as
   * TREE_AND_DIRECTORY).
   */
  outputDirectoryFormat: Command_OutputDirectoryFormat;
}

export enum Command_OutputDirectoryFormat {
  /**
   * TREE_ONLY - The client is only interested in receiving output directories in
   * the form of a single Tree object, using the `tree_digest` field.
   */
  TREE_ONLY = 0,
  /**
   * DIRECTORY_ONLY - The client is only interested in receiving output directories in
   * the form of a hierarchy of separately stored Directory objects,
   * using the `root_directory_digest` field.
   */
  DIRECTORY_ONLY = 1,
  /**
   * TREE_AND_DIRECTORY - The client is interested in receiving output directories both in
   * the form of a single Tree object and a hierarchy of separately
   * stored Directory objects, using both the `tree_digest` and
   * `root_directory_digest` fields.
   */
  TREE_AND_DIRECTORY = 2,
  UNRECOGNIZED = -1,
}

export function command_OutputDirectoryFormatFromJSON(object: any): Command_OutputDirectoryFormat {
  switch (object) {
    case 0:
    case "TREE_ONLY":
      return Command_OutputDirectoryFormat.TREE_ONLY;
    case 1:
    case "DIRECTORY_ONLY":
      return Command_OutputDirectoryFormat.DIRECTORY_ONLY;
    case 2:
    case "TREE_AND_DIRECTORY":
      return Command_OutputDirectoryFormat.TREE_AND_DIRECTORY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Command_OutputDirectoryFormat.UNRECOGNIZED;
  }
}

export function command_OutputDirectoryFormatToJSON(object: Command_OutputDirectoryFormat): string {
  switch (object) {
    case Command_OutputDirectoryFormat.TREE_ONLY:
      return "TREE_ONLY";
    case Command_OutputDirectoryFormat.DIRECTORY_ONLY:
      return "DIRECTORY_ONLY";
    case Command_OutputDirectoryFormat.TREE_AND_DIRECTORY:
      return "TREE_AND_DIRECTORY";
    case Command_OutputDirectoryFormat.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * An `EnvironmentVariable` is one variable to set in the running program's
 * environment.
 */
export interface Command_EnvironmentVariable {
  /** The variable name. */
  name: string;
  /** The variable value. */
  value: string;
}

/**
 * A `Platform` is a set of requirements, such as hardware, operating system, or
 * compiler toolchain, for an
 * [Action][build.bazel.remote.execution.v2.Action]'s execution
 * environment. A `Platform` is represented as a series of key-value pairs
 * representing the properties that are required of the platform.
 */
export interface Platform {
  /**
   * The properties that make up this platform. In order to ensure that
   * equivalent `Platform`s always hash to the same value, the properties MUST
   * be lexicographically sorted by name, and then by value. Sorting of strings
   * is done by code point, equivalently, by the UTF-8 bytes.
   */
  properties: Platform_Property[];
}

/**
 * A single property for the environment. The server is responsible for
 * specifying the property `name`s that it accepts. If an unknown `name` is
 * provided in the requirements for an
 * [Action][build.bazel.remote.execution.v2.Action], the server SHOULD
 * reject the execution request. If permitted by the server, the same `name`
 * may occur multiple times.
 *
 * The server is also responsible for specifying the interpretation of
 * property `value`s. For instance, a property describing how much RAM must be
 * available may be interpreted as allowing a worker with 16GB to fulfill a
 * request for 8GB, while a property describing the OS environment on which
 * the action must be performed may require an exact match with the worker's
 * OS.
 *
 * The server MAY use the `value` of one or more properties to determine how
 * it sets up the execution environment, such as by making specific system
 * files available to the worker.
 *
 * Both names and values are typically case-sensitive. Note that the platform
 * is implicitly part of the action digest, so even tiny changes in the names
 * or values (like changing case) may result in different action cache
 * entries.
 */
export interface Platform_Property {
  /** The property name. */
  name: string;
  /** The property value. */
  value: string;
}

/**
 * A `Directory` represents a directory node in a file tree, containing zero or
 * more children [FileNodes][build.bazel.remote.execution.v2.FileNode],
 * [DirectoryNodes][build.bazel.remote.execution.v2.DirectoryNode] and
 * [SymlinkNodes][build.bazel.remote.execution.v2.SymlinkNode].
 * Each `Node` contains its name in the directory, either the digest of its
 * content (either a file blob or a `Directory` proto) or a symlink target, as
 * well as possibly some metadata about the file or directory.
 *
 * In order to ensure that two equivalent directory trees hash to the same
 * value, the following restrictions MUST be obeyed when constructing a
 * a `Directory`:
 *
 * * Every child in the directory must have a path of exactly one segment.
 *   Multiple levels of directory hierarchy may not be collapsed.
 * * Each child in the directory must have a unique path segment (file name).
 *   Note that while the API itself is case-sensitive, the environment where
 *   the Action is executed may or may not be case-sensitive. That is, it is
 *   legal to call the API with a Directory that has both "Foo" and "foo" as
 *   children, but the Action may be rejected by the remote system upon
 *   execution.
 * * The files, directories and symlinks in the directory must each be sorted
 *   in lexicographical order by path. The path strings must be sorted by code
 *   point, equivalently, by UTF-8 bytes.
 * * The [NodeProperties][build.bazel.remote.execution.v2.NodeProperty] of files,
 *   directories, and symlinks must be sorted in lexicographical order by
 *   property name.
 *
 * A `Directory` that obeys the restrictions is said to be in canonical form.
 *
 * As an example, the following could be used for a file named `bar` and a
 * directory named `foo` with an executable file named `baz` (hashes shortened
 * for readability):
 *
 * ```json
 * // (Directory proto)
 * {
 *   files: [
 *     {
 *       name: "bar",
 *       digest: {
 *         hash: "4a73bc9d03...",
 *         size: 65534
 *       },
 *       node_properties: [
 *         {
 *           "name": "MTime",
 *           "value": "2017-01-15T01:30:15.01Z"
 *         }
 *       ]
 *     }
 *   ],
 *   directories: [
 *     {
 *       name: "foo",
 *       digest: {
 *         hash: "4cf2eda940...",
 *         size: 43
 *       }
 *     }
 *   ]
 * }
 *
 * // (Directory proto with hash "4cf2eda940..." and size 43)
 * {
 *   files: [
 *     {
 *       name: "baz",
 *       digest: {
 *         hash: "b2c941073e...",
 *         size: 1294,
 *       },
 *       is_executable: true
 *     }
 *   ]
 * }
 * ```
 */
export interface Directory {
  /** The files in the directory. */
  files: FileNode[];
  /** The subdirectories in the directory. */
  directories: DirectoryNode[];
  /** The symlinks in the directory. */
  symlinks: SymlinkNode[];
  nodeProperties: NodeProperties | undefined;
}

/**
 * A single property for [FileNodes][build.bazel.remote.execution.v2.FileNode],
 * [DirectoryNodes][build.bazel.remote.execution.v2.DirectoryNode], and
 * [SymlinkNodes][build.bazel.remote.execution.v2.SymlinkNode]. The server is
 * responsible for specifying the property `name`s that it accepts. If
 * permitted by the server, the same `name` may occur multiple times.
 */
export interface NodeProperty {
  /** The property name. */
  name: string;
  /** The property value. */
  value: string;
}

/**
 * Node properties for [FileNodes][build.bazel.remote.execution.v2.FileNode],
 * [DirectoryNodes][build.bazel.remote.execution.v2.DirectoryNode], and
 * [SymlinkNodes][build.bazel.remote.execution.v2.SymlinkNode]. The server is
 * responsible for specifying the properties that it accepts.
 */
export interface NodeProperties {
  /**
   * A list of string-based
   * [NodeProperties][build.bazel.remote.execution.v2.NodeProperty].
   */
  properties: NodeProperty[];
  /** The file's last modification timestamp. */
  mtime:
    | Date
    | undefined;
  /** The UNIX file mode, e.g., 0755. */
  unixMode: number | undefined;
}

/** A `FileNode` represents a single file and associated metadata. */
export interface FileNode {
  /** The name of the file. */
  name: string;
  /** The digest of the file's content. */
  digest:
    | Digest
    | undefined;
  /** True if file is executable, false otherwise. */
  isExecutable: boolean;
  nodeProperties: NodeProperties | undefined;
}

/**
 * A `DirectoryNode` represents a child of a
 * [Directory][build.bazel.remote.execution.v2.Directory] which is itself
 * a `Directory` and its associated metadata.
 */
export interface DirectoryNode {
  /** The name of the directory. */
  name: string;
  /**
   * The digest of the
   * [Directory][build.bazel.remote.execution.v2.Directory] object
   * represented. See [Digest][build.bazel.remote.execution.v2.Digest]
   * for information about how to take the digest of a proto message.
   */
  digest: Digest | undefined;
}

/** A `SymlinkNode` represents a symbolic link. */
export interface SymlinkNode {
  /** The name of the symlink. */
  name: string;
  /**
   * The target path of the symlink. The path separator is a forward slash `/`.
   * The target path can be relative to the parent directory of the symlink or
   * it can be an absolute path starting with `/`. Support for absolute paths
   * can be checked using the [Capabilities][build.bazel.remote.execution.v2.Capabilities]
   * API. `..` components are allowed anywhere in the target path as logical
   * canonicalization may lead to different behavior in the presence of
   * directory symlinks (e.g. `foo/../bar` may not be the same as `bar`).
   * To reduce potential cache misses, canonicalization is still recommended
   * where this is possible without impacting correctness.
   */
  target: string;
  nodeProperties: NodeProperties | undefined;
}

/**
 * A content digest. A digest for a given blob consists of the size of the blob
 * and its hash. The hash algorithm to use is defined by the server.
 *
 * The size is considered to be an integral part of the digest and cannot be
 * separated. That is, even if the `hash` field is correctly specified but
 * `size_bytes` is not, the server MUST reject the request.
 *
 * The reason for including the size in the digest is as follows: in a great
 * many cases, the server needs to know the size of the blob it is about to work
 * with prior to starting an operation with it, such as flattening Merkle tree
 * structures or streaming it to a worker. Technically, the server could
 * implement a separate metadata store, but this results in a significantly more
 * complicated implementation as opposed to having the client specify the size
 * up-front (or storing the size along with the digest in every message where
 * digests are embedded). This does mean that the API leaks some implementation
 * details of (what we consider to be) a reasonable server implementation, but
 * we consider this to be a worthwhile tradeoff.
 *
 * When a `Digest` is used to refer to a proto message, it always refers to the
 * message in binary encoded form. To ensure consistent hashing, clients and
 * servers MUST ensure that they serialize messages according to the following
 * rules, even if there are alternate valid encodings for the same message:
 *
 * * Fields are serialized in tag order.
 * * There are no unknown fields.
 * * There are no duplicate fields.
 * * Fields are serialized according to the default semantics for their type.
 *
 * Most protocol buffer implementations will always follow these rules when
 * serializing, but care should be taken to avoid shortcuts. For instance,
 * concatenating two messages to merge them may produce duplicate fields.
 */
export interface Digest {
  /**
   * The hash, represented as a lowercase hexadecimal string, padded with
   * leading zeroes up to the hash function length.
   */
  hash: string;
  /** The size of the blob, in bytes. */
  sizeBytes: string;
}

/** ExecutedActionMetadata contains details about a completed execution. */
export interface ExecutedActionMetadata {
  /** The name of the worker which ran the execution. */
  worker: string;
  /** When was the action added to the queue. */
  queuedTimestamp:
    | Date
    | undefined;
  /** When the worker received the action. */
  workerStartTimestamp:
    | Date
    | undefined;
  /** When the worker completed the action, including all stages. */
  workerCompletedTimestamp:
    | Date
    | undefined;
  /** When the worker started fetching action inputs. */
  inputFetchStartTimestamp:
    | Date
    | undefined;
  /** When the worker finished fetching action inputs. */
  inputFetchCompletedTimestamp:
    | Date
    | undefined;
  /** When the worker started executing the action command. */
  executionStartTimestamp:
    | Date
    | undefined;
  /** When the worker completed executing the action command. */
  executionCompletedTimestamp:
    | Date
    | undefined;
  /**
   * New in v2.3: the amount of time the worker spent executing the action
   * command, potentially computed using a worker-specific virtual clock.
   *
   * The virtual execution duration is only intended to cover the "execution" of
   * the specified action and not time in queue nor any overheads before or
   * after execution such as marshalling inputs/outputs. The server SHOULD avoid
   * including time spent the client doesn't have control over, and MAY extend
   * or reduce the execution duration to account for delays or speedups that
   * occur during execution itself (e.g., lazily loading data from the Content
   * Addressable Storage, live migration of virtual machines, emulation
   * overhead).
   *
   * The method of timekeeping used to compute the virtual execution duration
   * MUST be consistent with what is used to enforce the
   * [Action][[build.bazel.remote.execution.v2.Action]'s `timeout`. There is no
   * relationship between the virtual execution duration and the values of
   * `execution_start_timestamp` and `execution_completed_timestamp`.
   */
  virtualExecutionDuration:
    | Duration
    | undefined;
  /** When the worker started uploading action outputs. */
  outputUploadStartTimestamp:
    | Date
    | undefined;
  /** When the worker finished uploading action outputs. */
  outputUploadCompletedTimestamp:
    | Date
    | undefined;
  /**
   * Details that are specific to the kind of worker used. For example,
   * on POSIX-like systems this could contain a message with
   * getrusage(2) statistics.
   */
  auxiliaryMetadata: Any[];
}

/**
 * An ActionResult represents the result of an
 * [Action][build.bazel.remote.execution.v2.Action] being run.
 *
 * It is advised that at least one field (for example
 * `ActionResult.execution_metadata.Worker`) have a non-default value, to
 * ensure that the serialized value is non-empty, which can then be used
 * as a basic data sanity check.
 */
export interface ActionResult {
  /**
   * The output files of the action. For each output file requested in the
   * `output_files` or `output_paths` field of the Action, if the corresponding
   * file existed after the action completed, a single entry will be present
   * either in this field, or the `output_file_symlinks` field if the file was
   * a symbolic link to another file (`output_symlinks` field after v2.1).
   *
   * If an output listed in `output_files` was found, but was a directory rather
   * than a regular file, the server will return a FAILED_PRECONDITION.
   * If the action does not produce the requested output, then that output
   * will be omitted from the list. The server is free to arrange the output
   * list as desired; clients MUST NOT assume that the output list is sorted.
   */
  outputFiles: OutputFile[];
  /**
   * The output files of the action that are symbolic links to other files. Those
   * may be links to other output files, or input files, or even absolute paths
   * outside of the working directory, if the server supports
   * [SymlinkAbsolutePathStrategy.ALLOWED][build.bazel.remote.execution.v2.CacheCapabilities.SymlinkAbsolutePathStrategy].
   * For each output file requested in the `output_files` or `output_paths`
   * field of the Action, if the corresponding file existed after
   * the action completed, a single entry will be present either in this field,
   * or in the `output_files` field, if the file was not a symbolic link.
   *
   * If an output symbolic link of the same name as listed in `output_files` of
   * the Command was found, but its target type was not a regular file, the
   * server will return a FAILED_PRECONDITION.
   * If the action does not produce the requested output, then that output
   * will be omitted from the list. The server is free to arrange the output
   * list as desired; clients MUST NOT assume that the output list is sorted.
   *
   * DEPRECATED as of v2.1. Servers that wish to be compatible with v2.0 API
   * should still populate this field in addition to `output_symlinks`.
   *
   * @deprecated
   */
  outputFileSymlinks: OutputSymlink[];
  /**
   * New in v2.1: this field will only be populated if the command
   * `output_paths` field was used, and not the pre v2.1 `output_files` or
   * `output_directories` fields.
   * The output paths of the action that are symbolic links to other paths. Those
   * may be links to other outputs, or inputs, or even absolute paths
   * outside of the working directory, if the server supports
   * [SymlinkAbsolutePathStrategy.ALLOWED][build.bazel.remote.execution.v2.CacheCapabilities.SymlinkAbsolutePathStrategy].
   * A single entry for each output requested in `output_paths`
   * field of the Action, if the corresponding path existed after
   * the action completed and was a symbolic link.
   *
   * If the action does not produce a requested output, then that output
   * will be omitted from the list. The server is free to arrange the output
   * list as desired; clients MUST NOT assume that the output list is sorted.
   */
  outputSymlinks: OutputSymlink[];
  /**
   * The output directories of the action. For each output directory requested
   * in the `output_directories` or `output_paths` field of the Action, if the
   * corresponding directory existed after the action completed, a single entry
   * will be present in the output list, which will contain the digest of a
   * [Tree][build.bazel.remote.execution.v2.Tree] message containing the
   * directory tree, and the path equal exactly to the corresponding Action
   * output_directories member.
   *
   * As an example, suppose the Action had an output directory `a/b/dir` and the
   * execution produced the following contents in `a/b/dir`: a file named `bar`
   * and a directory named `foo` with an executable file named `baz`. Then,
   * output_directory will contain (hashes shortened for readability):
   *
   * ```json
   * // OutputDirectory proto:
   * {
   *   path: "a/b/dir"
   *   tree_digest: {
   *     hash: "4a73bc9d03...",
   *     size: 55
   *   }
   * }
   * // Tree proto with hash "4a73bc9d03..." and size 55:
   * {
   *   root: {
   *     files: [
   *       {
   *         name: "bar",
   *         digest: {
   *           hash: "4a73bc9d03...",
   *           size: 65534
   *         }
   *       }
   *     ],
   *     directories: [
   *       {
   *         name: "foo",
   *         digest: {
   *           hash: "4cf2eda940...",
   *           size: 43
   *         }
   *       }
   *     ]
   *   }
   *   children : {
   *     // (Directory proto with hash "4cf2eda940..." and size 43)
   *     files: [
   *       {
   *         name: "baz",
   *         digest: {
   *           hash: "b2c941073e...",
   *           size: 1294,
   *         },
   *         is_executable: true
   *       }
   *     ]
   *   }
   * }
   * ```
   * If an output of the same name as listed in `output_files` of
   * the Command was found in `output_directories`, but was not a directory, the
   * server will return a FAILED_PRECONDITION.
   */
  outputDirectories: OutputDirectory[];
  /**
   * The output directories of the action that are symbolic links to other
   * directories. Those may be links to other output directories, or input
   * directories, or even absolute paths outside of the working directory,
   * if the server supports
   * [SymlinkAbsolutePathStrategy.ALLOWED][build.bazel.remote.execution.v2.CacheCapabilities.SymlinkAbsolutePathStrategy].
   * For each output directory requested in the `output_directories` field of
   * the Action, if the directory existed after the action completed, a
   * single entry will be present either in this field, or in the
   * `output_directories` field, if the directory was not a symbolic link.
   *
   * If an output of the same name was found, but was a symbolic link to a file
   * instead of a directory, the server will return a FAILED_PRECONDITION.
   * If the action does not produce the requested output, then that output
   * will be omitted from the list. The server is free to arrange the output
   * list as desired; clients MUST NOT assume that the output list is sorted.
   *
   * DEPRECATED as of v2.1. Servers that wish to be compatible with v2.0 API
   * should still populate this field in addition to `output_symlinks`.
   *
   * @deprecated
   */
  outputDirectorySymlinks: OutputSymlink[];
  /** The exit code of the command. */
  exitCode: number;
  /**
   * The standard output buffer of the action. The server SHOULD NOT inline
   * stdout unless requested by the client in the
   * [GetActionResultRequest][build.bazel.remote.execution.v2.GetActionResultRequest]
   * message. The server MAY omit inlining, even if requested, and MUST do so if inlining
   * would cause the response to exceed message size limits.
   * Clients SHOULD NOT populate this field when uploading to the cache.
   */
  stdoutRaw: Uint8Array;
  /**
   * The digest for a blob containing the standard output of the action, which
   * can be retrieved from the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
   */
  stdoutDigest:
    | Digest
    | undefined;
  /**
   * The standard error buffer of the action. The server SHOULD NOT inline
   * stderr unless requested by the client in the
   * [GetActionResultRequest][build.bazel.remote.execution.v2.GetActionResultRequest]
   * message. The server MAY omit inlining, even if requested, and MUST do so if inlining
   * would cause the response to exceed message size limits.
   * Clients SHOULD NOT populate this field when uploading to the cache.
   */
  stderrRaw: Uint8Array;
  /**
   * The digest for a blob containing the standard error of the action, which
   * can be retrieved from the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
   */
  stderrDigest:
    | Digest
    | undefined;
  /** The details of the execution that originally produced this result. */
  executionMetadata: ExecutedActionMetadata | undefined;
}

/**
 * An `OutputFile` is similar to a
 * [FileNode][build.bazel.remote.execution.v2.FileNode], but it is used as an
 * output in an `ActionResult`. It allows a full file path rather than
 * only a name.
 */
export interface OutputFile {
  /**
   * The full path of the file relative to the working directory, including the
   * filename. The path separator is a forward slash `/`. Since this is a
   * relative path, it MUST NOT begin with a leading forward slash.
   */
  path: string;
  /** The digest of the file's content. */
  digest:
    | Digest
    | undefined;
  /** True if file is executable, false otherwise. */
  isExecutable: boolean;
  /**
   * The contents of the file if inlining was requested. The server SHOULD NOT inline
   * file contents unless requested by the client in the
   * [GetActionResultRequest][build.bazel.remote.execution.v2.GetActionResultRequest]
   * message. The server MAY omit inlining, even if requested, and MUST do so if inlining
   * would cause the response to exceed message size limits.
   * Clients SHOULD NOT populate this field when uploading to the cache.
   */
  contents: Uint8Array;
  nodeProperties: NodeProperties | undefined;
}

/**
 * A `Tree` contains all the
 * [Directory][build.bazel.remote.execution.v2.Directory] protos in a
 * single directory Merkle tree, compressed into one message.
 */
export interface Tree {
  /** The root directory in the tree. */
  root:
    | Directory
    | undefined;
  /**
   * All the child directories: the directories referred to by the root and,
   * recursively, all its children. In order to reconstruct the directory tree,
   * the client must take the digests of each of the child directories and then
   * build up a tree starting from the `root`.
   * Servers SHOULD ensure that these are ordered consistently such that two
   * actions producing equivalent output directories on the same server
   * implementation also produce Tree messages with matching digests.
   */
  children: Directory[];
}

/**
 * An `OutputDirectory` is the output in an `ActionResult` corresponding to a
 * directory's full contents rather than a single file.
 */
export interface OutputDirectory {
  /**
   * The full path of the directory relative to the working directory. The path
   * separator is a forward slash `/`. Since this is a relative path, it MUST
   * NOT begin with a leading forward slash. The empty string value is allowed,
   * and it denotes the entire working directory.
   */
  path: string;
  /**
   * The digest of the encoded
   * [Tree][build.bazel.remote.execution.v2.Tree] proto containing the
   * directory's contents.
   */
  treeDigest:
    | Digest
    | undefined;
  /**
   * If set, consumers MAY make the following assumptions about the
   * directories contained in the the Tree, so that it may be
   * instantiated on a local file system by scanning through it
   * sequentially:
   *
   * - All directories with the same binary representation are stored
   *   exactly once.
   * - All directories, apart from the root directory, are referenced by
   *   at least one parent directory.
   * - Directories are stored in topological order, with parents being
   *   stored before the child. The root directory is thus the first to
   *   be stored.
   *
   * Additionally, the Tree MUST be encoded as a stream of records,
   * where each record has the following format:
   *
   * - A tag byte, having one of the following two values:
   *   - (1 << 3) | 2 == 0x0a: First record (the root directory).
   *   - (2 << 3) | 2 == 0x12: Any subsequent records (child directories).
   * - The size of the directory, encoded as a base 128 varint.
   * - The contents of the directory, encoded as a binary serialized
   *   Protobuf message.
   *
   * This encoding is a subset of the Protobuf wire format of the Tree
   * message. As it is only permitted to store data associated with
   * field numbers 1 and 2, the tag MUST be encoded as a single byte.
   * More details on the Protobuf wire format can be found here:
   * https://developers.google.com/protocol-buffers/docs/encoding
   *
   * It is recommended that implementations using this feature construct
   * Tree objects manually using the specification given above, as
   * opposed to using a Protobuf library to marshal a full Tree message.
   * As individual Directory messages already need to be marshaled to
   * compute their digests, constructing the Tree object manually avoids
   * redundant marshaling.
   */
  isTopologicallySorted: boolean;
  /**
   * The digest of the encoded
   * [Directory][build.bazel.remote.execution.v2.Directory] proto
   * containing the contents the directory's root.
   *
   * If both `tree_digest` and `root_directory_digest` are set, this
   * field MUST match the digest of the root directory contained in the
   * Tree message.
   */
  rootDirectoryDigest: Digest | undefined;
}

/**
 * An `OutputSymlink` is similar to a
 * [Symlink][build.bazel.remote.execution.v2.SymlinkNode], but it is used as an
 * output in an `ActionResult`.
 *
 * `OutputSymlink` is binary-compatible with `SymlinkNode`.
 */
export interface OutputSymlink {
  /**
   * The full path of the symlink relative to the working directory, including the
   * filename. The path separator is a forward slash `/`. Since this is a
   * relative path, it MUST NOT begin with a leading forward slash.
   */
  path: string;
  /**
   * The target path of the symlink. The path separator is a forward slash `/`.
   * The target path can be relative to the parent directory of the symlink or
   * it can be an absolute path starting with `/`. Support for absolute paths
   * can be checked using the [Capabilities][build.bazel.remote.execution.v2.Capabilities]
   * API. `..` components are allowed anywhere in the target path.
   */
  target: string;
  nodeProperties: NodeProperties | undefined;
}

/** An `ExecutionPolicy` can be used to control the scheduling of the action. */
export interface ExecutionPolicy {
  /**
   * The priority (relative importance) of this action. Generally, a lower value
   * means that the action should be run sooner than actions having a greater
   * priority value, but the interpretation of a given value is server-
   * dependent. A priority of 0 means the *default* priority. Priorities may be
   * positive or negative, and such actions should run later or sooner than
   * actions having the default priority, respectively. The particular semantics
   * of this field is up to the server. In particular, every server will have
   * their own supported range of priorities, and will decide how these map into
   * scheduling policy.
   */
  priority: number;
}

/**
 * A `ResultsCachePolicy` is used for fine-grained control over how action
 * outputs are stored in the CAS and Action Cache.
 */
export interface ResultsCachePolicy {
  /**
   * The priority (relative importance) of this content in the overall cache.
   * Generally, a lower value means a longer retention time or other advantage,
   * but the interpretation of a given value is server-dependent. A priority of
   * 0 means a *default* value, decided by the server.
   *
   * The particular semantics of this field is up to the server. In particular,
   * every server will have their own supported range of priorities, and will
   * decide how these map into retention/eviction policy.
   */
  priority: number;
}

/**
 * A request message for
 * [Execution.Execute][build.bazel.remote.execution.v2.Execution.Execute].
 */
export interface ExecuteRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
  /**
   * If true, the action will be executed even if its result is already
   * present in the [ActionCache][build.bazel.remote.execution.v2.ActionCache].
   * The execution is still allowed to be merged with other in-flight executions
   * of the same action, however - semantically, the service MUST only guarantee
   * that the results of an execution with this field set were not visible
   * before the corresponding execution request was sent.
   * Note that actions from execution requests setting this field set are still
   * eligible to be entered into the action cache upon completion, and services
   * SHOULD overwrite any existing entries that may exist. This allows
   * skip_cache_lookup requests to be used as a mechanism for replacing action
   * cache entries that reference outputs no longer available or that are
   * poisoned in any way.
   * If false, the result may be served from the action cache.
   */
  skipCacheLookup: boolean;
  /**
   * The digest of the [Action][build.bazel.remote.execution.v2.Action] to
   * execute.
   */
  actionDigest:
    | Digest
    | undefined;
  /**
   * An optional policy for execution of the action.
   * The server will have a default policy if this is not provided.
   */
  executionPolicy:
    | ExecutionPolicy
    | undefined;
  /**
   * An optional policy for the results of this execution in the remote cache.
   * The server will have a default policy if this is not provided.
   * This may be applied to both the ActionResult and the associated blobs.
   */
  resultsCachePolicy:
    | ResultsCachePolicy
    | undefined;
  /**
   * The digest function that was used to compute the action digest.
   *
   * If the digest function used is one of MD5, MURMUR3, SHA1, SHA256,
   * SHA384, SHA512, or VSO, the client MAY leave this field unset. In
   * that case the server SHOULD infer the digest function using the
   * length of the action digest hash and the digest functions announced
   * in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
  /**
   * A hint to the server to request inlining stdout in the
   * [ActionResult][build.bazel.remote.execution.v2.ActionResult] message.
   */
  inlineStdout: boolean;
  /**
   * A hint to the server to request inlining stderr in the
   * [ActionResult][build.bazel.remote.execution.v2.ActionResult] message.
   */
  inlineStderr: boolean;
  /**
   * A hint to the server to inline the contents of the listed output files.
   * Each path needs to exactly match one file path in either `output_paths` or
   * `output_files` (DEPRECATED since v2.1) in the
   * [Command][build.bazel.remote.execution.v2.Command] message.
   */
  inlineOutputFiles: string[];
}

/** A `LogFile` is a log stored in the CAS. */
export interface LogFile {
  /** The digest of the log contents. */
  digest:
    | Digest
    | undefined;
  /**
   * This is a hint as to the purpose of the log, and is set to true if the log
   * is human-readable text that can be usefully displayed to a user, and false
   * otherwise. For instance, if a command-line client wishes to print the
   * server logs to the terminal for a failed action, this allows it to avoid
   * displaying a binary file.
   */
  humanReadable: boolean;
}

/**
 * The response message for
 * [Execution.Execute][build.bazel.remote.execution.v2.Execution.Execute],
 * which will be contained in the [response
 * field][google.longrunning.Operation.response] of the
 * [Operation][google.longrunning.Operation].
 */
export interface ExecuteResponse {
  /** The result of the action. */
  result:
    | ActionResult
    | undefined;
  /** True if the result was served from cache, false if it was executed. */
  cachedResult: boolean;
  /**
   * If the status has a code other than `OK`, it indicates that the action did
   * not finish execution. For example, if the operation times out during
   * execution, the status will have a `DEADLINE_EXCEEDED` code. Servers MUST
   * use this field for errors in execution, rather than the error field on the
   * `Operation` object.
   *
   * If the status code is other than `OK`, then the result MUST NOT be cached.
   * For an error status, the `result` field is optional; the server may
   * populate the output-, stdout-, and stderr-related fields if it has any
   * information available, such as the stdout and stderr of a timed-out action.
   */
  status:
    | Status
    | undefined;
  /**
   * An optional list of additional log outputs the server wishes to provide. A
   * server can use this to return execution-specific logs however it wishes.
   * This is intended primarily to make it easier for users to debug issues that
   * may be outside of the actual job execution, such as by identifying the
   * worker executing the action or by providing logs from the worker's setup
   * phase. The keys SHOULD be human readable so that a client can display them
   * to a user.
   */
  serverLogs: { [key: string]: LogFile };
  /**
   * Freeform informational message with details on the execution of the action
   * that may be displayed to the user upon failure or when requested explicitly.
   */
  message: string;
}

export interface ExecuteResponse_ServerLogsEntry {
  key: string;
  value: LogFile | undefined;
}

/**
 * The current stage of action execution.
 *
 * Even though these stages are numbered according to the order in which
 * they generally occur, there is no requirement that the remote
 * execution system reports events along this order. For example, an
 * operation MAY transition from the EXECUTING stage back to QUEUED
 * in case the hardware on which the operation executes fails.
 *
 * If and only if the remote execution system reports that an operation
 * has reached the COMPLETED stage, it MUST set the [done
 * field][google.longrunning.Operation.done] of the
 * [Operation][google.longrunning.Operation] and terminate the stream.
 */
export interface ExecutionStage {
}

export enum ExecutionStage_Value {
  /** UNKNOWN - Invalid value. */
  UNKNOWN = 0,
  /** CACHE_CHECK - Checking the result against the cache. */
  CACHE_CHECK = 1,
  /** QUEUED - Currently idle, awaiting a free machine to execute. */
  QUEUED = 2,
  /** EXECUTING - Currently being executed by a worker. */
  EXECUTING = 3,
  /** COMPLETED - Finished execution. */
  COMPLETED = 4,
  UNRECOGNIZED = -1,
}

export function executionStage_ValueFromJSON(object: any): ExecutionStage_Value {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return ExecutionStage_Value.UNKNOWN;
    case 1:
    case "CACHE_CHECK":
      return ExecutionStage_Value.CACHE_CHECK;
    case 2:
    case "QUEUED":
      return ExecutionStage_Value.QUEUED;
    case 3:
    case "EXECUTING":
      return ExecutionStage_Value.EXECUTING;
    case 4:
    case "COMPLETED":
      return ExecutionStage_Value.COMPLETED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExecutionStage_Value.UNRECOGNIZED;
  }
}

export function executionStage_ValueToJSON(object: ExecutionStage_Value): string {
  switch (object) {
    case ExecutionStage_Value.UNKNOWN:
      return "UNKNOWN";
    case ExecutionStage_Value.CACHE_CHECK:
      return "CACHE_CHECK";
    case ExecutionStage_Value.QUEUED:
      return "QUEUED";
    case ExecutionStage_Value.EXECUTING:
      return "EXECUTING";
    case ExecutionStage_Value.COMPLETED:
      return "COMPLETED";
    case ExecutionStage_Value.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Metadata about an ongoing
 * [execution][build.bazel.remote.execution.v2.Execution.Execute], which
 * will be contained in the [metadata
 * field][google.longrunning.Operation.metadata] of the
 * [Operation][google.longrunning.Operation].
 */
export interface ExecuteOperationMetadata {
  /** The current stage of execution. */
  stage: ExecutionStage_Value;
  /**
   * The digest of the [Action][build.bazel.remote.execution.v2.Action]
   * being executed.
   */
  actionDigest:
    | Digest
    | undefined;
  /**
   * If set, the client can use this resource name with
   * [ByteStream.Read][google.bytestream.ByteStream.Read] to stream the
   * standard output from the endpoint hosting streamed responses.
   */
  stdoutStreamName: string;
  /**
   * If set, the client can use this resource name with
   * [ByteStream.Read][google.bytestream.ByteStream.Read] to stream the
   * standard error from the endpoint hosting streamed responses.
   */
  stderrStreamName: string;
  /**
   * The client can read this field to view details about the ongoing
   * execution.
   */
  partialExecutionMetadata:
    | ExecutedActionMetadata
    | undefined;
  /**
   * The digest function that was used to compute the action digest.
   *
   * If the digest function used is one of BLAKE3, MD5, MURMUR3, SHA1,
   * SHA256, SHA256TREE, SHA384, SHA512, or VSO, the server MAY leave
   * this field unset. In that case the client SHOULD infer the digest
   * function using the length of the action digest hash and the digest
   * functions announced in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
}

/**
 * A request message for
 * [WaitExecution][build.bazel.remote.execution.v2.Execution.WaitExecution].
 */
export interface WaitExecutionRequest {
  /**
   * The name of the [Operation][google.longrunning.Operation]
   * returned by [Execute][build.bazel.remote.execution.v2.Execution.Execute].
   */
  name: string;
}

/**
 * A request message for
 * [ActionCache.GetActionResult][build.bazel.remote.execution.v2.ActionCache.GetActionResult].
 */
export interface GetActionResultRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
  /**
   * The digest of the [Action][build.bazel.remote.execution.v2.Action]
   * whose result is requested.
   */
  actionDigest:
    | Digest
    | undefined;
  /**
   * A hint to the server to request inlining stdout in the
   * [ActionResult][build.bazel.remote.execution.v2.ActionResult] message.
   */
  inlineStdout: boolean;
  /**
   * A hint to the server to request inlining stderr in the
   * [ActionResult][build.bazel.remote.execution.v2.ActionResult] message.
   */
  inlineStderr: boolean;
  /**
   * A hint to the server to inline the contents of the listed output files.
   * Each path needs to exactly match one file path in either `output_paths` or
   * `output_files` (DEPRECATED since v2.1) in the
   * [Command][build.bazel.remote.execution.v2.Command] message.
   */
  inlineOutputFiles: string[];
  /**
   * The digest function that was used to compute the action digest.
   *
   * If the digest function used is one of MD5, MURMUR3, SHA1, SHA256,
   * SHA384, SHA512, or VSO, the client MAY leave this field unset. In
   * that case the server SHOULD infer the digest function using the
   * length of the action digest hash and the digest functions announced
   * in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
}

/**
 * A request message for
 * [ActionCache.UpdateActionResult][build.bazel.remote.execution.v2.ActionCache.UpdateActionResult].
 */
export interface UpdateActionResultRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
  /**
   * The digest of the [Action][build.bazel.remote.execution.v2.Action]
   * whose result is being uploaded.
   */
  actionDigest:
    | Digest
    | undefined;
  /**
   * The [ActionResult][build.bazel.remote.execution.v2.ActionResult]
   * to store in the cache.
   */
  actionResult:
    | ActionResult
    | undefined;
  /**
   * An optional policy for the results of this execution in the remote cache.
   * The server will have a default policy if this is not provided.
   * This may be applied to both the ActionResult and the associated blobs.
   */
  resultsCachePolicy:
    | ResultsCachePolicy
    | undefined;
  /**
   * The digest function that was used to compute the action digest.
   *
   * If the digest function used is one of MD5, MURMUR3, SHA1, SHA256,
   * SHA384, SHA512, or VSO, the client MAY leave this field unset. In
   * that case the server SHOULD infer the digest function using the
   * length of the action digest hash and the digest functions announced
   * in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
}

/**
 * A request message for
 * [ContentAddressableStorage.FindMissingBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.FindMissingBlobs].
 */
export interface FindMissingBlobsRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
  /**
   * A list of the blobs to check. All digests MUST use the same digest
   * function.
   */
  blobDigests: Digest[];
  /**
   * The digest function of the blobs whose existence is checked.
   *
   * If the digest function used is one of MD5, MURMUR3, SHA1, SHA256,
   * SHA384, SHA512, or VSO, the client MAY leave this field unset. In
   * that case the server SHOULD infer the digest function using the
   * length of the blob digest hashes and the digest functions announced
   * in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
}

/**
 * A response message for
 * [ContentAddressableStorage.FindMissingBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.FindMissingBlobs].
 */
export interface FindMissingBlobsResponse {
  /** A list of the blobs requested *not* present in the storage. */
  missingBlobDigests: Digest[];
}

/**
 * A request message for
 * [ContentAddressableStorage.BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs].
 */
export interface BatchUpdateBlobsRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
  /** The individual upload requests. */
  requests: BatchUpdateBlobsRequest_Request[];
  /**
   * The digest function that was used to compute the digests of the
   * blobs being uploaded.
   *
   * If the digest function used is one of MD5, MURMUR3, SHA1, SHA256,
   * SHA384, SHA512, or VSO, the client MAY leave this field unset. In
   * that case the server SHOULD infer the digest function using the
   * length of the blob digest hashes and the digest functions announced
   * in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
}

/** A request corresponding to a single blob that the client wants to upload. */
export interface BatchUpdateBlobsRequest_Request {
  /**
   * The digest of the blob. This MUST be the digest of `data`. All
   * digests MUST use the same digest function.
   */
  digest:
    | Digest
    | undefined;
  /** The raw binary data. */
  data: Uint8Array;
  /**
   * The format of `data`. Must be `IDENTITY`/unspecified, or one of the
   * compressors advertised by the
   * [CacheCapabilities.supported_batch_compressors][build.bazel.remote.execution.v2.CacheCapabilities.supported_batch_compressors]
   * field.
   */
  compressor: Compressor_Value;
}

/**
 * A response message for
 * [ContentAddressableStorage.BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs].
 */
export interface BatchUpdateBlobsResponse {
  /** The responses to the requests. */
  responses: BatchUpdateBlobsResponse_Response[];
}

/** A response corresponding to a single blob that the client tried to upload. */
export interface BatchUpdateBlobsResponse_Response {
  /** The blob digest to which this response corresponds. */
  digest:
    | Digest
    | undefined;
  /** The result of attempting to upload that blob. */
  status: Status | undefined;
}

/**
 * A request message for
 * [ContentAddressableStorage.BatchReadBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchReadBlobs].
 */
export interface BatchReadBlobsRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
  /**
   * The individual blob digests. All digests MUST use the same digest
   * function.
   */
  digests: Digest[];
  /**
   * A list of acceptable encodings for the returned inlined data, in no
   * particular order. `IDENTITY` is always allowed even if not specified here.
   */
  acceptableCompressors: Compressor_Value[];
  /**
   * The digest function of the blobs being requested.
   *
   * If the digest function used is one of MD5, MURMUR3, SHA1, SHA256,
   * SHA384, SHA512, or VSO, the client MAY leave this field unset. In
   * that case the server SHOULD infer the digest function using the
   * length of the blob digest hashes and the digest functions announced
   * in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
}

/**
 * A response message for
 * [ContentAddressableStorage.BatchReadBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchReadBlobs].
 */
export interface BatchReadBlobsResponse {
  /** The responses to the requests. */
  responses: BatchReadBlobsResponse_Response[];
}

/** A response corresponding to a single blob that the client tried to download. */
export interface BatchReadBlobsResponse_Response {
  /** The digest to which this response corresponds. */
  digest:
    | Digest
    | undefined;
  /** The raw binary data. */
  data: Uint8Array;
  /**
   * The format the data is encoded in. MUST be `IDENTITY`/unspecified,
   * or one of the acceptable compressors specified in the `BatchReadBlobsRequest`.
   */
  compressor: Compressor_Value;
  /** The result of attempting to download that blob. */
  status: Status | undefined;
}

/**
 * A request message for
 * [ContentAddressableStorage.GetTree][build.bazel.remote.execution.v2.ContentAddressableStorage.GetTree].
 */
export interface GetTreeRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
  /**
   * The digest of the root, which must be an encoded
   * [Directory][build.bazel.remote.execution.v2.Directory] message
   * stored in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
   */
  rootDigest:
    | Digest
    | undefined;
  /**
   * A maximum page size to request. If present, the server will request no more
   * than this many items. Regardless of whether a page size is specified, the
   * server may place its own limit on the number of items to be returned and
   * require the client to retrieve more items using a subsequent request.
   */
  pageSize: number;
  /**
   * A page token, which must be a value received in a previous
   * [GetTreeResponse][build.bazel.remote.execution.v2.GetTreeResponse].
   * If present, the server will use that token as an offset, returning only
   * that page and the ones that succeed it.
   */
  pageToken: string;
  /**
   * The digest function that was used to compute the digest of the root
   * directory.
   *
   * If the digest function used is one of MD5, MURMUR3, SHA1, SHA256,
   * SHA384, SHA512, or VSO, the client MAY leave this field unset. In
   * that case the server SHOULD infer the digest function using the
   * length of the root digest hash and the digest functions announced
   * in the server's capabilities.
   */
  digestFunction: DigestFunction_Value;
}

/**
 * A response message for
 * [ContentAddressableStorage.GetTree][build.bazel.remote.execution.v2.ContentAddressableStorage.GetTree].
 */
export interface GetTreeResponse {
  /** The directories descended from the requested root. */
  directories: Directory[];
  /**
   * If present, signifies that there are more results which the client can
   * retrieve by passing this as the page_token in a subsequent
   * [request][build.bazel.remote.execution.v2.GetTreeRequest].
   * If empty, signifies that this is the last page of results.
   */
  nextPageToken: string;
}

/**
 * A request message for
 * [Capabilities.GetCapabilities][build.bazel.remote.execution.v2.Capabilities.GetCapabilities].
 */
export interface GetCapabilitiesRequest {
  /**
   * The instance of the execution system to operate against. A server may
   * support multiple instances of the execution system (with their own workers,
   * storage, caches, etc.). The server MAY require use of this field to select
   * between them in an implementation-defined fashion, otherwise it can be
   * omitted.
   */
  instanceName: string;
}

/**
 * A response message for
 * [Capabilities.GetCapabilities][build.bazel.remote.execution.v2.Capabilities.GetCapabilities].
 */
export interface ServerCapabilities {
  /** Capabilities of the remote cache system. */
  cacheCapabilities:
    | CacheCapabilities
    | undefined;
  /** Capabilities of the remote execution system. */
  executionCapabilities:
    | ExecutionCapabilities
    | undefined;
  /** Earliest RE API version supported, including deprecated versions. */
  deprecatedApiVersion:
    | SemVer
    | undefined;
  /** Earliest non-deprecated RE API version supported. */
  lowApiVersion:
    | SemVer
    | undefined;
  /** Latest RE API version supported. */
  highApiVersion: SemVer | undefined;
}

/**
 * The digest function used for converting values into keys for CAS and Action
 * Cache.
 */
export interface DigestFunction {
}

export enum DigestFunction_Value {
  /** UNKNOWN - It is an error for the server to return this value. */
  UNKNOWN = 0,
  /** SHA256 - The SHA-256 digest function. */
  SHA256 = 1,
  /** SHA1 - The SHA-1 digest function. */
  SHA1 = 2,
  /** MD5 - The MD5 digest function. */
  MD5 = 3,
  /**
   * VSO - The Microsoft "VSO-Hash" paged SHA256 digest function.
   * See https://github.com/microsoft/BuildXL/blob/master/Documentation/Specs/PagedHash.md .
   */
  VSO = 4,
  /** SHA384 - The SHA-384 digest function. */
  SHA384 = 5,
  /** SHA512 - The SHA-512 digest function. */
  SHA512 = 6,
  /**
   * MURMUR3 - Murmur3 128-bit digest function, x64 variant. Note that this is not a
   * cryptographic hash function and its collision properties are not strongly guaranteed.
   * See https://github.com/aappleby/smhasher/wiki/MurmurHash3 .
   */
  MURMUR3 = 7,
  /**
   * SHA256TREE - The SHA-256 digest function, modified to use a Merkle tree for
   * large objects. This permits implementations to store large blobs
   * as a decomposed sequence of 2^j sized chunks, where j >= 10,
   * while being able to validate integrity at the chunk level.
   *
   * Furthermore, on systems that do not offer dedicated instructions
   * for computing SHA-256 hashes (e.g., the Intel SHA and ARMv8
   * cryptographic extensions), SHA256TREE hashes can be computed more
   * efficiently than plain SHA-256 hashes by using generic SIMD
   * extensions, such as Intel AVX2 or ARM NEON.
   *
   * SHA256TREE hashes are computed as follows:
   *
   * - For blobs that are 1024 bytes or smaller, the hash is computed
   *   using the regular SHA-256 digest function.
   *
   * - For blobs that are more than 1024 bytes in size, the hash is
   *   computed as follows:
   *
   *   1. The blob is partitioned into a left (leading) and right
   *      (trailing) blob. These blobs have lengths m and n
   *      respectively, where m = 2^k and 0 < n <= m.
   *
   *   2. Hashes of the left and right blob, Hash(left) and
   *      Hash(right) respectively, are computed by recursively
   *      applying the SHA256TREE algorithm.
   *
   *   3. A single invocation is made to the SHA-256 block cipher with
   *      the following parameters:
   *
   *          M = Hash(left) || Hash(right)
   *          H = {
   *              0xcbbb9d5d, 0x629a292a, 0x9159015a, 0x152fecd8,
   *              0x67332667, 0x8eb44a87, 0xdb0c2e0d, 0x47b5481d,
   *          }
   *
   *      The values of H are the leading fractional parts of the
   *      square roots of the 9th to the 16th prime number (23 to 53).
   *      This differs from plain SHA-256, where the first eight prime
   *      numbers (2 to 19) are used, thereby preventing trivial hash
   *      collisions between small and large objects.
   *
   *   4. The hash of the full blob can then be obtained by
   *      concatenating the outputs of the block cipher:
   *
   *          Hash(blob) = a || b || c || d || e || f || g || h
   *
   *      Addition of the original values of H, as normally done
   *      through the use of the Davies-Meyer structure, is not
   *      performed. This isn't necessary, as the block cipher is only
   *      invoked once.
   *
   * Test vectors of this digest function can be found in the
   * accompanying sha256tree_test_vectors.txt file.
   */
  SHA256TREE = 8,
  /**
   * BLAKE3 - The BLAKE3 hash function.
   * See https://github.com/BLAKE3-team/BLAKE3.
   */
  BLAKE3 = 9,
  UNRECOGNIZED = -1,
}

export function digestFunction_ValueFromJSON(object: any): DigestFunction_Value {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return DigestFunction_Value.UNKNOWN;
    case 1:
    case "SHA256":
      return DigestFunction_Value.SHA256;
    case 2:
    case "SHA1":
      return DigestFunction_Value.SHA1;
    case 3:
    case "MD5":
      return DigestFunction_Value.MD5;
    case 4:
    case "VSO":
      return DigestFunction_Value.VSO;
    case 5:
    case "SHA384":
      return DigestFunction_Value.SHA384;
    case 6:
    case "SHA512":
      return DigestFunction_Value.SHA512;
    case 7:
    case "MURMUR3":
      return DigestFunction_Value.MURMUR3;
    case 8:
    case "SHA256TREE":
      return DigestFunction_Value.SHA256TREE;
    case 9:
    case "BLAKE3":
      return DigestFunction_Value.BLAKE3;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DigestFunction_Value.UNRECOGNIZED;
  }
}

export function digestFunction_ValueToJSON(object: DigestFunction_Value): string {
  switch (object) {
    case DigestFunction_Value.UNKNOWN:
      return "UNKNOWN";
    case DigestFunction_Value.SHA256:
      return "SHA256";
    case DigestFunction_Value.SHA1:
      return "SHA1";
    case DigestFunction_Value.MD5:
      return "MD5";
    case DigestFunction_Value.VSO:
      return "VSO";
    case DigestFunction_Value.SHA384:
      return "SHA384";
    case DigestFunction_Value.SHA512:
      return "SHA512";
    case DigestFunction_Value.MURMUR3:
      return "MURMUR3";
    case DigestFunction_Value.SHA256TREE:
      return "SHA256TREE";
    case DigestFunction_Value.BLAKE3:
      return "BLAKE3";
    case DigestFunction_Value.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Describes the server/instance capabilities for updating the action cache. */
export interface ActionCacheUpdateCapabilities {
  updateEnabled: boolean;
}

/**
 * Allowed values for priority in
 * [ResultsCachePolicy][build.bazel.remoteexecution.v2.ResultsCachePolicy] and
 * [ExecutionPolicy][build.bazel.remoteexecution.v2.ExecutionPolicy]
 * Used for querying both cache and execution valid priority ranges.
 */
export interface PriorityCapabilities {
  priorities: PriorityCapabilities_PriorityRange[];
}

/** Supported range of priorities, including boundaries. */
export interface PriorityCapabilities_PriorityRange {
  /**
   * The minimum numeric value for this priority range, which represents the
   * most urgent task or longest retained item.
   */
  minPriority: number;
  /**
   * The maximum numeric value for this priority range, which represents the
   * least urgent task or shortest retained item.
   */
  maxPriority: number;
}

/** Describes how the server treats absolute symlink targets. */
export interface SymlinkAbsolutePathStrategy {
}

export enum SymlinkAbsolutePathStrategy_Value {
  /** UNKNOWN - Invalid value. */
  UNKNOWN = 0,
  /**
   * DISALLOWED - Server will return an `INVALID_ARGUMENT` on input symlinks with absolute
   * targets.
   * If an action tries to create an output symlink with an absolute target, a
   * `FAILED_PRECONDITION` will be returned.
   */
  DISALLOWED = 1,
  /**
   * ALLOWED - Server will allow symlink targets to escape the input root tree, possibly
   * resulting in non-hermetic builds.
   */
  ALLOWED = 2,
  UNRECOGNIZED = -1,
}

export function symlinkAbsolutePathStrategy_ValueFromJSON(object: any): SymlinkAbsolutePathStrategy_Value {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return SymlinkAbsolutePathStrategy_Value.UNKNOWN;
    case 1:
    case "DISALLOWED":
      return SymlinkAbsolutePathStrategy_Value.DISALLOWED;
    case 2:
    case "ALLOWED":
      return SymlinkAbsolutePathStrategy_Value.ALLOWED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SymlinkAbsolutePathStrategy_Value.UNRECOGNIZED;
  }
}

export function symlinkAbsolutePathStrategy_ValueToJSON(object: SymlinkAbsolutePathStrategy_Value): string {
  switch (object) {
    case SymlinkAbsolutePathStrategy_Value.UNKNOWN:
      return "UNKNOWN";
    case SymlinkAbsolutePathStrategy_Value.DISALLOWED:
      return "DISALLOWED";
    case SymlinkAbsolutePathStrategy_Value.ALLOWED:
      return "ALLOWED";
    case SymlinkAbsolutePathStrategy_Value.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Compression formats which may be supported. */
export interface Compressor {
}

export enum Compressor_Value {
  /**
   * IDENTITY - No compression. Servers and clients MUST always support this, and do
   * not need to advertise it.
   */
  IDENTITY = 0,
  /** ZSTD - Zstandard compression. */
  ZSTD = 1,
  /**
   * DEFLATE - RFC 1951 Deflate. This format is identical to what is used by ZIP
   * files. Headers such as the one generated by gzip are not
   * included.
   *
   * It is advised to use algorithms such as Zstandard instead, as
   * those are faster and/or provide a better compression ratio.
   */
  DEFLATE = 2,
  /** BROTLI - Brotli compression. */
  BROTLI = 3,
  UNRECOGNIZED = -1,
}

export function compressor_ValueFromJSON(object: any): Compressor_Value {
  switch (object) {
    case 0:
    case "IDENTITY":
      return Compressor_Value.IDENTITY;
    case 1:
    case "ZSTD":
      return Compressor_Value.ZSTD;
    case 2:
    case "DEFLATE":
      return Compressor_Value.DEFLATE;
    case 3:
    case "BROTLI":
      return Compressor_Value.BROTLI;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Compressor_Value.UNRECOGNIZED;
  }
}

export function compressor_ValueToJSON(object: Compressor_Value): string {
  switch (object) {
    case Compressor_Value.IDENTITY:
      return "IDENTITY";
    case Compressor_Value.ZSTD:
      return "ZSTD";
    case Compressor_Value.DEFLATE:
      return "DEFLATE";
    case Compressor_Value.BROTLI:
      return "BROTLI";
    case Compressor_Value.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Capabilities of the remote cache system. */
export interface CacheCapabilities {
  /**
   * All the digest functions supported by the remote cache.
   * Remote cache may support multiple digest functions simultaneously.
   */
  digestFunctions: DigestFunction_Value[];
  /** Capabilities for updating the action cache. */
  actionCacheUpdateCapabilities:
    | ActionCacheUpdateCapabilities
    | undefined;
  /** Supported cache priority range for both CAS and ActionCache. */
  cachePriorityCapabilities:
    | PriorityCapabilities
    | undefined;
  /**
   * Maximum total size of blobs to be uploaded/downloaded using
   * batch methods. A value of 0 means no limit is set, although
   * in practice there will always be a message size limitation
   * of the protocol in use, e.g. GRPC.
   */
  maxBatchTotalSizeBytes: string;
  /** Whether absolute symlink targets are supported. */
  symlinkAbsolutePathStrategy: SymlinkAbsolutePathStrategy_Value;
  /**
   * Compressors supported by the "compressed-blobs" bytestream resources.
   * Servers MUST support identity/no-compression, even if it is not listed
   * here.
   *
   * Note that this does not imply which if any compressors are supported by
   * the server at the gRPC level.
   */
  supportedCompressors: Compressor_Value[];
  /**
   * Compressors supported for inlined data in
   * [BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs]
   * requests.
   */
  supportedBatchUpdateCompressors: Compressor_Value[];
  /**
   * The maximum blob size that the server will accept for CAS blob uploads.
   * - If it is 0, it means there is no limit set. A client may assume
   *   arbitrarily large blobs may be uploaded to and downloaded from the cache.
   * - If it is larger than 0, implementations SHOULD NOT attempt to upload
   *   blobs with size larger than the limit. Servers SHOULD reject blob
   *   uploads over the `max_cas_blob_size_bytes` limit with response code
   *   `INVALID_ARGUMENT`
   * - If the cache implementation returns a given limit, it MAY still serve
   *   blobs larger than this limit.
   */
  maxCasBlobSizeBytes: string;
}

/** Capabilities of the remote execution system. */
export interface ExecutionCapabilities {
  /**
   * Legacy field for indicating which digest function is supported by the
   * remote execution system. It MUST be set to a value other than UNKNOWN.
   * Implementations should consider the repeated digest_functions field
   * first, falling back to this singular field if digest_functions is unset.
   */
  digestFunction: DigestFunction_Value;
  /** Whether remote execution is enabled for the particular server/instance. */
  execEnabled: boolean;
  /** Supported execution priority range. */
  executionPriorityCapabilities:
    | PriorityCapabilities
    | undefined;
  /** Supported node properties. */
  supportedNodeProperties: string[];
  /**
   * All the digest functions supported by the remote execution system.
   * If this field is set, it MUST also contain digest_function.
   *
   * Even if the remote execution system announces support for multiple
   * digest functions, individual execution requests may only reference
   * CAS objects using a single digest function. For example, it is not
   * permitted to execute actions having both MD5 and SHA-256 hashed
   * files in their input root.
   *
   * The CAS objects referenced by action results generated by the
   * remote execution system MUST use the same digest function as the
   * one used to construct the action.
   */
  digestFunctions: DigestFunction_Value[];
}

/** Details for the tool used to call the API. */
export interface ToolDetails {
  /** Name of the tool, e.g. bazel. */
  toolName: string;
  /** Version of the tool used for the request, e.g. 5.0.3. */
  toolVersion: string;
}

/**
 * An optional Metadata to attach to any RPC request to tell the server about an
 * external context of the request. The server may use this for logging or other
 * purposes. To use it, the client attaches the header to the call using the
 * canonical proto serialization:
 *
 * * name: `build.bazel.remote.execution.v2.requestmetadata-bin`
 * * contents: the base64 encoded binary `RequestMetadata` message.
 * Note: the gRPC library serializes binary headers encoded in base64 by
 * default (https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests).
 * Therefore, if the gRPC library is used to pass/retrieve this
 * metadata, the user may ignore the base64 encoding and assume it is simply
 * serialized as a binary message.
 */
export interface RequestMetadata {
  /** The details for the tool invoking the requests. */
  toolDetails:
    | ToolDetails
    | undefined;
  /**
   * An identifier that ties multiple requests to the same action.
   * For example, multiple requests to the CAS, Action Cache, and Execution
   * API are used in order to compile foo.cc.
   */
  actionId: string;
  /**
   * An identifier that ties multiple actions together to a final result.
   * For example, multiple actions are required to build and run foo_test.
   */
  toolInvocationId: string;
  /**
   * An identifier to tie multiple tool invocations together. For example,
   * runs of foo_test, bar_test and baz_test on a post-submit of a given patch.
   */
  correlatedInvocationsId: string;
  /**
   * A brief description of the kind of action, for example, CppCompile or GoLink.
   * There is no standard agreed set of values for this, and they are expected to vary between different client tools.
   */
  actionMnemonic: string;
  /**
   * An identifier for the target which produced this action.
   * No guarantees are made around how many actions may relate to a single target.
   */
  targetId: string;
  /**
   * An identifier for the configuration in which the target was built,
   * e.g. for differentiating building host tools or different target platforms.
   * There is no expectation that this value will have any particular structure,
   * or equality across invocations, though some client tools may offer these guarantees.
   */
  configurationId: string;
}

function createBaseAction(): Action {
  return {
    commandDigest: undefined,
    inputRootDigest: undefined,
    timeout: undefined,
    doNotCache: false,
    salt: new Uint8Array(0),
    platform: undefined,
  };
}

export const Action: MessageFns<Action> = {
  encode(message: Action, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commandDigest !== undefined) {
      Digest.encode(message.commandDigest, writer.uint32(10).fork()).join();
    }
    if (message.inputRootDigest !== undefined) {
      Digest.encode(message.inputRootDigest, writer.uint32(18).fork()).join();
    }
    if (message.timeout !== undefined) {
      Duration.encode(message.timeout, writer.uint32(50).fork()).join();
    }
    if (message.doNotCache !== false) {
      writer.uint32(56).bool(message.doNotCache);
    }
    if (message.salt.length !== 0) {
      writer.uint32(74).bytes(message.salt);
    }
    if (message.platform !== undefined) {
      Platform.encode(message.platform, writer.uint32(82).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Action {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAction();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.commandDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.inputRootDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.timeout = Duration.decode(reader, reader.uint32());
          continue;
        }
        case 7: {
          if (tag !== 56) {
            break;
          }

          message.doNotCache = reader.bool();
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.salt = reader.bytes();
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.platform = Platform.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Action {
    return {
      commandDigest: isSet(object.commandDigest) ? Digest.fromJSON(object.commandDigest) : undefined,
      inputRootDigest: isSet(object.inputRootDigest) ? Digest.fromJSON(object.inputRootDigest) : undefined,
      timeout: isSet(object.timeout) ? Duration.fromJSON(object.timeout) : undefined,
      doNotCache: isSet(object.doNotCache) ? globalThis.Boolean(object.doNotCache) : false,
      salt: isSet(object.salt) ? bytesFromBase64(object.salt) : new Uint8Array(0),
      platform: isSet(object.platform) ? Platform.fromJSON(object.platform) : undefined,
    };
  },

  toJSON(message: Action): unknown {
    const obj: any = {};
    if (message.commandDigest !== undefined) {
      obj.commandDigest = Digest.toJSON(message.commandDigest);
    }
    if (message.inputRootDigest !== undefined) {
      obj.inputRootDigest = Digest.toJSON(message.inputRootDigest);
    }
    if (message.timeout !== undefined) {
      obj.timeout = Duration.toJSON(message.timeout);
    }
    if (message.doNotCache !== false) {
      obj.doNotCache = message.doNotCache;
    }
    if (message.salt.length !== 0) {
      obj.salt = base64FromBytes(message.salt);
    }
    if (message.platform !== undefined) {
      obj.platform = Platform.toJSON(message.platform);
    }
    return obj;
  },

  create(base?: DeepPartial<Action>): Action {
    return Action.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Action>): Action {
    const message = createBaseAction();
    message.commandDigest = (object.commandDigest !== undefined && object.commandDigest !== null)
      ? Digest.fromPartial(object.commandDigest)
      : undefined;
    message.inputRootDigest = (object.inputRootDigest !== undefined && object.inputRootDigest !== null)
      ? Digest.fromPartial(object.inputRootDigest)
      : undefined;
    message.timeout = (object.timeout !== undefined && object.timeout !== null)
      ? Duration.fromPartial(object.timeout)
      : undefined;
    message.doNotCache = object.doNotCache ?? false;
    message.salt = object.salt ?? new Uint8Array(0);
    message.platform = (object.platform !== undefined && object.platform !== null)
      ? Platform.fromPartial(object.platform)
      : undefined;
    return message;
  },
};

function createBaseCommand(): Command {
  return {
    arguments: [],
    environmentVariables: [],
    outputFiles: [],
    outputDirectories: [],
    outputPaths: [],
    platform: undefined,
    workingDirectory: "",
    outputNodeProperties: [],
    outputDirectoryFormat: 0,
  };
}

export const Command: MessageFns<Command> = {
  encode(message: Command, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.arguments) {
      writer.uint32(10).string(v!);
    }
    for (const v of message.environmentVariables) {
      Command_EnvironmentVariable.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.outputFiles) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.outputDirectories) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.outputPaths) {
      writer.uint32(58).string(v!);
    }
    if (message.platform !== undefined) {
      Platform.encode(message.platform, writer.uint32(42).fork()).join();
    }
    if (message.workingDirectory !== "") {
      writer.uint32(50).string(message.workingDirectory);
    }
    for (const v of message.outputNodeProperties) {
      writer.uint32(66).string(v!);
    }
    if (message.outputDirectoryFormat !== 0) {
      writer.uint32(72).int32(message.outputDirectoryFormat);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Command {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCommand();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.arguments.push(reader.string());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.environmentVariables.push(Command_EnvironmentVariable.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.outputFiles.push(reader.string());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.outputDirectories.push(reader.string());
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.outputPaths.push(reader.string());
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.platform = Platform.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.workingDirectory = reader.string();
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.outputNodeProperties.push(reader.string());
          continue;
        }
        case 9: {
          if (tag !== 72) {
            break;
          }

          message.outputDirectoryFormat = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Command {
    return {
      arguments: globalThis.Array.isArray(object?.arguments)
        ? object.arguments.map((e: any) => globalThis.String(e))
        : [],
      environmentVariables: globalThis.Array.isArray(object?.environmentVariables)
        ? object.environmentVariables.map((e: any) => Command_EnvironmentVariable.fromJSON(e))
        : [],
      outputFiles: globalThis.Array.isArray(object?.outputFiles)
        ? object.outputFiles.map((e: any) => globalThis.String(e))
        : [],
      outputDirectories: globalThis.Array.isArray(object?.outputDirectories)
        ? object.outputDirectories.map((e: any) => globalThis.String(e))
        : [],
      outputPaths: globalThis.Array.isArray(object?.outputPaths)
        ? object.outputPaths.map((e: any) => globalThis.String(e))
        : [],
      platform: isSet(object.platform) ? Platform.fromJSON(object.platform) : undefined,
      workingDirectory: isSet(object.workingDirectory) ? globalThis.String(object.workingDirectory) : "",
      outputNodeProperties: globalThis.Array.isArray(object?.outputNodeProperties)
        ? object.outputNodeProperties.map((e: any) => globalThis.String(e))
        : [],
      outputDirectoryFormat: isSet(object.outputDirectoryFormat)
        ? command_OutputDirectoryFormatFromJSON(object.outputDirectoryFormat)
        : 0,
    };
  },

  toJSON(message: Command): unknown {
    const obj: any = {};
    if (message.arguments?.length) {
      obj.arguments = message.arguments;
    }
    if (message.environmentVariables?.length) {
      obj.environmentVariables = message.environmentVariables.map((e) => Command_EnvironmentVariable.toJSON(e));
    }
    if (message.outputFiles?.length) {
      obj.outputFiles = message.outputFiles;
    }
    if (message.outputDirectories?.length) {
      obj.outputDirectories = message.outputDirectories;
    }
    if (message.outputPaths?.length) {
      obj.outputPaths = message.outputPaths;
    }
    if (message.platform !== undefined) {
      obj.platform = Platform.toJSON(message.platform);
    }
    if (message.workingDirectory !== "") {
      obj.workingDirectory = message.workingDirectory;
    }
    if (message.outputNodeProperties?.length) {
      obj.outputNodeProperties = message.outputNodeProperties;
    }
    if (message.outputDirectoryFormat !== 0) {
      obj.outputDirectoryFormat = command_OutputDirectoryFormatToJSON(message.outputDirectoryFormat);
    }
    return obj;
  },

  create(base?: DeepPartial<Command>): Command {
    return Command.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Command>): Command {
    const message = createBaseCommand();
    message.arguments = object.arguments?.map((e) => e) || [];
    message.environmentVariables =
      object.environmentVariables?.map((e) => Command_EnvironmentVariable.fromPartial(e)) || [];
    message.outputFiles = object.outputFiles?.map((e) => e) || [];
    message.outputDirectories = object.outputDirectories?.map((e) => e) || [];
    message.outputPaths = object.outputPaths?.map((e) => e) || [];
    message.platform = (object.platform !== undefined && object.platform !== null)
      ? Platform.fromPartial(object.platform)
      : undefined;
    message.workingDirectory = object.workingDirectory ?? "";
    message.outputNodeProperties = object.outputNodeProperties?.map((e) => e) || [];
    message.outputDirectoryFormat = object.outputDirectoryFormat ?? 0;
    return message;
  },
};

function createBaseCommand_EnvironmentVariable(): Command_EnvironmentVariable {
  return { name: "", value: "" };
}

export const Command_EnvironmentVariable: MessageFns<Command_EnvironmentVariable> = {
  encode(message: Command_EnvironmentVariable, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Command_EnvironmentVariable {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCommand_EnvironmentVariable();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Command_EnvironmentVariable {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Command_EnvironmentVariable): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Command_EnvironmentVariable>): Command_EnvironmentVariable {
    return Command_EnvironmentVariable.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Command_EnvironmentVariable>): Command_EnvironmentVariable {
    const message = createBaseCommand_EnvironmentVariable();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePlatform(): Platform {
  return { properties: [] };
}

export const Platform: MessageFns<Platform> = {
  encode(message: Platform, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.properties) {
      Platform_Property.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Platform {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePlatform();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.properties.push(Platform_Property.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Platform {
    return {
      properties: globalThis.Array.isArray(object?.properties)
        ? object.properties.map((e: any) => Platform_Property.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Platform): unknown {
    const obj: any = {};
    if (message.properties?.length) {
      obj.properties = message.properties.map((e) => Platform_Property.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Platform>): Platform {
    return Platform.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Platform>): Platform {
    const message = createBasePlatform();
    message.properties = object.properties?.map((e) => Platform_Property.fromPartial(e)) || [];
    return message;
  },
};

function createBasePlatform_Property(): Platform_Property {
  return { name: "", value: "" };
}

export const Platform_Property: MessageFns<Platform_Property> = {
  encode(message: Platform_Property, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Platform_Property {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePlatform_Property();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Platform_Property {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Platform_Property): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Platform_Property>): Platform_Property {
    return Platform_Property.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Platform_Property>): Platform_Property {
    const message = createBasePlatform_Property();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDirectory(): Directory {
  return { files: [], directories: [], symlinks: [], nodeProperties: undefined };
}

export const Directory: MessageFns<Directory> = {
  encode(message: Directory, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.files) {
      FileNode.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.directories) {
      DirectoryNode.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.symlinks) {
      SymlinkNode.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.nodeProperties !== undefined) {
      NodeProperties.encode(message.nodeProperties, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Directory {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectory();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.files.push(FileNode.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.directories.push(DirectoryNode.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.symlinks.push(SymlinkNode.decode(reader, reader.uint32()));
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.nodeProperties = NodeProperties.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Directory {
    return {
      files: globalThis.Array.isArray(object?.files) ? object.files.map((e: any) => FileNode.fromJSON(e)) : [],
      directories: globalThis.Array.isArray(object?.directories)
        ? object.directories.map((e: any) => DirectoryNode.fromJSON(e))
        : [],
      symlinks: globalThis.Array.isArray(object?.symlinks)
        ? object.symlinks.map((e: any) => SymlinkNode.fromJSON(e))
        : [],
      nodeProperties: isSet(object.nodeProperties) ? NodeProperties.fromJSON(object.nodeProperties) : undefined,
    };
  },

  toJSON(message: Directory): unknown {
    const obj: any = {};
    if (message.files?.length) {
      obj.files = message.files.map((e) => FileNode.toJSON(e));
    }
    if (message.directories?.length) {
      obj.directories = message.directories.map((e) => DirectoryNode.toJSON(e));
    }
    if (message.symlinks?.length) {
      obj.symlinks = message.symlinks.map((e) => SymlinkNode.toJSON(e));
    }
    if (message.nodeProperties !== undefined) {
      obj.nodeProperties = NodeProperties.toJSON(message.nodeProperties);
    }
    return obj;
  },

  create(base?: DeepPartial<Directory>): Directory {
    return Directory.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Directory>): Directory {
    const message = createBaseDirectory();
    message.files = object.files?.map((e) => FileNode.fromPartial(e)) || [];
    message.directories = object.directories?.map((e) => DirectoryNode.fromPartial(e)) || [];
    message.symlinks = object.symlinks?.map((e) => SymlinkNode.fromPartial(e)) || [];
    message.nodeProperties = (object.nodeProperties !== undefined && object.nodeProperties !== null)
      ? NodeProperties.fromPartial(object.nodeProperties)
      : undefined;
    return message;
  },
};

function createBaseNodeProperty(): NodeProperty {
  return { name: "", value: "" };
}

export const NodeProperty: MessageFns<NodeProperty> = {
  encode(message: NodeProperty, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NodeProperty {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNodeProperty();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NodeProperty {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: NodeProperty): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<NodeProperty>): NodeProperty {
    return NodeProperty.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NodeProperty>): NodeProperty {
    const message = createBaseNodeProperty();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseNodeProperties(): NodeProperties {
  return { properties: [], mtime: undefined, unixMode: undefined };
}

export const NodeProperties: MessageFns<NodeProperties> = {
  encode(message: NodeProperties, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.properties) {
      NodeProperty.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.mtime !== undefined) {
      Timestamp.encode(toTimestamp(message.mtime), writer.uint32(18).fork()).join();
    }
    if (message.unixMode !== undefined) {
      UInt32Value.encode({ value: message.unixMode! }, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NodeProperties {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNodeProperties();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.properties.push(NodeProperty.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.mtime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.unixMode = UInt32Value.decode(reader, reader.uint32()).value;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NodeProperties {
    return {
      properties: globalThis.Array.isArray(object?.properties)
        ? object.properties.map((e: any) => NodeProperty.fromJSON(e))
        : [],
      mtime: isSet(object.mtime) ? fromJsonTimestamp(object.mtime) : undefined,
      unixMode: isSet(object.unixMode) ? Number(object.unixMode) : undefined,
    };
  },

  toJSON(message: NodeProperties): unknown {
    const obj: any = {};
    if (message.properties?.length) {
      obj.properties = message.properties.map((e) => NodeProperty.toJSON(e));
    }
    if (message.mtime !== undefined) {
      obj.mtime = message.mtime.toISOString();
    }
    if (message.unixMode !== undefined) {
      obj.unixMode = message.unixMode;
    }
    return obj;
  },

  create(base?: DeepPartial<NodeProperties>): NodeProperties {
    return NodeProperties.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NodeProperties>): NodeProperties {
    const message = createBaseNodeProperties();
    message.properties = object.properties?.map((e) => NodeProperty.fromPartial(e)) || [];
    message.mtime = object.mtime ?? undefined;
    message.unixMode = object.unixMode ?? undefined;
    return message;
  },
};

function createBaseFileNode(): FileNode {
  return { name: "", digest: undefined, isExecutable: false, nodeProperties: undefined };
}

export const FileNode: MessageFns<FileNode> = {
  encode(message: FileNode, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.digest !== undefined) {
      Digest.encode(message.digest, writer.uint32(18).fork()).join();
    }
    if (message.isExecutable !== false) {
      writer.uint32(32).bool(message.isExecutable);
    }
    if (message.nodeProperties !== undefined) {
      NodeProperties.encode(message.nodeProperties, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FileNode {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFileNode();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.digest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.isExecutable = reader.bool();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.nodeProperties = NodeProperties.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FileNode {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      digest: isSet(object.digest) ? Digest.fromJSON(object.digest) : undefined,
      isExecutable: isSet(object.isExecutable) ? globalThis.Boolean(object.isExecutable) : false,
      nodeProperties: isSet(object.nodeProperties) ? NodeProperties.fromJSON(object.nodeProperties) : undefined,
    };
  },

  toJSON(message: FileNode): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.digest !== undefined) {
      obj.digest = Digest.toJSON(message.digest);
    }
    if (message.isExecutable !== false) {
      obj.isExecutable = message.isExecutable;
    }
    if (message.nodeProperties !== undefined) {
      obj.nodeProperties = NodeProperties.toJSON(message.nodeProperties);
    }
    return obj;
  },

  create(base?: DeepPartial<FileNode>): FileNode {
    return FileNode.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FileNode>): FileNode {
    const message = createBaseFileNode();
    message.name = object.name ?? "";
    message.digest = (object.digest !== undefined && object.digest !== null)
      ? Digest.fromPartial(object.digest)
      : undefined;
    message.isExecutable = object.isExecutable ?? false;
    message.nodeProperties = (object.nodeProperties !== undefined && object.nodeProperties !== null)
      ? NodeProperties.fromPartial(object.nodeProperties)
      : undefined;
    return message;
  },
};

function createBaseDirectoryNode(): DirectoryNode {
  return { name: "", digest: undefined };
}

export const DirectoryNode: MessageFns<DirectoryNode> = {
  encode(message: DirectoryNode, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.digest !== undefined) {
      Digest.encode(message.digest, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectoryNode {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectoryNode();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.digest = Digest.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectoryNode {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      digest: isSet(object.digest) ? Digest.fromJSON(object.digest) : undefined,
    };
  },

  toJSON(message: DirectoryNode): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.digest !== undefined) {
      obj.digest = Digest.toJSON(message.digest);
    }
    return obj;
  },

  create(base?: DeepPartial<DirectoryNode>): DirectoryNode {
    return DirectoryNode.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectoryNode>): DirectoryNode {
    const message = createBaseDirectoryNode();
    message.name = object.name ?? "";
    message.digest = (object.digest !== undefined && object.digest !== null)
      ? Digest.fromPartial(object.digest)
      : undefined;
    return message;
  },
};

function createBaseSymlinkNode(): SymlinkNode {
  return { name: "", target: "", nodeProperties: undefined };
}

export const SymlinkNode: MessageFns<SymlinkNode> = {
  encode(message: SymlinkNode, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.target !== "") {
      writer.uint32(18).string(message.target);
    }
    if (message.nodeProperties !== undefined) {
      NodeProperties.encode(message.nodeProperties, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SymlinkNode {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSymlinkNode();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.target = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.nodeProperties = NodeProperties.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SymlinkNode {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      target: isSet(object.target) ? globalThis.String(object.target) : "",
      nodeProperties: isSet(object.nodeProperties) ? NodeProperties.fromJSON(object.nodeProperties) : undefined,
    };
  },

  toJSON(message: SymlinkNode): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.target !== "") {
      obj.target = message.target;
    }
    if (message.nodeProperties !== undefined) {
      obj.nodeProperties = NodeProperties.toJSON(message.nodeProperties);
    }
    return obj;
  },

  create(base?: DeepPartial<SymlinkNode>): SymlinkNode {
    return SymlinkNode.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SymlinkNode>): SymlinkNode {
    const message = createBaseSymlinkNode();
    message.name = object.name ?? "";
    message.target = object.target ?? "";
    message.nodeProperties = (object.nodeProperties !== undefined && object.nodeProperties !== null)
      ? NodeProperties.fromPartial(object.nodeProperties)
      : undefined;
    return message;
  },
};

function createBaseDigest(): Digest {
  return { hash: "", sizeBytes: "0" };
}

export const Digest: MessageFns<Digest> = {
  encode(message: Digest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.hash !== "") {
      writer.uint32(10).string(message.hash);
    }
    if (message.sizeBytes !== "0") {
      writer.uint32(16).int64(message.sizeBytes);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Digest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDigest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.hash = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.sizeBytes = reader.int64().toString();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Digest {
    return {
      hash: isSet(object.hash) ? globalThis.String(object.hash) : "",
      sizeBytes: isSet(object.sizeBytes) ? globalThis.String(object.sizeBytes) : "0",
    };
  },

  toJSON(message: Digest): unknown {
    const obj: any = {};
    if (message.hash !== "") {
      obj.hash = message.hash;
    }
    if (message.sizeBytes !== "0") {
      obj.sizeBytes = message.sizeBytes;
    }
    return obj;
  },

  create(base?: DeepPartial<Digest>): Digest {
    return Digest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Digest>): Digest {
    const message = createBaseDigest();
    message.hash = object.hash ?? "";
    message.sizeBytes = object.sizeBytes ?? "0";
    return message;
  },
};

function createBaseExecutedActionMetadata(): ExecutedActionMetadata {
  return {
    worker: "",
    queuedTimestamp: undefined,
    workerStartTimestamp: undefined,
    workerCompletedTimestamp: undefined,
    inputFetchStartTimestamp: undefined,
    inputFetchCompletedTimestamp: undefined,
    executionStartTimestamp: undefined,
    executionCompletedTimestamp: undefined,
    virtualExecutionDuration: undefined,
    outputUploadStartTimestamp: undefined,
    outputUploadCompletedTimestamp: undefined,
    auxiliaryMetadata: [],
  };
}

export const ExecutedActionMetadata: MessageFns<ExecutedActionMetadata> = {
  encode(message: ExecutedActionMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.worker !== "") {
      writer.uint32(10).string(message.worker);
    }
    if (message.queuedTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.queuedTimestamp), writer.uint32(18).fork()).join();
    }
    if (message.workerStartTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.workerStartTimestamp), writer.uint32(26).fork()).join();
    }
    if (message.workerCompletedTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.workerCompletedTimestamp), writer.uint32(34).fork()).join();
    }
    if (message.inputFetchStartTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.inputFetchStartTimestamp), writer.uint32(42).fork()).join();
    }
    if (message.inputFetchCompletedTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.inputFetchCompletedTimestamp), writer.uint32(50).fork()).join();
    }
    if (message.executionStartTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.executionStartTimestamp), writer.uint32(58).fork()).join();
    }
    if (message.executionCompletedTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.executionCompletedTimestamp), writer.uint32(66).fork()).join();
    }
    if (message.virtualExecutionDuration !== undefined) {
      Duration.encode(message.virtualExecutionDuration, writer.uint32(98).fork()).join();
    }
    if (message.outputUploadStartTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.outputUploadStartTimestamp), writer.uint32(74).fork()).join();
    }
    if (message.outputUploadCompletedTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.outputUploadCompletedTimestamp), writer.uint32(82).fork()).join();
    }
    for (const v of message.auxiliaryMetadata) {
      Any.encode(v!, writer.uint32(90).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutedActionMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutedActionMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.worker = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.queuedTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.workerStartTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.workerCompletedTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.inputFetchStartTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.inputFetchCompletedTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.executionStartTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.executionCompletedTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.virtualExecutionDuration = Duration.decode(reader, reader.uint32());
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.outputUploadStartTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.outputUploadCompletedTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.auxiliaryMetadata.push(Any.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutedActionMetadata {
    return {
      worker: isSet(object.worker) ? globalThis.String(object.worker) : "",
      queuedTimestamp: isSet(object.queuedTimestamp) ? fromJsonTimestamp(object.queuedTimestamp) : undefined,
      workerStartTimestamp: isSet(object.workerStartTimestamp)
        ? fromJsonTimestamp(object.workerStartTimestamp)
        : undefined,
      workerCompletedTimestamp: isSet(object.workerCompletedTimestamp)
        ? fromJsonTimestamp(object.workerCompletedTimestamp)
        : undefined,
      inputFetchStartTimestamp: isSet(object.inputFetchStartTimestamp)
        ? fromJsonTimestamp(object.inputFetchStartTimestamp)
        : undefined,
      inputFetchCompletedTimestamp: isSet(object.inputFetchCompletedTimestamp)
        ? fromJsonTimestamp(object.inputFetchCompletedTimestamp)
        : undefined,
      executionStartTimestamp: isSet(object.executionStartTimestamp)
        ? fromJsonTimestamp(object.executionStartTimestamp)
        : undefined,
      executionCompletedTimestamp: isSet(object.executionCompletedTimestamp)
        ? fromJsonTimestamp(object.executionCompletedTimestamp)
        : undefined,
      virtualExecutionDuration: isSet(object.virtualExecutionDuration)
        ? Duration.fromJSON(object.virtualExecutionDuration)
        : undefined,
      outputUploadStartTimestamp: isSet(object.outputUploadStartTimestamp)
        ? fromJsonTimestamp(object.outputUploadStartTimestamp)
        : undefined,
      outputUploadCompletedTimestamp: isSet(object.outputUploadCompletedTimestamp)
        ? fromJsonTimestamp(object.outputUploadCompletedTimestamp)
        : undefined,
      auxiliaryMetadata: globalThis.Array.isArray(object?.auxiliaryMetadata)
        ? object.auxiliaryMetadata.map((e: any) => Any.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ExecutedActionMetadata): unknown {
    const obj: any = {};
    if (message.worker !== "") {
      obj.worker = message.worker;
    }
    if (message.queuedTimestamp !== undefined) {
      obj.queuedTimestamp = message.queuedTimestamp.toISOString();
    }
    if (message.workerStartTimestamp !== undefined) {
      obj.workerStartTimestamp = message.workerStartTimestamp.toISOString();
    }
    if (message.workerCompletedTimestamp !== undefined) {
      obj.workerCompletedTimestamp = message.workerCompletedTimestamp.toISOString();
    }
    if (message.inputFetchStartTimestamp !== undefined) {
      obj.inputFetchStartTimestamp = message.inputFetchStartTimestamp.toISOString();
    }
    if (message.inputFetchCompletedTimestamp !== undefined) {
      obj.inputFetchCompletedTimestamp = message.inputFetchCompletedTimestamp.toISOString();
    }
    if (message.executionStartTimestamp !== undefined) {
      obj.executionStartTimestamp = message.executionStartTimestamp.toISOString();
    }
    if (message.executionCompletedTimestamp !== undefined) {
      obj.executionCompletedTimestamp = message.executionCompletedTimestamp.toISOString();
    }
    if (message.virtualExecutionDuration !== undefined) {
      obj.virtualExecutionDuration = Duration.toJSON(message.virtualExecutionDuration);
    }
    if (message.outputUploadStartTimestamp !== undefined) {
      obj.outputUploadStartTimestamp = message.outputUploadStartTimestamp.toISOString();
    }
    if (message.outputUploadCompletedTimestamp !== undefined) {
      obj.outputUploadCompletedTimestamp = message.outputUploadCompletedTimestamp.toISOString();
    }
    if (message.auxiliaryMetadata?.length) {
      obj.auxiliaryMetadata = message.auxiliaryMetadata.map((e) => Any.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutedActionMetadata>): ExecutedActionMetadata {
    return ExecutedActionMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutedActionMetadata>): ExecutedActionMetadata {
    const message = createBaseExecutedActionMetadata();
    message.worker = object.worker ?? "";
    message.queuedTimestamp = object.queuedTimestamp ?? undefined;
    message.workerStartTimestamp = object.workerStartTimestamp ?? undefined;
    message.workerCompletedTimestamp = object.workerCompletedTimestamp ?? undefined;
    message.inputFetchStartTimestamp = object.inputFetchStartTimestamp ?? undefined;
    message.inputFetchCompletedTimestamp = object.inputFetchCompletedTimestamp ?? undefined;
    message.executionStartTimestamp = object.executionStartTimestamp ?? undefined;
    message.executionCompletedTimestamp = object.executionCompletedTimestamp ?? undefined;
    message.virtualExecutionDuration =
      (object.virtualExecutionDuration !== undefined && object.virtualExecutionDuration !== null)
        ? Duration.fromPartial(object.virtualExecutionDuration)
        : undefined;
    message.outputUploadStartTimestamp = object.outputUploadStartTimestamp ?? undefined;
    message.outputUploadCompletedTimestamp = object.outputUploadCompletedTimestamp ?? undefined;
    message.auxiliaryMetadata = object.auxiliaryMetadata?.map((e) => Any.fromPartial(e)) || [];
    return message;
  },
};

function createBaseActionResult(): ActionResult {
  return {
    outputFiles: [],
    outputFileSymlinks: [],
    outputSymlinks: [],
    outputDirectories: [],
    outputDirectorySymlinks: [],
    exitCode: 0,
    stdoutRaw: new Uint8Array(0),
    stdoutDigest: undefined,
    stderrRaw: new Uint8Array(0),
    stderrDigest: undefined,
    executionMetadata: undefined,
  };
}

export const ActionResult: MessageFns<ActionResult> = {
  encode(message: ActionResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.outputFiles) {
      OutputFile.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.outputFileSymlinks) {
      OutputSymlink.encode(v!, writer.uint32(82).fork()).join();
    }
    for (const v of message.outputSymlinks) {
      OutputSymlink.encode(v!, writer.uint32(98).fork()).join();
    }
    for (const v of message.outputDirectories) {
      OutputDirectory.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.outputDirectorySymlinks) {
      OutputSymlink.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.exitCode !== 0) {
      writer.uint32(32).int32(message.exitCode);
    }
    if (message.stdoutRaw.length !== 0) {
      writer.uint32(42).bytes(message.stdoutRaw);
    }
    if (message.stdoutDigest !== undefined) {
      Digest.encode(message.stdoutDigest, writer.uint32(50).fork()).join();
    }
    if (message.stderrRaw.length !== 0) {
      writer.uint32(58).bytes(message.stderrRaw);
    }
    if (message.stderrDigest !== undefined) {
      Digest.encode(message.stderrDigest, writer.uint32(66).fork()).join();
    }
    if (message.executionMetadata !== undefined) {
      ExecutedActionMetadata.encode(message.executionMetadata, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ActionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseActionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.outputFiles.push(OutputFile.decode(reader, reader.uint32()));
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.outputFileSymlinks.push(OutputSymlink.decode(reader, reader.uint32()));
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.outputSymlinks.push(OutputSymlink.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.outputDirectories.push(OutputDirectory.decode(reader, reader.uint32()));
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.outputDirectorySymlinks.push(OutputSymlink.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.exitCode = reader.int32();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.stdoutRaw = reader.bytes();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.stdoutDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.stderrRaw = reader.bytes();
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.stderrDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.executionMetadata = ExecutedActionMetadata.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ActionResult {
    return {
      outputFiles: globalThis.Array.isArray(object?.outputFiles)
        ? object.outputFiles.map((e: any) => OutputFile.fromJSON(e))
        : [],
      outputFileSymlinks: globalThis.Array.isArray(object?.outputFileSymlinks)
        ? object.outputFileSymlinks.map((e: any) => OutputSymlink.fromJSON(e))
        : [],
      outputSymlinks: globalThis.Array.isArray(object?.outputSymlinks)
        ? object.outputSymlinks.map((e: any) => OutputSymlink.fromJSON(e))
        : [],
      outputDirectories: globalThis.Array.isArray(object?.outputDirectories)
        ? object.outputDirectories.map((e: any) => OutputDirectory.fromJSON(e))
        : [],
      outputDirectorySymlinks: globalThis.Array.isArray(object?.outputDirectorySymlinks)
        ? object.outputDirectorySymlinks.map((e: any) => OutputSymlink.fromJSON(e))
        : [],
      exitCode: isSet(object.exitCode) ? globalThis.Number(object.exitCode) : 0,
      stdoutRaw: isSet(object.stdoutRaw) ? bytesFromBase64(object.stdoutRaw) : new Uint8Array(0),
      stdoutDigest: isSet(object.stdoutDigest) ? Digest.fromJSON(object.stdoutDigest) : undefined,
      stderrRaw: isSet(object.stderrRaw) ? bytesFromBase64(object.stderrRaw) : new Uint8Array(0),
      stderrDigest: isSet(object.stderrDigest) ? Digest.fromJSON(object.stderrDigest) : undefined,
      executionMetadata: isSet(object.executionMetadata)
        ? ExecutedActionMetadata.fromJSON(object.executionMetadata)
        : undefined,
    };
  },

  toJSON(message: ActionResult): unknown {
    const obj: any = {};
    if (message.outputFiles?.length) {
      obj.outputFiles = message.outputFiles.map((e) => OutputFile.toJSON(e));
    }
    if (message.outputFileSymlinks?.length) {
      obj.outputFileSymlinks = message.outputFileSymlinks.map((e) => OutputSymlink.toJSON(e));
    }
    if (message.outputSymlinks?.length) {
      obj.outputSymlinks = message.outputSymlinks.map((e) => OutputSymlink.toJSON(e));
    }
    if (message.outputDirectories?.length) {
      obj.outputDirectories = message.outputDirectories.map((e) => OutputDirectory.toJSON(e));
    }
    if (message.outputDirectorySymlinks?.length) {
      obj.outputDirectorySymlinks = message.outputDirectorySymlinks.map((e) => OutputSymlink.toJSON(e));
    }
    if (message.exitCode !== 0) {
      obj.exitCode = Math.round(message.exitCode);
    }
    if (message.stdoutRaw.length !== 0) {
      obj.stdoutRaw = base64FromBytes(message.stdoutRaw);
    }
    if (message.stdoutDigest !== undefined) {
      obj.stdoutDigest = Digest.toJSON(message.stdoutDigest);
    }
    if (message.stderrRaw.length !== 0) {
      obj.stderrRaw = base64FromBytes(message.stderrRaw);
    }
    if (message.stderrDigest !== undefined) {
      obj.stderrDigest = Digest.toJSON(message.stderrDigest);
    }
    if (message.executionMetadata !== undefined) {
      obj.executionMetadata = ExecutedActionMetadata.toJSON(message.executionMetadata);
    }
    return obj;
  },

  create(base?: DeepPartial<ActionResult>): ActionResult {
    return ActionResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ActionResult>): ActionResult {
    const message = createBaseActionResult();
    message.outputFiles = object.outputFiles?.map((e) => OutputFile.fromPartial(e)) || [];
    message.outputFileSymlinks = object.outputFileSymlinks?.map((e) => OutputSymlink.fromPartial(e)) || [];
    message.outputSymlinks = object.outputSymlinks?.map((e) => OutputSymlink.fromPartial(e)) || [];
    message.outputDirectories = object.outputDirectories?.map((e) => OutputDirectory.fromPartial(e)) || [];
    message.outputDirectorySymlinks = object.outputDirectorySymlinks?.map((e) => OutputSymlink.fromPartial(e)) || [];
    message.exitCode = object.exitCode ?? 0;
    message.stdoutRaw = object.stdoutRaw ?? new Uint8Array(0);
    message.stdoutDigest = (object.stdoutDigest !== undefined && object.stdoutDigest !== null)
      ? Digest.fromPartial(object.stdoutDigest)
      : undefined;
    message.stderrRaw = object.stderrRaw ?? new Uint8Array(0);
    message.stderrDigest = (object.stderrDigest !== undefined && object.stderrDigest !== null)
      ? Digest.fromPartial(object.stderrDigest)
      : undefined;
    message.executionMetadata = (object.executionMetadata !== undefined && object.executionMetadata !== null)
      ? ExecutedActionMetadata.fromPartial(object.executionMetadata)
      : undefined;
    return message;
  },
};

function createBaseOutputFile(): OutputFile {
  return { path: "", digest: undefined, isExecutable: false, contents: new Uint8Array(0), nodeProperties: undefined };
}

export const OutputFile: MessageFns<OutputFile> = {
  encode(message: OutputFile, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.path !== "") {
      writer.uint32(10).string(message.path);
    }
    if (message.digest !== undefined) {
      Digest.encode(message.digest, writer.uint32(18).fork()).join();
    }
    if (message.isExecutable !== false) {
      writer.uint32(32).bool(message.isExecutable);
    }
    if (message.contents.length !== 0) {
      writer.uint32(42).bytes(message.contents);
    }
    if (message.nodeProperties !== undefined) {
      NodeProperties.encode(message.nodeProperties, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputFile {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputFile();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.path = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.digest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.isExecutable = reader.bool();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.contents = reader.bytes();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.nodeProperties = NodeProperties.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputFile {
    return {
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      digest: isSet(object.digest) ? Digest.fromJSON(object.digest) : undefined,
      isExecutable: isSet(object.isExecutable) ? globalThis.Boolean(object.isExecutable) : false,
      contents: isSet(object.contents) ? bytesFromBase64(object.contents) : new Uint8Array(0),
      nodeProperties: isSet(object.nodeProperties) ? NodeProperties.fromJSON(object.nodeProperties) : undefined,
    };
  },

  toJSON(message: OutputFile): unknown {
    const obj: any = {};
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.digest !== undefined) {
      obj.digest = Digest.toJSON(message.digest);
    }
    if (message.isExecutable !== false) {
      obj.isExecutable = message.isExecutable;
    }
    if (message.contents.length !== 0) {
      obj.contents = base64FromBytes(message.contents);
    }
    if (message.nodeProperties !== undefined) {
      obj.nodeProperties = NodeProperties.toJSON(message.nodeProperties);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputFile>): OutputFile {
    return OutputFile.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputFile>): OutputFile {
    const message = createBaseOutputFile();
    message.path = object.path ?? "";
    message.digest = (object.digest !== undefined && object.digest !== null)
      ? Digest.fromPartial(object.digest)
      : undefined;
    message.isExecutable = object.isExecutable ?? false;
    message.contents = object.contents ?? new Uint8Array(0);
    message.nodeProperties = (object.nodeProperties !== undefined && object.nodeProperties !== null)
      ? NodeProperties.fromPartial(object.nodeProperties)
      : undefined;
    return message;
  },
};

function createBaseTree(): Tree {
  return { root: undefined, children: [] };
}

export const Tree: MessageFns<Tree> = {
  encode(message: Tree, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.root !== undefined) {
      Directory.encode(message.root, writer.uint32(10).fork()).join();
    }
    for (const v of message.children) {
      Directory.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Tree {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTree();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.root = Directory.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.children.push(Directory.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Tree {
    return {
      root: isSet(object.root) ? Directory.fromJSON(object.root) : undefined,
      children: globalThis.Array.isArray(object?.children)
        ? object.children.map((e: any) => Directory.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Tree): unknown {
    const obj: any = {};
    if (message.root !== undefined) {
      obj.root = Directory.toJSON(message.root);
    }
    if (message.children?.length) {
      obj.children = message.children.map((e) => Directory.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Tree>): Tree {
    return Tree.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Tree>): Tree {
    const message = createBaseTree();
    message.root = (object.root !== undefined && object.root !== null) ? Directory.fromPartial(object.root) : undefined;
    message.children = object.children?.map((e) => Directory.fromPartial(e)) || [];
    return message;
  },
};

function createBaseOutputDirectory(): OutputDirectory {
  return { path: "", treeDigest: undefined, isTopologicallySorted: false, rootDirectoryDigest: undefined };
}

export const OutputDirectory: MessageFns<OutputDirectory> = {
  encode(message: OutputDirectory, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.path !== "") {
      writer.uint32(10).string(message.path);
    }
    if (message.treeDigest !== undefined) {
      Digest.encode(message.treeDigest, writer.uint32(26).fork()).join();
    }
    if (message.isTopologicallySorted !== false) {
      writer.uint32(32).bool(message.isTopologicallySorted);
    }
    if (message.rootDirectoryDigest !== undefined) {
      Digest.encode(message.rootDirectoryDigest, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputDirectory {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputDirectory();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.path = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.treeDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.isTopologicallySorted = reader.bool();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.rootDirectoryDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputDirectory {
    return {
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      treeDigest: isSet(object.treeDigest) ? Digest.fromJSON(object.treeDigest) : undefined,
      isTopologicallySorted: isSet(object.isTopologicallySorted)
        ? globalThis.Boolean(object.isTopologicallySorted)
        : false,
      rootDirectoryDigest: isSet(object.rootDirectoryDigest) ? Digest.fromJSON(object.rootDirectoryDigest) : undefined,
    };
  },

  toJSON(message: OutputDirectory): unknown {
    const obj: any = {};
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.treeDigest !== undefined) {
      obj.treeDigest = Digest.toJSON(message.treeDigest);
    }
    if (message.isTopologicallySorted !== false) {
      obj.isTopologicallySorted = message.isTopologicallySorted;
    }
    if (message.rootDirectoryDigest !== undefined) {
      obj.rootDirectoryDigest = Digest.toJSON(message.rootDirectoryDigest);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputDirectory>): OutputDirectory {
    return OutputDirectory.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputDirectory>): OutputDirectory {
    const message = createBaseOutputDirectory();
    message.path = object.path ?? "";
    message.treeDigest = (object.treeDigest !== undefined && object.treeDigest !== null)
      ? Digest.fromPartial(object.treeDigest)
      : undefined;
    message.isTopologicallySorted = object.isTopologicallySorted ?? false;
    message.rootDirectoryDigest = (object.rootDirectoryDigest !== undefined && object.rootDirectoryDigest !== null)
      ? Digest.fromPartial(object.rootDirectoryDigest)
      : undefined;
    return message;
  },
};

function createBaseOutputSymlink(): OutputSymlink {
  return { path: "", target: "", nodeProperties: undefined };
}

export const OutputSymlink: MessageFns<OutputSymlink> = {
  encode(message: OutputSymlink, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.path !== "") {
      writer.uint32(10).string(message.path);
    }
    if (message.target !== "") {
      writer.uint32(18).string(message.target);
    }
    if (message.nodeProperties !== undefined) {
      NodeProperties.encode(message.nodeProperties, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputSymlink {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputSymlink();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.path = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.target = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.nodeProperties = NodeProperties.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputSymlink {
    return {
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      target: isSet(object.target) ? globalThis.String(object.target) : "",
      nodeProperties: isSet(object.nodeProperties) ? NodeProperties.fromJSON(object.nodeProperties) : undefined,
    };
  },

  toJSON(message: OutputSymlink): unknown {
    const obj: any = {};
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.target !== "") {
      obj.target = message.target;
    }
    if (message.nodeProperties !== undefined) {
      obj.nodeProperties = NodeProperties.toJSON(message.nodeProperties);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputSymlink>): OutputSymlink {
    return OutputSymlink.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputSymlink>): OutputSymlink {
    const message = createBaseOutputSymlink();
    message.path = object.path ?? "";
    message.target = object.target ?? "";
    message.nodeProperties = (object.nodeProperties !== undefined && object.nodeProperties !== null)
      ? NodeProperties.fromPartial(object.nodeProperties)
      : undefined;
    return message;
  },
};

function createBaseExecutionPolicy(): ExecutionPolicy {
  return { priority: 0 };
}

export const ExecutionPolicy: MessageFns<ExecutionPolicy> = {
  encode(message: ExecutionPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.priority !== 0) {
      writer.uint32(8).int32(message.priority);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.priority = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionPolicy {
    return { priority: isSet(object.priority) ? globalThis.Number(object.priority) : 0 };
  },

  toJSON(message: ExecutionPolicy): unknown {
    const obj: any = {};
    if (message.priority !== 0) {
      obj.priority = Math.round(message.priority);
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionPolicy>): ExecutionPolicy {
    return ExecutionPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionPolicy>): ExecutionPolicy {
    const message = createBaseExecutionPolicy();
    message.priority = object.priority ?? 0;
    return message;
  },
};

function createBaseResultsCachePolicy(): ResultsCachePolicy {
  return { priority: 0 };
}

export const ResultsCachePolicy: MessageFns<ResultsCachePolicy> = {
  encode(message: ResultsCachePolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.priority !== 0) {
      writer.uint32(8).int32(message.priority);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResultsCachePolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResultsCachePolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.priority = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ResultsCachePolicy {
    return { priority: isSet(object.priority) ? globalThis.Number(object.priority) : 0 };
  },

  toJSON(message: ResultsCachePolicy): unknown {
    const obj: any = {};
    if (message.priority !== 0) {
      obj.priority = Math.round(message.priority);
    }
    return obj;
  },

  create(base?: DeepPartial<ResultsCachePolicy>): ResultsCachePolicy {
    return ResultsCachePolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResultsCachePolicy>): ResultsCachePolicy {
    const message = createBaseResultsCachePolicy();
    message.priority = object.priority ?? 0;
    return message;
  },
};

function createBaseExecuteRequest(): ExecuteRequest {
  return {
    instanceName: "",
    skipCacheLookup: false,
    actionDigest: undefined,
    executionPolicy: undefined,
    resultsCachePolicy: undefined,
    digestFunction: 0,
    inlineStdout: false,
    inlineStderr: false,
    inlineOutputFiles: [],
  };
}

export const ExecuteRequest: MessageFns<ExecuteRequest> = {
  encode(message: ExecuteRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    if (message.skipCacheLookup !== false) {
      writer.uint32(24).bool(message.skipCacheLookup);
    }
    if (message.actionDigest !== undefined) {
      Digest.encode(message.actionDigest, writer.uint32(50).fork()).join();
    }
    if (message.executionPolicy !== undefined) {
      ExecutionPolicy.encode(message.executionPolicy, writer.uint32(58).fork()).join();
    }
    if (message.resultsCachePolicy !== undefined) {
      ResultsCachePolicy.encode(message.resultsCachePolicy, writer.uint32(66).fork()).join();
    }
    if (message.digestFunction !== 0) {
      writer.uint32(72).int32(message.digestFunction);
    }
    if (message.inlineStdout !== false) {
      writer.uint32(80).bool(message.inlineStdout);
    }
    if (message.inlineStderr !== false) {
      writer.uint32(88).bool(message.inlineStderr);
    }
    for (const v of message.inlineOutputFiles) {
      writer.uint32(98).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.skipCacheLookup = reader.bool();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.actionDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.executionPolicy = ExecutionPolicy.decode(reader, reader.uint32());
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.resultsCachePolicy = ResultsCachePolicy.decode(reader, reader.uint32());
          continue;
        }
        case 9: {
          if (tag !== 72) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
        case 10: {
          if (tag !== 80) {
            break;
          }

          message.inlineStdout = reader.bool();
          continue;
        }
        case 11: {
          if (tag !== 88) {
            break;
          }

          message.inlineStderr = reader.bool();
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.inlineOutputFiles.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteRequest {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      skipCacheLookup: isSet(object.skipCacheLookup) ? globalThis.Boolean(object.skipCacheLookup) : false,
      actionDigest: isSet(object.actionDigest) ? Digest.fromJSON(object.actionDigest) : undefined,
      executionPolicy: isSet(object.executionPolicy) ? ExecutionPolicy.fromJSON(object.executionPolicy) : undefined,
      resultsCachePolicy: isSet(object.resultsCachePolicy)
        ? ResultsCachePolicy.fromJSON(object.resultsCachePolicy)
        : undefined,
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
      inlineStdout: isSet(object.inlineStdout) ? globalThis.Boolean(object.inlineStdout) : false,
      inlineStderr: isSet(object.inlineStderr) ? globalThis.Boolean(object.inlineStderr) : false,
      inlineOutputFiles: globalThis.Array.isArray(object?.inlineOutputFiles)
        ? object.inlineOutputFiles.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ExecuteRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.skipCacheLookup !== false) {
      obj.skipCacheLookup = message.skipCacheLookup;
    }
    if (message.actionDigest !== undefined) {
      obj.actionDigest = Digest.toJSON(message.actionDigest);
    }
    if (message.executionPolicy !== undefined) {
      obj.executionPolicy = ExecutionPolicy.toJSON(message.executionPolicy);
    }
    if (message.resultsCachePolicy !== undefined) {
      obj.resultsCachePolicy = ResultsCachePolicy.toJSON(message.resultsCachePolicy);
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    if (message.inlineStdout !== false) {
      obj.inlineStdout = message.inlineStdout;
    }
    if (message.inlineStderr !== false) {
      obj.inlineStderr = message.inlineStderr;
    }
    if (message.inlineOutputFiles?.length) {
      obj.inlineOutputFiles = message.inlineOutputFiles;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteRequest>): ExecuteRequest {
    return ExecuteRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteRequest>): ExecuteRequest {
    const message = createBaseExecuteRequest();
    message.instanceName = object.instanceName ?? "";
    message.skipCacheLookup = object.skipCacheLookup ?? false;
    message.actionDigest = (object.actionDigest !== undefined && object.actionDigest !== null)
      ? Digest.fromPartial(object.actionDigest)
      : undefined;
    message.executionPolicy = (object.executionPolicy !== undefined && object.executionPolicy !== null)
      ? ExecutionPolicy.fromPartial(object.executionPolicy)
      : undefined;
    message.resultsCachePolicy = (object.resultsCachePolicy !== undefined && object.resultsCachePolicy !== null)
      ? ResultsCachePolicy.fromPartial(object.resultsCachePolicy)
      : undefined;
    message.digestFunction = object.digestFunction ?? 0;
    message.inlineStdout = object.inlineStdout ?? false;
    message.inlineStderr = object.inlineStderr ?? false;
    message.inlineOutputFiles = object.inlineOutputFiles?.map((e) => e) || [];
    return message;
  },
};

function createBaseLogFile(): LogFile {
  return { digest: undefined, humanReadable: false };
}

export const LogFile: MessageFns<LogFile> = {
  encode(message: LogFile, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.digest !== undefined) {
      Digest.encode(message.digest, writer.uint32(10).fork()).join();
    }
    if (message.humanReadable !== false) {
      writer.uint32(16).bool(message.humanReadable);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LogFile {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLogFile();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.digest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.humanReadable = reader.bool();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LogFile {
    return {
      digest: isSet(object.digest) ? Digest.fromJSON(object.digest) : undefined,
      humanReadable: isSet(object.humanReadable) ? globalThis.Boolean(object.humanReadable) : false,
    };
  },

  toJSON(message: LogFile): unknown {
    const obj: any = {};
    if (message.digest !== undefined) {
      obj.digest = Digest.toJSON(message.digest);
    }
    if (message.humanReadable !== false) {
      obj.humanReadable = message.humanReadable;
    }
    return obj;
  },

  create(base?: DeepPartial<LogFile>): LogFile {
    return LogFile.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LogFile>): LogFile {
    const message = createBaseLogFile();
    message.digest = (object.digest !== undefined && object.digest !== null)
      ? Digest.fromPartial(object.digest)
      : undefined;
    message.humanReadable = object.humanReadable ?? false;
    return message;
  },
};

function createBaseExecuteResponse(): ExecuteResponse {
  return { result: undefined, cachedResult: false, status: undefined, serverLogs: {}, message: "" };
}

export const ExecuteResponse: MessageFns<ExecuteResponse> = {
  encode(message: ExecuteResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.result !== undefined) {
      ActionResult.encode(message.result, writer.uint32(10).fork()).join();
    }
    if (message.cachedResult !== false) {
      writer.uint32(16).bool(message.cachedResult);
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(26).fork()).join();
    }
    Object.entries(message.serverLogs).forEach(([key, value]) => {
      ExecuteResponse_ServerLogsEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    if (message.message !== "") {
      writer.uint32(42).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.result = ActionResult.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.cachedResult = reader.bool();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          const entry4 = ExecuteResponse_ServerLogsEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.serverLogs[entry4.key] = entry4.value;
          }
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.message = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteResponse {
    return {
      result: isSet(object.result) ? ActionResult.fromJSON(object.result) : undefined,
      cachedResult: isSet(object.cachedResult) ? globalThis.Boolean(object.cachedResult) : false,
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
      serverLogs: isObject(object.serverLogs)
        ? Object.entries(object.serverLogs).reduce<{ [key: string]: LogFile }>((acc, [key, value]) => {
          acc[key] = LogFile.fromJSON(value);
          return acc;
        }, {})
        : {},
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: ExecuteResponse): unknown {
    const obj: any = {};
    if (message.result !== undefined) {
      obj.result = ActionResult.toJSON(message.result);
    }
    if (message.cachedResult !== false) {
      obj.cachedResult = message.cachedResult;
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    if (message.serverLogs) {
      const entries = Object.entries(message.serverLogs);
      if (entries.length > 0) {
        obj.serverLogs = {};
        entries.forEach(([k, v]) => {
          obj.serverLogs[k] = LogFile.toJSON(v);
        });
      }
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteResponse>): ExecuteResponse {
    return ExecuteResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteResponse>): ExecuteResponse {
    const message = createBaseExecuteResponse();
    message.result = (object.result !== undefined && object.result !== null)
      ? ActionResult.fromPartial(object.result)
      : undefined;
    message.cachedResult = object.cachedResult ?? false;
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    message.serverLogs = Object.entries(object.serverLogs ?? {}).reduce<{ [key: string]: LogFile }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = LogFile.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.message = object.message ?? "";
    return message;
  },
};

function createBaseExecuteResponse_ServerLogsEntry(): ExecuteResponse_ServerLogsEntry {
  return { key: "", value: undefined };
}

export const ExecuteResponse_ServerLogsEntry: MessageFns<ExecuteResponse_ServerLogsEntry> = {
  encode(message: ExecuteResponse_ServerLogsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      LogFile.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteResponse_ServerLogsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteResponse_ServerLogsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = LogFile.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteResponse_ServerLogsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? LogFile.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ExecuteResponse_ServerLogsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = LogFile.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteResponse_ServerLogsEntry>): ExecuteResponse_ServerLogsEntry {
    return ExecuteResponse_ServerLogsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteResponse_ServerLogsEntry>): ExecuteResponse_ServerLogsEntry {
    const message = createBaseExecuteResponse_ServerLogsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? LogFile.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseExecutionStage(): ExecutionStage {
  return {};
}

export const ExecutionStage: MessageFns<ExecutionStage> = {
  encode(_: ExecutionStage, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionStage {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionStage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): ExecutionStage {
    return {};
  },

  toJSON(_: ExecutionStage): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<ExecutionStage>): ExecutionStage {
    return ExecutionStage.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<ExecutionStage>): ExecutionStage {
    const message = createBaseExecutionStage();
    return message;
  },
};

function createBaseExecuteOperationMetadata(): ExecuteOperationMetadata {
  return {
    stage: 0,
    actionDigest: undefined,
    stdoutStreamName: "",
    stderrStreamName: "",
    partialExecutionMetadata: undefined,
    digestFunction: 0,
  };
}

export const ExecuteOperationMetadata: MessageFns<ExecuteOperationMetadata> = {
  encode(message: ExecuteOperationMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.stage !== 0) {
      writer.uint32(8).int32(message.stage);
    }
    if (message.actionDigest !== undefined) {
      Digest.encode(message.actionDigest, writer.uint32(18).fork()).join();
    }
    if (message.stdoutStreamName !== "") {
      writer.uint32(26).string(message.stdoutStreamName);
    }
    if (message.stderrStreamName !== "") {
      writer.uint32(34).string(message.stderrStreamName);
    }
    if (message.partialExecutionMetadata !== undefined) {
      ExecutedActionMetadata.encode(message.partialExecutionMetadata, writer.uint32(42).fork()).join();
    }
    if (message.digestFunction !== 0) {
      writer.uint32(48).int32(message.digestFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteOperationMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteOperationMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.stage = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.actionDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.stdoutStreamName = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.stderrStreamName = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.partialExecutionMetadata = ExecutedActionMetadata.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 48) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteOperationMetadata {
    return {
      stage: isSet(object.stage) ? executionStage_ValueFromJSON(object.stage) : 0,
      actionDigest: isSet(object.actionDigest) ? Digest.fromJSON(object.actionDigest) : undefined,
      stdoutStreamName: isSet(object.stdoutStreamName) ? globalThis.String(object.stdoutStreamName) : "",
      stderrStreamName: isSet(object.stderrStreamName) ? globalThis.String(object.stderrStreamName) : "",
      partialExecutionMetadata: isSet(object.partialExecutionMetadata)
        ? ExecutedActionMetadata.fromJSON(object.partialExecutionMetadata)
        : undefined,
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
    };
  },

  toJSON(message: ExecuteOperationMetadata): unknown {
    const obj: any = {};
    if (message.stage !== 0) {
      obj.stage = executionStage_ValueToJSON(message.stage);
    }
    if (message.actionDigest !== undefined) {
      obj.actionDigest = Digest.toJSON(message.actionDigest);
    }
    if (message.stdoutStreamName !== "") {
      obj.stdoutStreamName = message.stdoutStreamName;
    }
    if (message.stderrStreamName !== "") {
      obj.stderrStreamName = message.stderrStreamName;
    }
    if (message.partialExecutionMetadata !== undefined) {
      obj.partialExecutionMetadata = ExecutedActionMetadata.toJSON(message.partialExecutionMetadata);
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteOperationMetadata>): ExecuteOperationMetadata {
    return ExecuteOperationMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteOperationMetadata>): ExecuteOperationMetadata {
    const message = createBaseExecuteOperationMetadata();
    message.stage = object.stage ?? 0;
    message.actionDigest = (object.actionDigest !== undefined && object.actionDigest !== null)
      ? Digest.fromPartial(object.actionDigest)
      : undefined;
    message.stdoutStreamName = object.stdoutStreamName ?? "";
    message.stderrStreamName = object.stderrStreamName ?? "";
    message.partialExecutionMetadata =
      (object.partialExecutionMetadata !== undefined && object.partialExecutionMetadata !== null)
        ? ExecutedActionMetadata.fromPartial(object.partialExecutionMetadata)
        : undefined;
    message.digestFunction = object.digestFunction ?? 0;
    return message;
  },
};

function createBaseWaitExecutionRequest(): WaitExecutionRequest {
  return { name: "" };
}

export const WaitExecutionRequest: MessageFns<WaitExecutionRequest> = {
  encode(message: WaitExecutionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WaitExecutionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWaitExecutionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WaitExecutionRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: WaitExecutionRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<WaitExecutionRequest>): WaitExecutionRequest {
    return WaitExecutionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WaitExecutionRequest>): WaitExecutionRequest {
    const message = createBaseWaitExecutionRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseGetActionResultRequest(): GetActionResultRequest {
  return {
    instanceName: "",
    actionDigest: undefined,
    inlineStdout: false,
    inlineStderr: false,
    inlineOutputFiles: [],
    digestFunction: 0,
  };
}

export const GetActionResultRequest: MessageFns<GetActionResultRequest> = {
  encode(message: GetActionResultRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    if (message.actionDigest !== undefined) {
      Digest.encode(message.actionDigest, writer.uint32(18).fork()).join();
    }
    if (message.inlineStdout !== false) {
      writer.uint32(24).bool(message.inlineStdout);
    }
    if (message.inlineStderr !== false) {
      writer.uint32(32).bool(message.inlineStderr);
    }
    for (const v of message.inlineOutputFiles) {
      writer.uint32(42).string(v!);
    }
    if (message.digestFunction !== 0) {
      writer.uint32(48).int32(message.digestFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetActionResultRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetActionResultRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.actionDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.inlineStdout = reader.bool();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.inlineStderr = reader.bool();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.inlineOutputFiles.push(reader.string());
          continue;
        }
        case 6: {
          if (tag !== 48) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetActionResultRequest {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      actionDigest: isSet(object.actionDigest) ? Digest.fromJSON(object.actionDigest) : undefined,
      inlineStdout: isSet(object.inlineStdout) ? globalThis.Boolean(object.inlineStdout) : false,
      inlineStderr: isSet(object.inlineStderr) ? globalThis.Boolean(object.inlineStderr) : false,
      inlineOutputFiles: globalThis.Array.isArray(object?.inlineOutputFiles)
        ? object.inlineOutputFiles.map((e: any) => globalThis.String(e))
        : [],
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
    };
  },

  toJSON(message: GetActionResultRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.actionDigest !== undefined) {
      obj.actionDigest = Digest.toJSON(message.actionDigest);
    }
    if (message.inlineStdout !== false) {
      obj.inlineStdout = message.inlineStdout;
    }
    if (message.inlineStderr !== false) {
      obj.inlineStderr = message.inlineStderr;
    }
    if (message.inlineOutputFiles?.length) {
      obj.inlineOutputFiles = message.inlineOutputFiles;
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    return obj;
  },

  create(base?: DeepPartial<GetActionResultRequest>): GetActionResultRequest {
    return GetActionResultRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetActionResultRequest>): GetActionResultRequest {
    const message = createBaseGetActionResultRequest();
    message.instanceName = object.instanceName ?? "";
    message.actionDigest = (object.actionDigest !== undefined && object.actionDigest !== null)
      ? Digest.fromPartial(object.actionDigest)
      : undefined;
    message.inlineStdout = object.inlineStdout ?? false;
    message.inlineStderr = object.inlineStderr ?? false;
    message.inlineOutputFiles = object.inlineOutputFiles?.map((e) => e) || [];
    message.digestFunction = object.digestFunction ?? 0;
    return message;
  },
};

function createBaseUpdateActionResultRequest(): UpdateActionResultRequest {
  return {
    instanceName: "",
    actionDigest: undefined,
    actionResult: undefined,
    resultsCachePolicy: undefined,
    digestFunction: 0,
  };
}

export const UpdateActionResultRequest: MessageFns<UpdateActionResultRequest> = {
  encode(message: UpdateActionResultRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    if (message.actionDigest !== undefined) {
      Digest.encode(message.actionDigest, writer.uint32(18).fork()).join();
    }
    if (message.actionResult !== undefined) {
      ActionResult.encode(message.actionResult, writer.uint32(26).fork()).join();
    }
    if (message.resultsCachePolicy !== undefined) {
      ResultsCachePolicy.encode(message.resultsCachePolicy, writer.uint32(34).fork()).join();
    }
    if (message.digestFunction !== 0) {
      writer.uint32(40).int32(message.digestFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateActionResultRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateActionResultRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.actionDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.actionResult = ActionResult.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.resultsCachePolicy = ResultsCachePolicy.decode(reader, reader.uint32());
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateActionResultRequest {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      actionDigest: isSet(object.actionDigest) ? Digest.fromJSON(object.actionDigest) : undefined,
      actionResult: isSet(object.actionResult) ? ActionResult.fromJSON(object.actionResult) : undefined,
      resultsCachePolicy: isSet(object.resultsCachePolicy)
        ? ResultsCachePolicy.fromJSON(object.resultsCachePolicy)
        : undefined,
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
    };
  },

  toJSON(message: UpdateActionResultRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.actionDigest !== undefined) {
      obj.actionDigest = Digest.toJSON(message.actionDigest);
    }
    if (message.actionResult !== undefined) {
      obj.actionResult = ActionResult.toJSON(message.actionResult);
    }
    if (message.resultsCachePolicy !== undefined) {
      obj.resultsCachePolicy = ResultsCachePolicy.toJSON(message.resultsCachePolicy);
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateActionResultRequest>): UpdateActionResultRequest {
    return UpdateActionResultRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateActionResultRequest>): UpdateActionResultRequest {
    const message = createBaseUpdateActionResultRequest();
    message.instanceName = object.instanceName ?? "";
    message.actionDigest = (object.actionDigest !== undefined && object.actionDigest !== null)
      ? Digest.fromPartial(object.actionDigest)
      : undefined;
    message.actionResult = (object.actionResult !== undefined && object.actionResult !== null)
      ? ActionResult.fromPartial(object.actionResult)
      : undefined;
    message.resultsCachePolicy = (object.resultsCachePolicy !== undefined && object.resultsCachePolicy !== null)
      ? ResultsCachePolicy.fromPartial(object.resultsCachePolicy)
      : undefined;
    message.digestFunction = object.digestFunction ?? 0;
    return message;
  },
};

function createBaseFindMissingBlobsRequest(): FindMissingBlobsRequest {
  return { instanceName: "", blobDigests: [], digestFunction: 0 };
}

export const FindMissingBlobsRequest: MessageFns<FindMissingBlobsRequest> = {
  encode(message: FindMissingBlobsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    for (const v of message.blobDigests) {
      Digest.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.digestFunction !== 0) {
      writer.uint32(24).int32(message.digestFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FindMissingBlobsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFindMissingBlobsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.blobDigests.push(Digest.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FindMissingBlobsRequest {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      blobDigests: globalThis.Array.isArray(object?.blobDigests)
        ? object.blobDigests.map((e: any) => Digest.fromJSON(e))
        : [],
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
    };
  },

  toJSON(message: FindMissingBlobsRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.blobDigests?.length) {
      obj.blobDigests = message.blobDigests.map((e) => Digest.toJSON(e));
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    return obj;
  },

  create(base?: DeepPartial<FindMissingBlobsRequest>): FindMissingBlobsRequest {
    return FindMissingBlobsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FindMissingBlobsRequest>): FindMissingBlobsRequest {
    const message = createBaseFindMissingBlobsRequest();
    message.instanceName = object.instanceName ?? "";
    message.blobDigests = object.blobDigests?.map((e) => Digest.fromPartial(e)) || [];
    message.digestFunction = object.digestFunction ?? 0;
    return message;
  },
};

function createBaseFindMissingBlobsResponse(): FindMissingBlobsResponse {
  return { missingBlobDigests: [] };
}

export const FindMissingBlobsResponse: MessageFns<FindMissingBlobsResponse> = {
  encode(message: FindMissingBlobsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.missingBlobDigests) {
      Digest.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FindMissingBlobsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFindMissingBlobsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.missingBlobDigests.push(Digest.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FindMissingBlobsResponse {
    return {
      missingBlobDigests: globalThis.Array.isArray(object?.missingBlobDigests)
        ? object.missingBlobDigests.map((e: any) => Digest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: FindMissingBlobsResponse): unknown {
    const obj: any = {};
    if (message.missingBlobDigests?.length) {
      obj.missingBlobDigests = message.missingBlobDigests.map((e) => Digest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<FindMissingBlobsResponse>): FindMissingBlobsResponse {
    return FindMissingBlobsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FindMissingBlobsResponse>): FindMissingBlobsResponse {
    const message = createBaseFindMissingBlobsResponse();
    message.missingBlobDigests = object.missingBlobDigests?.map((e) => Digest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchUpdateBlobsRequest(): BatchUpdateBlobsRequest {
  return { instanceName: "", requests: [], digestFunction: 0 };
}

export const BatchUpdateBlobsRequest: MessageFns<BatchUpdateBlobsRequest> = {
  encode(message: BatchUpdateBlobsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    for (const v of message.requests) {
      BatchUpdateBlobsRequest_Request.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.digestFunction !== 0) {
      writer.uint32(40).int32(message.digestFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchUpdateBlobsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchUpdateBlobsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.requests.push(BatchUpdateBlobsRequest_Request.decode(reader, reader.uint32()));
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchUpdateBlobsRequest {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => BatchUpdateBlobsRequest_Request.fromJSON(e))
        : [],
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
    };
  },

  toJSON(message: BatchUpdateBlobsRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => BatchUpdateBlobsRequest_Request.toJSON(e));
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchUpdateBlobsRequest>): BatchUpdateBlobsRequest {
    return BatchUpdateBlobsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchUpdateBlobsRequest>): BatchUpdateBlobsRequest {
    const message = createBaseBatchUpdateBlobsRequest();
    message.instanceName = object.instanceName ?? "";
    message.requests = object.requests?.map((e) => BatchUpdateBlobsRequest_Request.fromPartial(e)) || [];
    message.digestFunction = object.digestFunction ?? 0;
    return message;
  },
};

function createBaseBatchUpdateBlobsRequest_Request(): BatchUpdateBlobsRequest_Request {
  return { digest: undefined, data: new Uint8Array(0), compressor: 0 };
}

export const BatchUpdateBlobsRequest_Request: MessageFns<BatchUpdateBlobsRequest_Request> = {
  encode(message: BatchUpdateBlobsRequest_Request, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.digest !== undefined) {
      Digest.encode(message.digest, writer.uint32(10).fork()).join();
    }
    if (message.data.length !== 0) {
      writer.uint32(18).bytes(message.data);
    }
    if (message.compressor !== 0) {
      writer.uint32(24).int32(message.compressor);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchUpdateBlobsRequest_Request {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchUpdateBlobsRequest_Request();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.digest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.data = reader.bytes();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.compressor = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchUpdateBlobsRequest_Request {
    return {
      digest: isSet(object.digest) ? Digest.fromJSON(object.digest) : undefined,
      data: isSet(object.data) ? bytesFromBase64(object.data) : new Uint8Array(0),
      compressor: isSet(object.compressor) ? compressor_ValueFromJSON(object.compressor) : 0,
    };
  },

  toJSON(message: BatchUpdateBlobsRequest_Request): unknown {
    const obj: any = {};
    if (message.digest !== undefined) {
      obj.digest = Digest.toJSON(message.digest);
    }
    if (message.data.length !== 0) {
      obj.data = base64FromBytes(message.data);
    }
    if (message.compressor !== 0) {
      obj.compressor = compressor_ValueToJSON(message.compressor);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchUpdateBlobsRequest_Request>): BatchUpdateBlobsRequest_Request {
    return BatchUpdateBlobsRequest_Request.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchUpdateBlobsRequest_Request>): BatchUpdateBlobsRequest_Request {
    const message = createBaseBatchUpdateBlobsRequest_Request();
    message.digest = (object.digest !== undefined && object.digest !== null)
      ? Digest.fromPartial(object.digest)
      : undefined;
    message.data = object.data ?? new Uint8Array(0);
    message.compressor = object.compressor ?? 0;
    return message;
  },
};

function createBaseBatchUpdateBlobsResponse(): BatchUpdateBlobsResponse {
  return { responses: [] };
}

export const BatchUpdateBlobsResponse: MessageFns<BatchUpdateBlobsResponse> = {
  encode(message: BatchUpdateBlobsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.responses) {
      BatchUpdateBlobsResponse_Response.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchUpdateBlobsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchUpdateBlobsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.responses.push(BatchUpdateBlobsResponse_Response.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchUpdateBlobsResponse {
    return {
      responses: globalThis.Array.isArray(object?.responses)
        ? object.responses.map((e: any) => BatchUpdateBlobsResponse_Response.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchUpdateBlobsResponse): unknown {
    const obj: any = {};
    if (message.responses?.length) {
      obj.responses = message.responses.map((e) => BatchUpdateBlobsResponse_Response.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchUpdateBlobsResponse>): BatchUpdateBlobsResponse {
    return BatchUpdateBlobsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchUpdateBlobsResponse>): BatchUpdateBlobsResponse {
    const message = createBaseBatchUpdateBlobsResponse();
    message.responses = object.responses?.map((e) => BatchUpdateBlobsResponse_Response.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchUpdateBlobsResponse_Response(): BatchUpdateBlobsResponse_Response {
  return { digest: undefined, status: undefined };
}

export const BatchUpdateBlobsResponse_Response: MessageFns<BatchUpdateBlobsResponse_Response> = {
  encode(message: BatchUpdateBlobsResponse_Response, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.digest !== undefined) {
      Digest.encode(message.digest, writer.uint32(10).fork()).join();
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchUpdateBlobsResponse_Response {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchUpdateBlobsResponse_Response();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.digest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchUpdateBlobsResponse_Response {
    return {
      digest: isSet(object.digest) ? Digest.fromJSON(object.digest) : undefined,
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
    };
  },

  toJSON(message: BatchUpdateBlobsResponse_Response): unknown {
    const obj: any = {};
    if (message.digest !== undefined) {
      obj.digest = Digest.toJSON(message.digest);
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchUpdateBlobsResponse_Response>): BatchUpdateBlobsResponse_Response {
    return BatchUpdateBlobsResponse_Response.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchUpdateBlobsResponse_Response>): BatchUpdateBlobsResponse_Response {
    const message = createBaseBatchUpdateBlobsResponse_Response();
    message.digest = (object.digest !== undefined && object.digest !== null)
      ? Digest.fromPartial(object.digest)
      : undefined;
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    return message;
  },
};

function createBaseBatchReadBlobsRequest(): BatchReadBlobsRequest {
  return { instanceName: "", digests: [], acceptableCompressors: [], digestFunction: 0 };
}

export const BatchReadBlobsRequest: MessageFns<BatchReadBlobsRequest> = {
  encode(message: BatchReadBlobsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    for (const v of message.digests) {
      Digest.encode(v!, writer.uint32(18).fork()).join();
    }
    writer.uint32(26).fork();
    for (const v of message.acceptableCompressors) {
      writer.int32(v);
    }
    writer.join();
    if (message.digestFunction !== 0) {
      writer.uint32(32).int32(message.digestFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchReadBlobsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchReadBlobsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.digests.push(Digest.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag === 24) {
            message.acceptableCompressors.push(reader.int32() as any);

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.acceptableCompressors.push(reader.int32() as any);
            }

            continue;
          }

          break;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchReadBlobsRequest {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      digests: globalThis.Array.isArray(object?.digests) ? object.digests.map((e: any) => Digest.fromJSON(e)) : [],
      acceptableCompressors: globalThis.Array.isArray(object?.acceptableCompressors)
        ? object.acceptableCompressors.map((e: any) => compressor_ValueFromJSON(e))
        : [],
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
    };
  },

  toJSON(message: BatchReadBlobsRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.digests?.length) {
      obj.digests = message.digests.map((e) => Digest.toJSON(e));
    }
    if (message.acceptableCompressors?.length) {
      obj.acceptableCompressors = message.acceptableCompressors.map((e) => compressor_ValueToJSON(e));
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchReadBlobsRequest>): BatchReadBlobsRequest {
    return BatchReadBlobsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchReadBlobsRequest>): BatchReadBlobsRequest {
    const message = createBaseBatchReadBlobsRequest();
    message.instanceName = object.instanceName ?? "";
    message.digests = object.digests?.map((e) => Digest.fromPartial(e)) || [];
    message.acceptableCompressors = object.acceptableCompressors?.map((e) => e) || [];
    message.digestFunction = object.digestFunction ?? 0;
    return message;
  },
};

function createBaseBatchReadBlobsResponse(): BatchReadBlobsResponse {
  return { responses: [] };
}

export const BatchReadBlobsResponse: MessageFns<BatchReadBlobsResponse> = {
  encode(message: BatchReadBlobsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.responses) {
      BatchReadBlobsResponse_Response.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchReadBlobsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchReadBlobsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.responses.push(BatchReadBlobsResponse_Response.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchReadBlobsResponse {
    return {
      responses: globalThis.Array.isArray(object?.responses)
        ? object.responses.map((e: any) => BatchReadBlobsResponse_Response.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchReadBlobsResponse): unknown {
    const obj: any = {};
    if (message.responses?.length) {
      obj.responses = message.responses.map((e) => BatchReadBlobsResponse_Response.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchReadBlobsResponse>): BatchReadBlobsResponse {
    return BatchReadBlobsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchReadBlobsResponse>): BatchReadBlobsResponse {
    const message = createBaseBatchReadBlobsResponse();
    message.responses = object.responses?.map((e) => BatchReadBlobsResponse_Response.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchReadBlobsResponse_Response(): BatchReadBlobsResponse_Response {
  return { digest: undefined, data: new Uint8Array(0), compressor: 0, status: undefined };
}

export const BatchReadBlobsResponse_Response: MessageFns<BatchReadBlobsResponse_Response> = {
  encode(message: BatchReadBlobsResponse_Response, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.digest !== undefined) {
      Digest.encode(message.digest, writer.uint32(10).fork()).join();
    }
    if (message.data.length !== 0) {
      writer.uint32(18).bytes(message.data);
    }
    if (message.compressor !== 0) {
      writer.uint32(32).int32(message.compressor);
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchReadBlobsResponse_Response {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchReadBlobsResponse_Response();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.digest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.data = reader.bytes();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.compressor = reader.int32() as any;
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchReadBlobsResponse_Response {
    return {
      digest: isSet(object.digest) ? Digest.fromJSON(object.digest) : undefined,
      data: isSet(object.data) ? bytesFromBase64(object.data) : new Uint8Array(0),
      compressor: isSet(object.compressor) ? compressor_ValueFromJSON(object.compressor) : 0,
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
    };
  },

  toJSON(message: BatchReadBlobsResponse_Response): unknown {
    const obj: any = {};
    if (message.digest !== undefined) {
      obj.digest = Digest.toJSON(message.digest);
    }
    if (message.data.length !== 0) {
      obj.data = base64FromBytes(message.data);
    }
    if (message.compressor !== 0) {
      obj.compressor = compressor_ValueToJSON(message.compressor);
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchReadBlobsResponse_Response>): BatchReadBlobsResponse_Response {
    return BatchReadBlobsResponse_Response.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchReadBlobsResponse_Response>): BatchReadBlobsResponse_Response {
    const message = createBaseBatchReadBlobsResponse_Response();
    message.digest = (object.digest !== undefined && object.digest !== null)
      ? Digest.fromPartial(object.digest)
      : undefined;
    message.data = object.data ?? new Uint8Array(0);
    message.compressor = object.compressor ?? 0;
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    return message;
  },
};

function createBaseGetTreeRequest(): GetTreeRequest {
  return { instanceName: "", rootDigest: undefined, pageSize: 0, pageToken: "", digestFunction: 0 };
}

export const GetTreeRequest: MessageFns<GetTreeRequest> = {
  encode(message: GetTreeRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    if (message.rootDigest !== undefined) {
      Digest.encode(message.rootDigest, writer.uint32(18).fork()).join();
    }
    if (message.pageSize !== 0) {
      writer.uint32(24).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(34).string(message.pageToken);
    }
    if (message.digestFunction !== 0) {
      writer.uint32(40).int32(message.digestFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetTreeRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetTreeRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.rootDigest = Digest.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetTreeRequest {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      rootDigest: isSet(object.rootDigest) ? Digest.fromJSON(object.rootDigest) : undefined,
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
    };
  },

  toJSON(message: GetTreeRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.rootDigest !== undefined) {
      obj.rootDigest = Digest.toJSON(message.rootDigest);
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    return obj;
  },

  create(base?: DeepPartial<GetTreeRequest>): GetTreeRequest {
    return GetTreeRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetTreeRequest>): GetTreeRequest {
    const message = createBaseGetTreeRequest();
    message.instanceName = object.instanceName ?? "";
    message.rootDigest = (object.rootDigest !== undefined && object.rootDigest !== null)
      ? Digest.fromPartial(object.rootDigest)
      : undefined;
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.digestFunction = object.digestFunction ?? 0;
    return message;
  },
};

function createBaseGetTreeResponse(): GetTreeResponse {
  return { directories: [], nextPageToken: "" };
}

export const GetTreeResponse: MessageFns<GetTreeResponse> = {
  encode(message: GetTreeResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.directories) {
      Directory.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetTreeResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetTreeResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.directories.push(Directory.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetTreeResponse {
    return {
      directories: globalThis.Array.isArray(object?.directories)
        ? object.directories.map((e: any) => Directory.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: GetTreeResponse): unknown {
    const obj: any = {};
    if (message.directories?.length) {
      obj.directories = message.directories.map((e) => Directory.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<GetTreeResponse>): GetTreeResponse {
    return GetTreeResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetTreeResponse>): GetTreeResponse {
    const message = createBaseGetTreeResponse();
    message.directories = object.directories?.map((e) => Directory.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseGetCapabilitiesRequest(): GetCapabilitiesRequest {
  return { instanceName: "" };
}

export const GetCapabilitiesRequest: MessageFns<GetCapabilitiesRequest> = {
  encode(message: GetCapabilitiesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetCapabilitiesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetCapabilitiesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetCapabilitiesRequest {
    return { instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "" };
  },

  toJSON(message: GetCapabilitiesRequest): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    return obj;
  },

  create(base?: DeepPartial<GetCapabilitiesRequest>): GetCapabilitiesRequest {
    return GetCapabilitiesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetCapabilitiesRequest>): GetCapabilitiesRequest {
    const message = createBaseGetCapabilitiesRequest();
    message.instanceName = object.instanceName ?? "";
    return message;
  },
};

function createBaseServerCapabilities(): ServerCapabilities {
  return {
    cacheCapabilities: undefined,
    executionCapabilities: undefined,
    deprecatedApiVersion: undefined,
    lowApiVersion: undefined,
    highApiVersion: undefined,
  };
}

export const ServerCapabilities: MessageFns<ServerCapabilities> = {
  encode(message: ServerCapabilities, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cacheCapabilities !== undefined) {
      CacheCapabilities.encode(message.cacheCapabilities, writer.uint32(10).fork()).join();
    }
    if (message.executionCapabilities !== undefined) {
      ExecutionCapabilities.encode(message.executionCapabilities, writer.uint32(18).fork()).join();
    }
    if (message.deprecatedApiVersion !== undefined) {
      SemVer.encode(message.deprecatedApiVersion, writer.uint32(26).fork()).join();
    }
    if (message.lowApiVersion !== undefined) {
      SemVer.encode(message.lowApiVersion, writer.uint32(34).fork()).join();
    }
    if (message.highApiVersion !== undefined) {
      SemVer.encode(message.highApiVersion, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ServerCapabilities {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseServerCapabilities();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.cacheCapabilities = CacheCapabilities.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.executionCapabilities = ExecutionCapabilities.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.deprecatedApiVersion = SemVer.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.lowApiVersion = SemVer.decode(reader, reader.uint32());
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.highApiVersion = SemVer.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ServerCapabilities {
    return {
      cacheCapabilities: isSet(object.cacheCapabilities)
        ? CacheCapabilities.fromJSON(object.cacheCapabilities)
        : undefined,
      executionCapabilities: isSet(object.executionCapabilities)
        ? ExecutionCapabilities.fromJSON(object.executionCapabilities)
        : undefined,
      deprecatedApiVersion: isSet(object.deprecatedApiVersion)
        ? SemVer.fromJSON(object.deprecatedApiVersion)
        : undefined,
      lowApiVersion: isSet(object.lowApiVersion) ? SemVer.fromJSON(object.lowApiVersion) : undefined,
      highApiVersion: isSet(object.highApiVersion) ? SemVer.fromJSON(object.highApiVersion) : undefined,
    };
  },

  toJSON(message: ServerCapabilities): unknown {
    const obj: any = {};
    if (message.cacheCapabilities !== undefined) {
      obj.cacheCapabilities = CacheCapabilities.toJSON(message.cacheCapabilities);
    }
    if (message.executionCapabilities !== undefined) {
      obj.executionCapabilities = ExecutionCapabilities.toJSON(message.executionCapabilities);
    }
    if (message.deprecatedApiVersion !== undefined) {
      obj.deprecatedApiVersion = SemVer.toJSON(message.deprecatedApiVersion);
    }
    if (message.lowApiVersion !== undefined) {
      obj.lowApiVersion = SemVer.toJSON(message.lowApiVersion);
    }
    if (message.highApiVersion !== undefined) {
      obj.highApiVersion = SemVer.toJSON(message.highApiVersion);
    }
    return obj;
  },

  create(base?: DeepPartial<ServerCapabilities>): ServerCapabilities {
    return ServerCapabilities.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ServerCapabilities>): ServerCapabilities {
    const message = createBaseServerCapabilities();
    message.cacheCapabilities = (object.cacheCapabilities !== undefined && object.cacheCapabilities !== null)
      ? CacheCapabilities.fromPartial(object.cacheCapabilities)
      : undefined;
    message.executionCapabilities =
      (object.executionCapabilities !== undefined && object.executionCapabilities !== null)
        ? ExecutionCapabilities.fromPartial(object.executionCapabilities)
        : undefined;
    message.deprecatedApiVersion = (object.deprecatedApiVersion !== undefined && object.deprecatedApiVersion !== null)
      ? SemVer.fromPartial(object.deprecatedApiVersion)
      : undefined;
    message.lowApiVersion = (object.lowApiVersion !== undefined && object.lowApiVersion !== null)
      ? SemVer.fromPartial(object.lowApiVersion)
      : undefined;
    message.highApiVersion = (object.highApiVersion !== undefined && object.highApiVersion !== null)
      ? SemVer.fromPartial(object.highApiVersion)
      : undefined;
    return message;
  },
};

function createBaseDigestFunction(): DigestFunction {
  return {};
}

export const DigestFunction: MessageFns<DigestFunction> = {
  encode(_: DigestFunction, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DigestFunction {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDigestFunction();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): DigestFunction {
    return {};
  },

  toJSON(_: DigestFunction): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<DigestFunction>): DigestFunction {
    return DigestFunction.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<DigestFunction>): DigestFunction {
    const message = createBaseDigestFunction();
    return message;
  },
};

function createBaseActionCacheUpdateCapabilities(): ActionCacheUpdateCapabilities {
  return { updateEnabled: false };
}

export const ActionCacheUpdateCapabilities: MessageFns<ActionCacheUpdateCapabilities> = {
  encode(message: ActionCacheUpdateCapabilities, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.updateEnabled !== false) {
      writer.uint32(8).bool(message.updateEnabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ActionCacheUpdateCapabilities {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseActionCacheUpdateCapabilities();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.updateEnabled = reader.bool();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ActionCacheUpdateCapabilities {
    return { updateEnabled: isSet(object.updateEnabled) ? globalThis.Boolean(object.updateEnabled) : false };
  },

  toJSON(message: ActionCacheUpdateCapabilities): unknown {
    const obj: any = {};
    if (message.updateEnabled !== false) {
      obj.updateEnabled = message.updateEnabled;
    }
    return obj;
  },

  create(base?: DeepPartial<ActionCacheUpdateCapabilities>): ActionCacheUpdateCapabilities {
    return ActionCacheUpdateCapabilities.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ActionCacheUpdateCapabilities>): ActionCacheUpdateCapabilities {
    const message = createBaseActionCacheUpdateCapabilities();
    message.updateEnabled = object.updateEnabled ?? false;
    return message;
  },
};

function createBasePriorityCapabilities(): PriorityCapabilities {
  return { priorities: [] };
}

export const PriorityCapabilities: MessageFns<PriorityCapabilities> = {
  encode(message: PriorityCapabilities, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.priorities) {
      PriorityCapabilities_PriorityRange.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PriorityCapabilities {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePriorityCapabilities();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.priorities.push(PriorityCapabilities_PriorityRange.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PriorityCapabilities {
    return {
      priorities: globalThis.Array.isArray(object?.priorities)
        ? object.priorities.map((e: any) => PriorityCapabilities_PriorityRange.fromJSON(e))
        : [],
    };
  },

  toJSON(message: PriorityCapabilities): unknown {
    const obj: any = {};
    if (message.priorities?.length) {
      obj.priorities = message.priorities.map((e) => PriorityCapabilities_PriorityRange.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<PriorityCapabilities>): PriorityCapabilities {
    return PriorityCapabilities.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PriorityCapabilities>): PriorityCapabilities {
    const message = createBasePriorityCapabilities();
    message.priorities = object.priorities?.map((e) => PriorityCapabilities_PriorityRange.fromPartial(e)) || [];
    return message;
  },
};

function createBasePriorityCapabilities_PriorityRange(): PriorityCapabilities_PriorityRange {
  return { minPriority: 0, maxPriority: 0 };
}

export const PriorityCapabilities_PriorityRange: MessageFns<PriorityCapabilities_PriorityRange> = {
  encode(message: PriorityCapabilities_PriorityRange, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minPriority !== 0) {
      writer.uint32(8).int32(message.minPriority);
    }
    if (message.maxPriority !== 0) {
      writer.uint32(16).int32(message.maxPriority);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PriorityCapabilities_PriorityRange {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePriorityCapabilities_PriorityRange();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.minPriority = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.maxPriority = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PriorityCapabilities_PriorityRange {
    return {
      minPriority: isSet(object.minPriority) ? globalThis.Number(object.minPriority) : 0,
      maxPriority: isSet(object.maxPriority) ? globalThis.Number(object.maxPriority) : 0,
    };
  },

  toJSON(message: PriorityCapabilities_PriorityRange): unknown {
    const obj: any = {};
    if (message.minPriority !== 0) {
      obj.minPriority = Math.round(message.minPriority);
    }
    if (message.maxPriority !== 0) {
      obj.maxPriority = Math.round(message.maxPriority);
    }
    return obj;
  },

  create(base?: DeepPartial<PriorityCapabilities_PriorityRange>): PriorityCapabilities_PriorityRange {
    return PriorityCapabilities_PriorityRange.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PriorityCapabilities_PriorityRange>): PriorityCapabilities_PriorityRange {
    const message = createBasePriorityCapabilities_PriorityRange();
    message.minPriority = object.minPriority ?? 0;
    message.maxPriority = object.maxPriority ?? 0;
    return message;
  },
};

function createBaseSymlinkAbsolutePathStrategy(): SymlinkAbsolutePathStrategy {
  return {};
}

export const SymlinkAbsolutePathStrategy: MessageFns<SymlinkAbsolutePathStrategy> = {
  encode(_: SymlinkAbsolutePathStrategy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SymlinkAbsolutePathStrategy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSymlinkAbsolutePathStrategy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): SymlinkAbsolutePathStrategy {
    return {};
  },

  toJSON(_: SymlinkAbsolutePathStrategy): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<SymlinkAbsolutePathStrategy>): SymlinkAbsolutePathStrategy {
    return SymlinkAbsolutePathStrategy.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<SymlinkAbsolutePathStrategy>): SymlinkAbsolutePathStrategy {
    const message = createBaseSymlinkAbsolutePathStrategy();
    return message;
  },
};

function createBaseCompressor(): Compressor {
  return {};
}

export const Compressor: MessageFns<Compressor> = {
  encode(_: Compressor, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Compressor {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCompressor();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): Compressor {
    return {};
  },

  toJSON(_: Compressor): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<Compressor>): Compressor {
    return Compressor.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<Compressor>): Compressor {
    const message = createBaseCompressor();
    return message;
  },
};

function createBaseCacheCapabilities(): CacheCapabilities {
  return {
    digestFunctions: [],
    actionCacheUpdateCapabilities: undefined,
    cachePriorityCapabilities: undefined,
    maxBatchTotalSizeBytes: "0",
    symlinkAbsolutePathStrategy: 0,
    supportedCompressors: [],
    supportedBatchUpdateCompressors: [],
    maxCasBlobSizeBytes: "0",
  };
}

export const CacheCapabilities: MessageFns<CacheCapabilities> = {
  encode(message: CacheCapabilities, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.digestFunctions) {
      writer.int32(v);
    }
    writer.join();
    if (message.actionCacheUpdateCapabilities !== undefined) {
      ActionCacheUpdateCapabilities.encode(message.actionCacheUpdateCapabilities, writer.uint32(18).fork()).join();
    }
    if (message.cachePriorityCapabilities !== undefined) {
      PriorityCapabilities.encode(message.cachePriorityCapabilities, writer.uint32(26).fork()).join();
    }
    if (message.maxBatchTotalSizeBytes !== "0") {
      writer.uint32(32).int64(message.maxBatchTotalSizeBytes);
    }
    if (message.symlinkAbsolutePathStrategy !== 0) {
      writer.uint32(40).int32(message.symlinkAbsolutePathStrategy);
    }
    writer.uint32(50).fork();
    for (const v of message.supportedCompressors) {
      writer.int32(v);
    }
    writer.join();
    writer.uint32(58).fork();
    for (const v of message.supportedBatchUpdateCompressors) {
      writer.int32(v);
    }
    writer.join();
    if (message.maxCasBlobSizeBytes !== "0") {
      writer.uint32(64).int64(message.maxCasBlobSizeBytes);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CacheCapabilities {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCacheCapabilities();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag === 8) {
            message.digestFunctions.push(reader.int32() as any);

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.digestFunctions.push(reader.int32() as any);
            }

            continue;
          }

          break;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.actionCacheUpdateCapabilities = ActionCacheUpdateCapabilities.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.cachePriorityCapabilities = PriorityCapabilities.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.maxBatchTotalSizeBytes = reader.int64().toString();
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.symlinkAbsolutePathStrategy = reader.int32() as any;
          continue;
        }
        case 6: {
          if (tag === 48) {
            message.supportedCompressors.push(reader.int32() as any);

            continue;
          }

          if (tag === 50) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.supportedCompressors.push(reader.int32() as any);
            }

            continue;
          }

          break;
        }
        case 7: {
          if (tag === 56) {
            message.supportedBatchUpdateCompressors.push(reader.int32() as any);

            continue;
          }

          if (tag === 58) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.supportedBatchUpdateCompressors.push(reader.int32() as any);
            }

            continue;
          }

          break;
        }
        case 8: {
          if (tag !== 64) {
            break;
          }

          message.maxCasBlobSizeBytes = reader.int64().toString();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CacheCapabilities {
    return {
      digestFunctions: globalThis.Array.isArray(object?.digestFunctions)
        ? object.digestFunctions.map((e: any) => digestFunction_ValueFromJSON(e))
        : [],
      actionCacheUpdateCapabilities: isSet(object.actionCacheUpdateCapabilities)
        ? ActionCacheUpdateCapabilities.fromJSON(object.actionCacheUpdateCapabilities)
        : undefined,
      cachePriorityCapabilities: isSet(object.cachePriorityCapabilities)
        ? PriorityCapabilities.fromJSON(object.cachePriorityCapabilities)
        : undefined,
      maxBatchTotalSizeBytes: isSet(object.maxBatchTotalSizeBytes)
        ? globalThis.String(object.maxBatchTotalSizeBytes)
        : "0",
      symlinkAbsolutePathStrategy: isSet(object.symlinkAbsolutePathStrategy)
        ? symlinkAbsolutePathStrategy_ValueFromJSON(object.symlinkAbsolutePathStrategy)
        : 0,
      supportedCompressors: globalThis.Array.isArray(object?.supportedCompressors)
        ? object.supportedCompressors.map((e: any) => compressor_ValueFromJSON(e))
        : [],
      supportedBatchUpdateCompressors: globalThis.Array.isArray(object?.supportedBatchUpdateCompressors)
        ? object.supportedBatchUpdateCompressors.map((e: any) => compressor_ValueFromJSON(e))
        : [],
      maxCasBlobSizeBytes: isSet(object.maxCasBlobSizeBytes) ? globalThis.String(object.maxCasBlobSizeBytes) : "0",
    };
  },

  toJSON(message: CacheCapabilities): unknown {
    const obj: any = {};
    if (message.digestFunctions?.length) {
      obj.digestFunctions = message.digestFunctions.map((e) => digestFunction_ValueToJSON(e));
    }
    if (message.actionCacheUpdateCapabilities !== undefined) {
      obj.actionCacheUpdateCapabilities = ActionCacheUpdateCapabilities.toJSON(message.actionCacheUpdateCapabilities);
    }
    if (message.cachePriorityCapabilities !== undefined) {
      obj.cachePriorityCapabilities = PriorityCapabilities.toJSON(message.cachePriorityCapabilities);
    }
    if (message.maxBatchTotalSizeBytes !== "0") {
      obj.maxBatchTotalSizeBytes = message.maxBatchTotalSizeBytes;
    }
    if (message.symlinkAbsolutePathStrategy !== 0) {
      obj.symlinkAbsolutePathStrategy = symlinkAbsolutePathStrategy_ValueToJSON(message.symlinkAbsolutePathStrategy);
    }
    if (message.supportedCompressors?.length) {
      obj.supportedCompressors = message.supportedCompressors.map((e) => compressor_ValueToJSON(e));
    }
    if (message.supportedBatchUpdateCompressors?.length) {
      obj.supportedBatchUpdateCompressors = message.supportedBatchUpdateCompressors.map((e) =>
        compressor_ValueToJSON(e)
      );
    }
    if (message.maxCasBlobSizeBytes !== "0") {
      obj.maxCasBlobSizeBytes = message.maxCasBlobSizeBytes;
    }
    return obj;
  },

  create(base?: DeepPartial<CacheCapabilities>): CacheCapabilities {
    return CacheCapabilities.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CacheCapabilities>): CacheCapabilities {
    const message = createBaseCacheCapabilities();
    message.digestFunctions = object.digestFunctions?.map((e) => e) || [];
    message.actionCacheUpdateCapabilities =
      (object.actionCacheUpdateCapabilities !== undefined && object.actionCacheUpdateCapabilities !== null)
        ? ActionCacheUpdateCapabilities.fromPartial(object.actionCacheUpdateCapabilities)
        : undefined;
    message.cachePriorityCapabilities =
      (object.cachePriorityCapabilities !== undefined && object.cachePriorityCapabilities !== null)
        ? PriorityCapabilities.fromPartial(object.cachePriorityCapabilities)
        : undefined;
    message.maxBatchTotalSizeBytes = object.maxBatchTotalSizeBytes ?? "0";
    message.symlinkAbsolutePathStrategy = object.symlinkAbsolutePathStrategy ?? 0;
    message.supportedCompressors = object.supportedCompressors?.map((e) => e) || [];
    message.supportedBatchUpdateCompressors = object.supportedBatchUpdateCompressors?.map((e) => e) || [];
    message.maxCasBlobSizeBytes = object.maxCasBlobSizeBytes ?? "0";
    return message;
  },
};

function createBaseExecutionCapabilities(): ExecutionCapabilities {
  return {
    digestFunction: 0,
    execEnabled: false,
    executionPriorityCapabilities: undefined,
    supportedNodeProperties: [],
    digestFunctions: [],
  };
}

export const ExecutionCapabilities: MessageFns<ExecutionCapabilities> = {
  encode(message: ExecutionCapabilities, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.digestFunction !== 0) {
      writer.uint32(8).int32(message.digestFunction);
    }
    if (message.execEnabled !== false) {
      writer.uint32(16).bool(message.execEnabled);
    }
    if (message.executionPriorityCapabilities !== undefined) {
      PriorityCapabilities.encode(message.executionPriorityCapabilities, writer.uint32(26).fork()).join();
    }
    for (const v of message.supportedNodeProperties) {
      writer.uint32(34).string(v!);
    }
    writer.uint32(42).fork();
    for (const v of message.digestFunctions) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionCapabilities {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionCapabilities();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.digestFunction = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.execEnabled = reader.bool();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.executionPriorityCapabilities = PriorityCapabilities.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.supportedNodeProperties.push(reader.string());
          continue;
        }
        case 5: {
          if (tag === 40) {
            message.digestFunctions.push(reader.int32() as any);

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.digestFunctions.push(reader.int32() as any);
            }

            continue;
          }

          break;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionCapabilities {
    return {
      digestFunction: isSet(object.digestFunction) ? digestFunction_ValueFromJSON(object.digestFunction) : 0,
      execEnabled: isSet(object.execEnabled) ? globalThis.Boolean(object.execEnabled) : false,
      executionPriorityCapabilities: isSet(object.executionPriorityCapabilities)
        ? PriorityCapabilities.fromJSON(object.executionPriorityCapabilities)
        : undefined,
      supportedNodeProperties: globalThis.Array.isArray(object?.supportedNodeProperties)
        ? object.supportedNodeProperties.map((e: any) => globalThis.String(e))
        : [],
      digestFunctions: globalThis.Array.isArray(object?.digestFunctions)
        ? object.digestFunctions.map((e: any) => digestFunction_ValueFromJSON(e))
        : [],
    };
  },

  toJSON(message: ExecutionCapabilities): unknown {
    const obj: any = {};
    if (message.digestFunction !== 0) {
      obj.digestFunction = digestFunction_ValueToJSON(message.digestFunction);
    }
    if (message.execEnabled !== false) {
      obj.execEnabled = message.execEnabled;
    }
    if (message.executionPriorityCapabilities !== undefined) {
      obj.executionPriorityCapabilities = PriorityCapabilities.toJSON(message.executionPriorityCapabilities);
    }
    if (message.supportedNodeProperties?.length) {
      obj.supportedNodeProperties = message.supportedNodeProperties;
    }
    if (message.digestFunctions?.length) {
      obj.digestFunctions = message.digestFunctions.map((e) => digestFunction_ValueToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionCapabilities>): ExecutionCapabilities {
    return ExecutionCapabilities.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionCapabilities>): ExecutionCapabilities {
    const message = createBaseExecutionCapabilities();
    message.digestFunction = object.digestFunction ?? 0;
    message.execEnabled = object.execEnabled ?? false;
    message.executionPriorityCapabilities =
      (object.executionPriorityCapabilities !== undefined && object.executionPriorityCapabilities !== null)
        ? PriorityCapabilities.fromPartial(object.executionPriorityCapabilities)
        : undefined;
    message.supportedNodeProperties = object.supportedNodeProperties?.map((e) => e) || [];
    message.digestFunctions = object.digestFunctions?.map((e) => e) || [];
    return message;
  },
};

function createBaseToolDetails(): ToolDetails {
  return { toolName: "", toolVersion: "" };
}

export const ToolDetails: MessageFns<ToolDetails> = {
  encode(message: ToolDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.toolName !== "") {
      writer.uint32(10).string(message.toolName);
    }
    if (message.toolVersion !== "") {
      writer.uint32(18).string(message.toolVersion);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ToolDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseToolDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.toolName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.toolVersion = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ToolDetails {
    return {
      toolName: isSet(object.toolName) ? globalThis.String(object.toolName) : "",
      toolVersion: isSet(object.toolVersion) ? globalThis.String(object.toolVersion) : "",
    };
  },

  toJSON(message: ToolDetails): unknown {
    const obj: any = {};
    if (message.toolName !== "") {
      obj.toolName = message.toolName;
    }
    if (message.toolVersion !== "") {
      obj.toolVersion = message.toolVersion;
    }
    return obj;
  },

  create(base?: DeepPartial<ToolDetails>): ToolDetails {
    return ToolDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ToolDetails>): ToolDetails {
    const message = createBaseToolDetails();
    message.toolName = object.toolName ?? "";
    message.toolVersion = object.toolVersion ?? "";
    return message;
  },
};

function createBaseRequestMetadata(): RequestMetadata {
  return {
    toolDetails: undefined,
    actionId: "",
    toolInvocationId: "",
    correlatedInvocationsId: "",
    actionMnemonic: "",
    targetId: "",
    configurationId: "",
  };
}

export const RequestMetadata: MessageFns<RequestMetadata> = {
  encode(message: RequestMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.toolDetails !== undefined) {
      ToolDetails.encode(message.toolDetails, writer.uint32(10).fork()).join();
    }
    if (message.actionId !== "") {
      writer.uint32(18).string(message.actionId);
    }
    if (message.toolInvocationId !== "") {
      writer.uint32(26).string(message.toolInvocationId);
    }
    if (message.correlatedInvocationsId !== "") {
      writer.uint32(34).string(message.correlatedInvocationsId);
    }
    if (message.actionMnemonic !== "") {
      writer.uint32(42).string(message.actionMnemonic);
    }
    if (message.targetId !== "") {
      writer.uint32(50).string(message.targetId);
    }
    if (message.configurationId !== "") {
      writer.uint32(58).string(message.configurationId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RequestMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRequestMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.toolDetails = ToolDetails.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.actionId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.toolInvocationId = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.correlatedInvocationsId = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.actionMnemonic = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.targetId = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.configurationId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RequestMetadata {
    return {
      toolDetails: isSet(object.toolDetails) ? ToolDetails.fromJSON(object.toolDetails) : undefined,
      actionId: isSet(object.actionId) ? globalThis.String(object.actionId) : "",
      toolInvocationId: isSet(object.toolInvocationId) ? globalThis.String(object.toolInvocationId) : "",
      correlatedInvocationsId: isSet(object.correlatedInvocationsId)
        ? globalThis.String(object.correlatedInvocationsId)
        : "",
      actionMnemonic: isSet(object.actionMnemonic) ? globalThis.String(object.actionMnemonic) : "",
      targetId: isSet(object.targetId) ? globalThis.String(object.targetId) : "",
      configurationId: isSet(object.configurationId) ? globalThis.String(object.configurationId) : "",
    };
  },

  toJSON(message: RequestMetadata): unknown {
    const obj: any = {};
    if (message.toolDetails !== undefined) {
      obj.toolDetails = ToolDetails.toJSON(message.toolDetails);
    }
    if (message.actionId !== "") {
      obj.actionId = message.actionId;
    }
    if (message.toolInvocationId !== "") {
      obj.toolInvocationId = message.toolInvocationId;
    }
    if (message.correlatedInvocationsId !== "") {
      obj.correlatedInvocationsId = message.correlatedInvocationsId;
    }
    if (message.actionMnemonic !== "") {
      obj.actionMnemonic = message.actionMnemonic;
    }
    if (message.targetId !== "") {
      obj.targetId = message.targetId;
    }
    if (message.configurationId !== "") {
      obj.configurationId = message.configurationId;
    }
    return obj;
  },

  create(base?: DeepPartial<RequestMetadata>): RequestMetadata {
    return RequestMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RequestMetadata>): RequestMetadata {
    const message = createBaseRequestMetadata();
    message.toolDetails = (object.toolDetails !== undefined && object.toolDetails !== null)
      ? ToolDetails.fromPartial(object.toolDetails)
      : undefined;
    message.actionId = object.actionId ?? "";
    message.toolInvocationId = object.toolInvocationId ?? "";
    message.correlatedInvocationsId = object.correlatedInvocationsId ?? "";
    message.actionMnemonic = object.actionMnemonic ?? "";
    message.targetId = object.targetId ?? "";
    message.configurationId = object.configurationId ?? "";
    return message;
  },
};

/**
 * The Remote Execution API is used to execute an
 * [Action][build.bazel.remote.execution.v2.Action] on the remote
 * workers.
 *
 * As with other services in the Remote Execution API, any call may return an
 * error with a [RetryInfo][google.rpc.RetryInfo] error detail providing
 * information about when the client should retry the request; clients SHOULD
 * respect the information provided.
 */
export type ExecutionDefinition = typeof ExecutionDefinition;
export const ExecutionDefinition = {
  name: "Execution",
  fullName: "build.bazel.remote.execution.v2.Execution",
  methods: {
    /**
     * Execute an action remotely.
     *
     * In order to execute an action, the client must first upload all of the
     * inputs, the
     * [Command][build.bazel.remote.execution.v2.Command] to run, and the
     * [Action][build.bazel.remote.execution.v2.Action] into the
     * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
     * It then calls `Execute` with an `action_digest` referring to them. The
     * server will run the action and eventually return the result.
     *
     * The input `Action`'s fields MUST meet the various canonicalization
     * requirements specified in the documentation for their types so that it has
     * the same digest as other logically equivalent `Action`s. The server MAY
     * enforce the requirements and return errors if a non-canonical input is
     * received. It MAY also proceed without verifying some or all of the
     * requirements, such as for performance reasons. If the server does not
     * verify the requirement, then it will treat the `Action` as distinct from
     * another logically equivalent action if they hash differently.
     *
     * Returns a stream of
     * [google.longrunning.Operation][google.longrunning.Operation] messages
     * describing the resulting execution, with eventual `response`
     * [ExecuteResponse][build.bazel.remote.execution.v2.ExecuteResponse]. The
     * `metadata` on the operation is of type
     * [ExecuteOperationMetadata][build.bazel.remote.execution.v2.ExecuteOperationMetadata].
     *
     * If the client remains connected after the first response is returned after
     * the server, then updates are streamed as if the client had called
     * [WaitExecution][build.bazel.remote.execution.v2.Execution.WaitExecution]
     * until the execution completes or the request reaches an error. The
     * operation can also be queried using [Operations
     * API][google.longrunning.Operations.GetOperation].
     *
     * The server NEED NOT implement other methods or functionality of the
     * Operations API.
     *
     * Errors discovered during creation of the `Operation` will be reported
     * as gRPC Status errors, while errors that occurred while running the
     * action will be reported in the `status` field of the `ExecuteResponse`. The
     * server MUST NOT set the `error` field of the `Operation` proto.
     * The possible errors include:
     *
     * * `INVALID_ARGUMENT`: One or more arguments are invalid.
     * * `FAILED_PRECONDITION`: One or more errors occurred in setting up the
     *   action requested, such as a missing input or command or no worker being
     *   available. The client may be able to fix the errors and retry.
     * * `RESOURCE_EXHAUSTED`: There is insufficient quota of some resource to run
     *   the action.
     * * `UNAVAILABLE`: Due to a transient condition, such as all workers being
     *   occupied (and the server does not support a queue), the action could not
     *   be started. The client should retry.
     * * `INTERNAL`: An internal error occurred in the execution engine or the
     *   worker.
     * * `DEADLINE_EXCEEDED`: The execution timed out.
     * * `CANCELLED`: The operation was cancelled by the client. This status is
     *   only possible if the server implements the Operations API CancelOperation
     *   method, and it was called for the current execution.
     *
     * In the case of a missing input or command, the server SHOULD additionally
     * send a [PreconditionFailure][google.rpc.PreconditionFailure] error detail
     * where, for each requested blob not present in the CAS, there is a
     * `Violation` with a `type` of `MISSING` and a `subject` of
     * `"blobs/{digest_function/}{hash}/{size}"` indicating the digest of the
     * missing blob. The `subject` is formatted the same way as the
     * `resource_name` provided to
     * [ByteStream.Read][google.bytestream.ByteStream.Read], with the leading
     * instance name omitted. `digest_function` MUST thus be omitted if its value
     * is one of MD5, MURMUR3, SHA1, SHA256, SHA384, SHA512, or VSO.
     *
     * The server does not need to guarantee that a call to this method leads to
     * at most one execution of the action. The server MAY execute the action
     * multiple times, potentially in parallel. These redundant executions MAY
     * continue to run, even if the operation is completed.
     */
    execute: {
      name: "Execute",
      requestType: ExecuteRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: true,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              43,
              34,
              38,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              97,
              99,
              116,
              105,
              111,
              110,
              115,
              58,
              101,
              120,
              101,
              99,
              117,
              116,
              101,
              58,
              1,
              42,
            ]),
          ],
        },
      },
    },
    /**
     * Wait for an execution operation to complete. When the client initially
     * makes the request, the server immediately responds with the current status
     * of the execution. The server will leave the request stream open until the
     * operation completes, and then respond with the completed operation. The
     * server MAY choose to stream additional updates as execution progresses,
     * such as to provide an update as to the state of the execution.
     *
     * In addition to the cases describe for Execute, the WaitExecution method
     * may fail as follows:
     *
     * * `NOT_FOUND`: The operation no longer exists due to any of a transient
     *   condition, an unknown operation name, or if the server implements the
     *   Operations API DeleteOperation method and it was called for the current
     *   execution. The client should call `Execute` to retry.
     */
    waitExecution: {
      name: "WaitExecution",
      requestType: WaitExecutionRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: true,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              43,
              34,
              38,
              47,
              118,
              50,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              111,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              42,
              125,
              58,
              119,
              97,
              105,
              116,
              69,
              120,
              101,
              99,
              117,
              116,
              105,
              111,
              110,
              58,
              1,
              42,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface ExecutionServiceImplementation<CallContextExt = {}> {
  /**
   * Execute an action remotely.
   *
   * In order to execute an action, the client must first upload all of the
   * inputs, the
   * [Command][build.bazel.remote.execution.v2.Command] to run, and the
   * [Action][build.bazel.remote.execution.v2.Action] into the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
   * It then calls `Execute` with an `action_digest` referring to them. The
   * server will run the action and eventually return the result.
   *
   * The input `Action`'s fields MUST meet the various canonicalization
   * requirements specified in the documentation for their types so that it has
   * the same digest as other logically equivalent `Action`s. The server MAY
   * enforce the requirements and return errors if a non-canonical input is
   * received. It MAY also proceed without verifying some or all of the
   * requirements, such as for performance reasons. If the server does not
   * verify the requirement, then it will treat the `Action` as distinct from
   * another logically equivalent action if they hash differently.
   *
   * Returns a stream of
   * [google.longrunning.Operation][google.longrunning.Operation] messages
   * describing the resulting execution, with eventual `response`
   * [ExecuteResponse][build.bazel.remote.execution.v2.ExecuteResponse]. The
   * `metadata` on the operation is of type
   * [ExecuteOperationMetadata][build.bazel.remote.execution.v2.ExecuteOperationMetadata].
   *
   * If the client remains connected after the first response is returned after
   * the server, then updates are streamed as if the client had called
   * [WaitExecution][build.bazel.remote.execution.v2.Execution.WaitExecution]
   * until the execution completes or the request reaches an error. The
   * operation can also be queried using [Operations
   * API][google.longrunning.Operations.GetOperation].
   *
   * The server NEED NOT implement other methods or functionality of the
   * Operations API.
   *
   * Errors discovered during creation of the `Operation` will be reported
   * as gRPC Status errors, while errors that occurred while running the
   * action will be reported in the `status` field of the `ExecuteResponse`. The
   * server MUST NOT set the `error` field of the `Operation` proto.
   * The possible errors include:
   *
   * * `INVALID_ARGUMENT`: One or more arguments are invalid.
   * * `FAILED_PRECONDITION`: One or more errors occurred in setting up the
   *   action requested, such as a missing input or command or no worker being
   *   available. The client may be able to fix the errors and retry.
   * * `RESOURCE_EXHAUSTED`: There is insufficient quota of some resource to run
   *   the action.
   * * `UNAVAILABLE`: Due to a transient condition, such as all workers being
   *   occupied (and the server does not support a queue), the action could not
   *   be started. The client should retry.
   * * `INTERNAL`: An internal error occurred in the execution engine or the
   *   worker.
   * * `DEADLINE_EXCEEDED`: The execution timed out.
   * * `CANCELLED`: The operation was cancelled by the client. This status is
   *   only possible if the server implements the Operations API CancelOperation
   *   method, and it was called for the current execution.
   *
   * In the case of a missing input or command, the server SHOULD additionally
   * send a [PreconditionFailure][google.rpc.PreconditionFailure] error detail
   * where, for each requested blob not present in the CAS, there is a
   * `Violation` with a `type` of `MISSING` and a `subject` of
   * `"blobs/{digest_function/}{hash}/{size}"` indicating the digest of the
   * missing blob. The `subject` is formatted the same way as the
   * `resource_name` provided to
   * [ByteStream.Read][google.bytestream.ByteStream.Read], with the leading
   * instance name omitted. `digest_function` MUST thus be omitted if its value
   * is one of MD5, MURMUR3, SHA1, SHA256, SHA384, SHA512, or VSO.
   *
   * The server does not need to guarantee that a call to this method leads to
   * at most one execution of the action. The server MAY execute the action
   * multiple times, potentially in parallel. These redundant executions MAY
   * continue to run, even if the operation is completed.
   */
  execute(
    request: ExecuteRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<Operation>>;
  /**
   * Wait for an execution operation to complete. When the client initially
   * makes the request, the server immediately responds with the current status
   * of the execution. The server will leave the request stream open until the
   * operation completes, and then respond with the completed operation. The
   * server MAY choose to stream additional updates as execution progresses,
   * such as to provide an update as to the state of the execution.
   *
   * In addition to the cases describe for Execute, the WaitExecution method
   * may fail as follows:
   *
   * * `NOT_FOUND`: The operation no longer exists due to any of a transient
   *   condition, an unknown operation name, or if the server implements the
   *   Operations API DeleteOperation method and it was called for the current
   *   execution. The client should call `Execute` to retry.
   */
  waitExecution(
    request: WaitExecutionRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<Operation>>;
}

export interface ExecutionClient<CallOptionsExt = {}> {
  /**
   * Execute an action remotely.
   *
   * In order to execute an action, the client must first upload all of the
   * inputs, the
   * [Command][build.bazel.remote.execution.v2.Command] to run, and the
   * [Action][build.bazel.remote.execution.v2.Action] into the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage].
   * It then calls `Execute` with an `action_digest` referring to them. The
   * server will run the action and eventually return the result.
   *
   * The input `Action`'s fields MUST meet the various canonicalization
   * requirements specified in the documentation for their types so that it has
   * the same digest as other logically equivalent `Action`s. The server MAY
   * enforce the requirements and return errors if a non-canonical input is
   * received. It MAY also proceed without verifying some or all of the
   * requirements, such as for performance reasons. If the server does not
   * verify the requirement, then it will treat the `Action` as distinct from
   * another logically equivalent action if they hash differently.
   *
   * Returns a stream of
   * [google.longrunning.Operation][google.longrunning.Operation] messages
   * describing the resulting execution, with eventual `response`
   * [ExecuteResponse][build.bazel.remote.execution.v2.ExecuteResponse]. The
   * `metadata` on the operation is of type
   * [ExecuteOperationMetadata][build.bazel.remote.execution.v2.ExecuteOperationMetadata].
   *
   * If the client remains connected after the first response is returned after
   * the server, then updates are streamed as if the client had called
   * [WaitExecution][build.bazel.remote.execution.v2.Execution.WaitExecution]
   * until the execution completes or the request reaches an error. The
   * operation can also be queried using [Operations
   * API][google.longrunning.Operations.GetOperation].
   *
   * The server NEED NOT implement other methods or functionality of the
   * Operations API.
   *
   * Errors discovered during creation of the `Operation` will be reported
   * as gRPC Status errors, while errors that occurred while running the
   * action will be reported in the `status` field of the `ExecuteResponse`. The
   * server MUST NOT set the `error` field of the `Operation` proto.
   * The possible errors include:
   *
   * * `INVALID_ARGUMENT`: One or more arguments are invalid.
   * * `FAILED_PRECONDITION`: One or more errors occurred in setting up the
   *   action requested, such as a missing input or command or no worker being
   *   available. The client may be able to fix the errors and retry.
   * * `RESOURCE_EXHAUSTED`: There is insufficient quota of some resource to run
   *   the action.
   * * `UNAVAILABLE`: Due to a transient condition, such as all workers being
   *   occupied (and the server does not support a queue), the action could not
   *   be started. The client should retry.
   * * `INTERNAL`: An internal error occurred in the execution engine or the
   *   worker.
   * * `DEADLINE_EXCEEDED`: The execution timed out.
   * * `CANCELLED`: The operation was cancelled by the client. This status is
   *   only possible if the server implements the Operations API CancelOperation
   *   method, and it was called for the current execution.
   *
   * In the case of a missing input or command, the server SHOULD additionally
   * send a [PreconditionFailure][google.rpc.PreconditionFailure] error detail
   * where, for each requested blob not present in the CAS, there is a
   * `Violation` with a `type` of `MISSING` and a `subject` of
   * `"blobs/{digest_function/}{hash}/{size}"` indicating the digest of the
   * missing blob. The `subject` is formatted the same way as the
   * `resource_name` provided to
   * [ByteStream.Read][google.bytestream.ByteStream.Read], with the leading
   * instance name omitted. `digest_function` MUST thus be omitted if its value
   * is one of MD5, MURMUR3, SHA1, SHA256, SHA384, SHA512, or VSO.
   *
   * The server does not need to guarantee that a call to this method leads to
   * at most one execution of the action. The server MAY execute the action
   * multiple times, potentially in parallel. These redundant executions MAY
   * continue to run, even if the operation is completed.
   */
  execute(request: DeepPartial<ExecuteRequest>, options?: CallOptions & CallOptionsExt): AsyncIterable<Operation>;
  /**
   * Wait for an execution operation to complete. When the client initially
   * makes the request, the server immediately responds with the current status
   * of the execution. The server will leave the request stream open until the
   * operation completes, and then respond with the completed operation. The
   * server MAY choose to stream additional updates as execution progresses,
   * such as to provide an update as to the state of the execution.
   *
   * In addition to the cases describe for Execute, the WaitExecution method
   * may fail as follows:
   *
   * * `NOT_FOUND`: The operation no longer exists due to any of a transient
   *   condition, an unknown operation name, or if the server implements the
   *   Operations API DeleteOperation method and it was called for the current
   *   execution. The client should call `Execute` to retry.
   */
  waitExecution(
    request: DeepPartial<WaitExecutionRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<Operation>;
}

/**
 * The action cache API is used to query whether a given action has already been
 * performed and, if so, retrieve its result. Unlike the
 * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage],
 * which addresses blobs by their own content, the action cache addresses the
 * [ActionResult][build.bazel.remote.execution.v2.ActionResult] by a
 * digest of the encoded [Action][build.bazel.remote.execution.v2.Action]
 * which produced them.
 *
 * The lifetime of entries in the action cache is implementation-specific, but
 * the server SHOULD assume that more recently used entries are more likely to
 * be used again.
 *
 * As with other services in the Remote Execution API, any call may return an
 * error with a [RetryInfo][google.rpc.RetryInfo] error detail providing
 * information about when the client should retry the request; clients SHOULD
 * respect the information provided.
 */
export type ActionCacheDefinition = typeof ActionCacheDefinition;
export const ActionCacheDefinition = {
  name: "ActionCache",
  fullName: "build.bazel.remote.execution.v2.ActionCache",
  methods: {
    /**
     * Retrieve a cached execution result.
     *
     * Implementations SHOULD ensure that any blobs referenced from the
     * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
     * are available at the time of returning the
     * [ActionResult][build.bazel.remote.execution.v2.ActionResult] and will be
     * for some period of time afterwards. The lifetimes of the referenced blobs SHOULD be increased
     * if necessary and applicable.
     *
     * Errors:
     *
     * * `NOT_FOUND`: The requested `ActionResult` is not in the cache.
     */
    getActionResult: {
      name: "GetActionResult",
      requestType: GetActionResultRequest,
      requestStream: false,
      responseType: ActionResult,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              86,
              18,
              84,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              97,
              99,
              116,
              105,
              111,
              110,
              82,
              101,
              115,
              117,
              108,
              116,
              115,
              47,
              123,
              97,
              99,
              116,
              105,
              111,
              110,
              95,
              100,
              105,
              103,
              101,
              115,
              116,
              46,
              104,
              97,
              115,
              104,
              125,
              47,
              123,
              97,
              99,
              116,
              105,
              111,
              110,
              95,
              100,
              105,
              103,
              101,
              115,
              116,
              46,
              115,
              105,
              122,
              101,
              95,
              98,
              121,
              116,
              101,
              115,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Upload a new execution result.
     *
     * In order to allow the server to perform access control based on the type of
     * action, and to assist with client debugging, the client MUST first upload
     * the [Action][build.bazel.remote.execution.v2.Action] that produced the
     * result, along with its
     * [Command][build.bazel.remote.execution.v2.Command], into the
     * `ContentAddressableStorage`.
     *
     * Server implementations MAY modify the
     * `UpdateActionResultRequest.action_result` and return an equivalent value.
     *
     * Errors:
     *
     * * `INVALID_ARGUMENT`: One or more arguments are invalid.
     * * `FAILED_PRECONDITION`: One or more errors occurred in updating the
     *   action result, such as a missing command or action.
     * * `RESOURCE_EXHAUSTED`: There is insufficient storage space to add the
     *   entry to the cache.
     */
    updateActionResult: {
      name: "UpdateActionResult",
      requestType: UpdateActionResultRequest,
      requestStream: false,
      responseType: ActionResult,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              101,
              26,
              84,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              97,
              99,
              116,
              105,
              111,
              110,
              82,
              101,
              115,
              117,
              108,
              116,
              115,
              47,
              123,
              97,
              99,
              116,
              105,
              111,
              110,
              95,
              100,
              105,
              103,
              101,
              115,
              116,
              46,
              104,
              97,
              115,
              104,
              125,
              47,
              123,
              97,
              99,
              116,
              105,
              111,
              110,
              95,
              100,
              105,
              103,
              101,
              115,
              116,
              46,
              115,
              105,
              122,
              101,
              95,
              98,
              121,
              116,
              101,
              115,
              125,
              58,
              13,
              97,
              99,
              116,
              105,
              111,
              110,
              95,
              114,
              101,
              115,
              117,
              108,
              116,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface ActionCacheServiceImplementation<CallContextExt = {}> {
  /**
   * Retrieve a cached execution result.
   *
   * Implementations SHOULD ensure that any blobs referenced from the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
   * are available at the time of returning the
   * [ActionResult][build.bazel.remote.execution.v2.ActionResult] and will be
   * for some period of time afterwards. The lifetimes of the referenced blobs SHOULD be increased
   * if necessary and applicable.
   *
   * Errors:
   *
   * * `NOT_FOUND`: The requested `ActionResult` is not in the cache.
   */
  getActionResult(
    request: GetActionResultRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ActionResult>>;
  /**
   * Upload a new execution result.
   *
   * In order to allow the server to perform access control based on the type of
   * action, and to assist with client debugging, the client MUST first upload
   * the [Action][build.bazel.remote.execution.v2.Action] that produced the
   * result, along with its
   * [Command][build.bazel.remote.execution.v2.Command], into the
   * `ContentAddressableStorage`.
   *
   * Server implementations MAY modify the
   * `UpdateActionResultRequest.action_result` and return an equivalent value.
   *
   * Errors:
   *
   * * `INVALID_ARGUMENT`: One or more arguments are invalid.
   * * `FAILED_PRECONDITION`: One or more errors occurred in updating the
   *   action result, such as a missing command or action.
   * * `RESOURCE_EXHAUSTED`: There is insufficient storage space to add the
   *   entry to the cache.
   */
  updateActionResult(
    request: UpdateActionResultRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ActionResult>>;
}

export interface ActionCacheClient<CallOptionsExt = {}> {
  /**
   * Retrieve a cached execution result.
   *
   * Implementations SHOULD ensure that any blobs referenced from the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
   * are available at the time of returning the
   * [ActionResult][build.bazel.remote.execution.v2.ActionResult] and will be
   * for some period of time afterwards. The lifetimes of the referenced blobs SHOULD be increased
   * if necessary and applicable.
   *
   * Errors:
   *
   * * `NOT_FOUND`: The requested `ActionResult` is not in the cache.
   */
  getActionResult(
    request: DeepPartial<GetActionResultRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ActionResult>;
  /**
   * Upload a new execution result.
   *
   * In order to allow the server to perform access control based on the type of
   * action, and to assist with client debugging, the client MUST first upload
   * the [Action][build.bazel.remote.execution.v2.Action] that produced the
   * result, along with its
   * [Command][build.bazel.remote.execution.v2.Command], into the
   * `ContentAddressableStorage`.
   *
   * Server implementations MAY modify the
   * `UpdateActionResultRequest.action_result` and return an equivalent value.
   *
   * Errors:
   *
   * * `INVALID_ARGUMENT`: One or more arguments are invalid.
   * * `FAILED_PRECONDITION`: One or more errors occurred in updating the
   *   action result, such as a missing command or action.
   * * `RESOURCE_EXHAUSTED`: There is insufficient storage space to add the
   *   entry to the cache.
   */
  updateActionResult(
    request: DeepPartial<UpdateActionResultRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ActionResult>;
}

/**
 * The CAS (content-addressable storage) is used to store the inputs to and
 * outputs from the execution service. Each piece of content is addressed by the
 * digest of its binary data.
 *
 * Most of the binary data stored in the CAS is opaque to the execution engine,
 * and is only used as a communication medium. In order to build an
 * [Action][build.bazel.remote.execution.v2.Action],
 * however, the client will need to also upload the
 * [Command][build.bazel.remote.execution.v2.Command] and input root
 * [Directory][build.bazel.remote.execution.v2.Directory] for the Action.
 * The Command and Directory messages must be marshalled to wire format and then
 * uploaded under the hash as with any other piece of content. In practice, the
 * input root directory is likely to refer to other Directories in its
 * hierarchy, which must also each be uploaded on their own.
 *
 * For small file uploads the client should group them together and call
 * [BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs].
 *
 * For large uploads, the client must use the
 * [Write method][google.bytestream.ByteStream.Write] of the ByteStream API.
 *
 * For uncompressed data, The `WriteRequest.resource_name` is of the following form:
 * `{instance_name}/uploads/{uuid}/blobs/{digest_function/}{hash}/{size}{/optional_metadata}`
 *
 * Where:
 * * `instance_name` is an identifier used to distinguish between the various
 *   instances on the server. Syntax and semantics of this field are defined
 *   by the server; Clients must not make any assumptions about it (e.g.,
 *   whether it spans multiple path segments or not). If it is the empty path,
 *   the leading slash is omitted, so that  the `resource_name` becomes
 *   `uploads/{uuid}/blobs/{digest_function/}{hash}/{size}{/optional_metadata}`.
 *   To simplify parsing, a path segment cannot equal any of the following
 *   keywords: `blobs`, `uploads`, `actions`, `actionResults`, `operations`,
 *   `capabilities` or `compressed-blobs`.
 * * `uuid` is a version 4 UUID generated by the client, used to avoid
 *   collisions between concurrent uploads of the same data. Clients MAY
 *   reuse the same `uuid` for uploading different blobs.
 * * `digest_function` is a lowercase string form of a `DigestFunction.Value`
 *   enum, indicating which digest function was used to compute `hash`. If the
 *   digest function used is one of MD5, MURMUR3, SHA1, SHA256, SHA384, SHA512,
 *   or VSO, this component MUST be omitted. In that case the server SHOULD
 *   infer the digest function using the length of the `hash` and the digest
 *   functions announced in the server's capabilities.
 * * `hash` and `size` refer to the [Digest][build.bazel.remote.execution.v2.Digest]
 *   of the data being uploaded.
 * * `optional_metadata` is implementation specific data, which clients MAY omit.
 *   Servers MAY ignore this metadata.
 *
 * Data can alternatively be uploaded in compressed form, with the following
 * `WriteRequest.resource_name` form:
 * `{instance_name}/uploads/{uuid}/compressed-blobs/{compressor}/{digest_function/}{uncompressed_hash}/{uncompressed_size}{/optional_metadata}`
 *
 * Where:
 * * `instance_name`, `uuid`, `digest_function` and `optional_metadata` are
 *   defined as above.
 * * `compressor` is a lowercase string form of a `Compressor.Value` enum
 *   other than `identity`, which is supported by the server and advertised in
 *   [CacheCapabilities.supported_compressor][build.bazel.remote.execution.v2.CacheCapabilities.supported_compressor].
 * * `uncompressed_hash` and `uncompressed_size` refer to the
 *   [Digest][build.bazel.remote.execution.v2.Digest] of the data being
 *   uploaded, once uncompressed. Servers MUST verify that these match
 *   the uploaded data once uncompressed, and MUST return an
 *   `INVALID_ARGUMENT` error in the case of mismatch.
 *
 * Note that when writing compressed blobs, the `WriteRequest.write_offset` in
 * the initial request in a stream refers to the offset in the uncompressed form
 * of the blob. In subsequent requests, `WriteRequest.write_offset` MUST be the
 * sum of the first request's 'WriteRequest.write_offset' and the total size of
 * all the compressed data bundles in the previous requests.
 * Note that this mixes an uncompressed offset with a compressed byte length,
 * which is nonsensical, but it is done to fit the semantics of the existing
 * ByteStream protocol.
 *
 * Uploads of the same data MAY occur concurrently in any form, compressed or
 * uncompressed.
 *
 * Clients SHOULD NOT use gRPC-level compression for ByteStream API `Write`
 * calls of compressed blobs, since this would compress already-compressed data.
 *
 * When attempting an upload, if another client has already completed the upload
 * (which may occur in the middle of a single upload if another client uploads
 * the same blob concurrently), the request will terminate immediately without
 * error, and with a response whose `committed_size` is the value `-1` if this
 * is a compressed upload, or with the full size of the uploaded file if this is
 * an uncompressed upload (regardless of how much data was transmitted by the
 * client). If the client completes the upload but the
 * [Digest][build.bazel.remote.execution.v2.Digest] does not match, an
 * `INVALID_ARGUMENT` error will be returned. In either case, the client should
 * not attempt to retry the upload.
 *
 * Small downloads can be grouped and requested in a batch via
 * [BatchReadBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchReadBlobs].
 *
 * For large downloads, the client must use the
 * [Read method][google.bytestream.ByteStream.Read] of the ByteStream API.
 *
 * For uncompressed data, The `ReadRequest.resource_name` is of the following form:
 * `{instance_name}/blobs/{digest_function/}{hash}/{size}`
 * Where `instance_name`, `digest_function`, `hash` and `size` are defined as
 * for uploads.
 *
 * Data can alternatively be downloaded in compressed form, with the following
 * `ReadRequest.resource_name` form:
 * `{instance_name}/compressed-blobs/{compressor}/{digest_function/}{uncompressed_hash}/{uncompressed_size}`
 *
 * Where:
 * * `instance_name`, `compressor` and `digest_function` are defined as for
 *   uploads.
 * * `uncompressed_hash` and `uncompressed_size` refer to the
 *   [Digest][build.bazel.remote.execution.v2.Digest] of the data being
 *   downloaded, once uncompressed. Clients MUST verify that these match
 *   the downloaded data once uncompressed, and take appropriate steps in
 *   the case of failure such as retrying a limited number of times or
 *   surfacing an error to the user.
 *
 * When downloading compressed blobs:
 * * `ReadRequest.read_offset` refers to the offset in the uncompressed form
 *   of the blob.
 * * Servers MUST return `INVALID_ARGUMENT` if `ReadRequest.read_limit` is
 *   non-zero.
 * * Servers MAY use any compression level they choose, including different
 *   levels for different blobs (e.g. choosing a level designed for maximum
 *   speed for data known to be incompressible).
 * * Clients SHOULD NOT use gRPC-level compression, since this would compress
 *   already-compressed data.
 *
 * Servers MUST be able to provide data for all recently advertised blobs in
 * each of the compression formats that the server supports, as well as in
 * uncompressed form.
 *
 * Additionally, ByteStream requests MAY come with an additional plain text header
 * that indicates the `resource_name` of the blob being sent.  The header, if
 * present, MUST follow the following convention:
 * * name: `build.bazel.remote.execution.v2.resource-name`.
 * * contents: the plain text resource_name of the request message.
 * If set, the contents of the header MUST match the `resource_name` of the request
 * message.  Servers MAY use this header to assist in routing requests to the
 * appropriate backend.
 *
 * The lifetime of entries in the CAS is implementation specific, but it SHOULD
 * be long enough to allow for newly-added and recently looked-up entries to be
 * used in subsequent calls (e.g. to
 * [Execute][build.bazel.remote.execution.v2.Execution.Execute]).
 *
 * Servers MUST behave as though empty blobs are always available, even if they
 * have not been uploaded. Clients MAY optimize away the uploading or
 * downloading of empty blobs.
 *
 * As with other services in the Remote Execution API, any call may return an
 * error with a [RetryInfo][google.rpc.RetryInfo] error detail providing
 * information about when the client should retry the request; clients SHOULD
 * respect the information provided.
 */
export type ContentAddressableStorageDefinition = typeof ContentAddressableStorageDefinition;
export const ContentAddressableStorageDefinition = {
  name: "ContentAddressableStorage",
  fullName: "build.bazel.remote.execution.v2.ContentAddressableStorage",
  methods: {
    /**
     * Determine if blobs are present in the CAS.
     *
     * Clients can use this API before uploading blobs to determine which ones are
     * already present in the CAS and do not need to be uploaded again.
     *
     * Servers SHOULD increase the lifetimes of the referenced blobs if necessary and
     * applicable.
     *
     * There are no method-specific errors.
     */
    findMissingBlobs: {
      name: "FindMissingBlobs",
      requestType: FindMissingBlobsRequest,
      requestStream: false,
      responseType: FindMissingBlobsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              45,
              34,
              40,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              98,
              108,
              111,
              98,
              115,
              58,
              102,
              105,
              110,
              100,
              77,
              105,
              115,
              115,
              105,
              110,
              103,
              58,
              1,
              42,
            ]),
          ],
        },
      },
    },
    /**
     * Upload many blobs at once.
     *
     * The server may enforce a limit of the combined total size of blobs
     * to be uploaded using this API. This limit may be obtained using the
     * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
     * Requests exceeding the limit should either be split into smaller
     * chunks or uploaded using the
     * [ByteStream API][google.bytestream.ByteStream], as appropriate.
     *
     * This request is equivalent to calling a Bytestream `Write` request
     * on each individual blob, in parallel. The requests may succeed or fail
     * independently.
     *
     * Errors:
     *
     * * `INVALID_ARGUMENT`: The client attempted to upload more than the
     *   server supported limit.
     *
     * Individual requests may return the following errors, additionally:
     *
     * * `RESOURCE_EXHAUSTED`: There is insufficient disk quota to store the blob.
     * * `INVALID_ARGUMENT`: The
     * [Digest][build.bazel.remote.execution.v2.Digest] does not match the
     * provided data.
     */
    batchUpdateBlobs: {
      name: "BatchUpdateBlobs",
      requestType: BatchUpdateBlobsRequest,
      requestStream: false,
      responseType: BatchUpdateBlobsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              45,
              34,
              40,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              98,
              108,
              111,
              98,
              115,
              58,
              98,
              97,
              116,
              99,
              104,
              85,
              112,
              100,
              97,
              116,
              101,
              58,
              1,
              42,
            ]),
          ],
        },
      },
    },
    /**
     * Download many blobs at once.
     *
     * The server may enforce a limit of the combined total size of blobs
     * to be downloaded using this API. This limit may be obtained using the
     * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
     * Requests exceeding the limit should either be split into smaller
     * chunks or downloaded using the
     * [ByteStream API][google.bytestream.ByteStream], as appropriate.
     *
     * This request is equivalent to calling a Bytestream `Read` request
     * on each individual blob, in parallel. The requests may succeed or fail
     * independently.
     *
     * Errors:
     *
     * * `INVALID_ARGUMENT`: The client attempted to read more than the
     *   server supported limit.
     *
     * Every error on individual read will be returned in the corresponding digest
     * status.
     */
    batchReadBlobs: {
      name: "BatchReadBlobs",
      requestType: BatchReadBlobsRequest,
      requestStream: false,
      responseType: BatchReadBlobsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              43,
              34,
              38,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              98,
              108,
              111,
              98,
              115,
              58,
              98,
              97,
              116,
              99,
              104,
              82,
              101,
              97,
              100,
              58,
              1,
              42,
            ]),
          ],
        },
      },
    },
    /**
     * Fetch the entire directory tree rooted at a node.
     *
     * This request must be targeted at a
     * [Directory][build.bazel.remote.execution.v2.Directory] stored in the
     * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
     * (CAS). The server will enumerate the `Directory` tree recursively and
     * return every node descended from the root.
     *
     * The GetTreeRequest.page_token parameter can be used to skip ahead in
     * the stream (e.g. when retrying a partially completed and aborted request),
     * by setting it to a value taken from GetTreeResponse.next_page_token of the
     * last successfully processed GetTreeResponse).
     *
     * The exact traversal order is unspecified and, unless retrieving subsequent
     * pages from an earlier request, is not guaranteed to be stable across
     * multiple invocations of `GetTree`.
     *
     * If part of the tree is missing from the CAS, the server will return the
     * portion present and omit the rest.
     *
     * Errors:
     *
     * * `NOT_FOUND`: The requested tree root is not present in the CAS.
     */
    getTree: {
      name: "GetTree",
      requestType: GetTreeRequest,
      requestStream: false,
      responseType: GetTreeResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              82,
              18,
              80,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              98,
              108,
              111,
              98,
              115,
              47,
              123,
              114,
              111,
              111,
              116,
              95,
              100,
              105,
              103,
              101,
              115,
              116,
              46,
              104,
              97,
              115,
              104,
              125,
              47,
              123,
              114,
              111,
              111,
              116,
              95,
              100,
              105,
              103,
              101,
              115,
              116,
              46,
              115,
              105,
              122,
              101,
              95,
              98,
              121,
              116,
              101,
              115,
              125,
              58,
              103,
              101,
              116,
              84,
              114,
              101,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface ContentAddressableStorageServiceImplementation<CallContextExt = {}> {
  /**
   * Determine if blobs are present in the CAS.
   *
   * Clients can use this API before uploading blobs to determine which ones are
   * already present in the CAS and do not need to be uploaded again.
   *
   * Servers SHOULD increase the lifetimes of the referenced blobs if necessary and
   * applicable.
   *
   * There are no method-specific errors.
   */
  findMissingBlobs(
    request: FindMissingBlobsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<FindMissingBlobsResponse>>;
  /**
   * Upload many blobs at once.
   *
   * The server may enforce a limit of the combined total size of blobs
   * to be uploaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or uploaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   *
   * This request is equivalent to calling a Bytestream `Write` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   *
   * Errors:
   *
   * * `INVALID_ARGUMENT`: The client attempted to upload more than the
   *   server supported limit.
   *
   * Individual requests may return the following errors, additionally:
   *
   * * `RESOURCE_EXHAUSTED`: There is insufficient disk quota to store the blob.
   * * `INVALID_ARGUMENT`: The
   * [Digest][build.bazel.remote.execution.v2.Digest] does not match the
   * provided data.
   */
  batchUpdateBlobs(
    request: BatchUpdateBlobsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchUpdateBlobsResponse>>;
  /**
   * Download many blobs at once.
   *
   * The server may enforce a limit of the combined total size of blobs
   * to be downloaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or downloaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   *
   * This request is equivalent to calling a Bytestream `Read` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   *
   * Errors:
   *
   * * `INVALID_ARGUMENT`: The client attempted to read more than the
   *   server supported limit.
   *
   * Every error on individual read will be returned in the corresponding digest
   * status.
   */
  batchReadBlobs(
    request: BatchReadBlobsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchReadBlobsResponse>>;
  /**
   * Fetch the entire directory tree rooted at a node.
   *
   * This request must be targeted at a
   * [Directory][build.bazel.remote.execution.v2.Directory] stored in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
   * (CAS). The server will enumerate the `Directory` tree recursively and
   * return every node descended from the root.
   *
   * The GetTreeRequest.page_token parameter can be used to skip ahead in
   * the stream (e.g. when retrying a partially completed and aborted request),
   * by setting it to a value taken from GetTreeResponse.next_page_token of the
   * last successfully processed GetTreeResponse).
   *
   * The exact traversal order is unspecified and, unless retrieving subsequent
   * pages from an earlier request, is not guaranteed to be stable across
   * multiple invocations of `GetTree`.
   *
   * If part of the tree is missing from the CAS, the server will return the
   * portion present and omit the rest.
   *
   * Errors:
   *
   * * `NOT_FOUND`: The requested tree root is not present in the CAS.
   */
  getTree(
    request: GetTreeRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<GetTreeResponse>>;
}

export interface ContentAddressableStorageClient<CallOptionsExt = {}> {
  /**
   * Determine if blobs are present in the CAS.
   *
   * Clients can use this API before uploading blobs to determine which ones are
   * already present in the CAS and do not need to be uploaded again.
   *
   * Servers SHOULD increase the lifetimes of the referenced blobs if necessary and
   * applicable.
   *
   * There are no method-specific errors.
   */
  findMissingBlobs(
    request: DeepPartial<FindMissingBlobsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<FindMissingBlobsResponse>;
  /**
   * Upload many blobs at once.
   *
   * The server may enforce a limit of the combined total size of blobs
   * to be uploaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or uploaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   *
   * This request is equivalent to calling a Bytestream `Write` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   *
   * Errors:
   *
   * * `INVALID_ARGUMENT`: The client attempted to upload more than the
   *   server supported limit.
   *
   * Individual requests may return the following errors, additionally:
   *
   * * `RESOURCE_EXHAUSTED`: There is insufficient disk quota to store the blob.
   * * `INVALID_ARGUMENT`: The
   * [Digest][build.bazel.remote.execution.v2.Digest] does not match the
   * provided data.
   */
  batchUpdateBlobs(
    request: DeepPartial<BatchUpdateBlobsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchUpdateBlobsResponse>;
  /**
   * Download many blobs at once.
   *
   * The server may enforce a limit of the combined total size of blobs
   * to be downloaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or downloaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   *
   * This request is equivalent to calling a Bytestream `Read` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   *
   * Errors:
   *
   * * `INVALID_ARGUMENT`: The client attempted to read more than the
   *   server supported limit.
   *
   * Every error on individual read will be returned in the corresponding digest
   * status.
   */
  batchReadBlobs(
    request: DeepPartial<BatchReadBlobsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchReadBlobsResponse>;
  /**
   * Fetch the entire directory tree rooted at a node.
   *
   * This request must be targeted at a
   * [Directory][build.bazel.remote.execution.v2.Directory] stored in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
   * (CAS). The server will enumerate the `Directory` tree recursively and
   * return every node descended from the root.
   *
   * The GetTreeRequest.page_token parameter can be used to skip ahead in
   * the stream (e.g. when retrying a partially completed and aborted request),
   * by setting it to a value taken from GetTreeResponse.next_page_token of the
   * last successfully processed GetTreeResponse).
   *
   * The exact traversal order is unspecified and, unless retrieving subsequent
   * pages from an earlier request, is not guaranteed to be stable across
   * multiple invocations of `GetTree`.
   *
   * If part of the tree is missing from the CAS, the server will return the
   * portion present and omit the rest.
   *
   * Errors:
   *
   * * `NOT_FOUND`: The requested tree root is not present in the CAS.
   */
  getTree(request: DeepPartial<GetTreeRequest>, options?: CallOptions & CallOptionsExt): AsyncIterable<GetTreeResponse>;
}

/**
 * The Capabilities service may be used by remote execution clients to query
 * various server properties, in order to self-configure or return meaningful
 * error messages.
 *
 * The query may include a particular `instance_name`, in which case the values
 * returned will pertain to that instance.
 */
export type CapabilitiesDefinition = typeof CapabilitiesDefinition;
export const CapabilitiesDefinition = {
  name: "Capabilities",
  fullName: "build.bazel.remote.execution.v2.Capabilities",
  methods: {
    /**
     * GetCapabilities returns the server capabilities configuration of the
     * remote endpoint.
     * Only the capabilities of the services supported by the endpoint will
     * be returned:
     * * Execution + CAS + Action Cache endpoints should return both
     *   CacheCapabilities and ExecutionCapabilities.
     * * Execution only endpoints should return ExecutionCapabilities.
     * * CAS + Action Cache only endpoints should return CacheCapabilities.
     *
     * There are no method-specific errors.
     */
    getCapabilities: {
      name: "GetCapabilities",
      requestType: GetCapabilitiesRequest,
      requestStream: false,
      responseType: ServerCapabilities,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            new Uint8Array([
              37,
              18,
              35,
              47,
              118,
              50,
              47,
              123,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              95,
              110,
              97,
              109,
              101,
              61,
              42,
              42,
              125,
              47,
              99,
              97,
              112,
              97,
              98,
              105,
              108,
              105,
              116,
              105,
              101,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface CapabilitiesServiceImplementation<CallContextExt = {}> {
  /**
   * GetCapabilities returns the server capabilities configuration of the
   * remote endpoint.
   * Only the capabilities of the services supported by the endpoint will
   * be returned:
   * * Execution + CAS + Action Cache endpoints should return both
   *   CacheCapabilities and ExecutionCapabilities.
   * * Execution only endpoints should return ExecutionCapabilities.
   * * CAS + Action Cache only endpoints should return CacheCapabilities.
   *
   * There are no method-specific errors.
   */
  getCapabilities(
    request: GetCapabilitiesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ServerCapabilities>>;
}

export interface CapabilitiesClient<CallOptionsExt = {}> {
  /**
   * GetCapabilities returns the server capabilities configuration of the
   * remote endpoint.
   * Only the capabilities of the services supported by the endpoint will
   * be returned:
   * * Execution + CAS + Action Cache endpoints should return both
   *   CacheCapabilities and ExecutionCapabilities.
   * * Execution only endpoints should return ExecutionCapabilities.
   * * CAS + Action Cache only endpoints should return CacheCapabilities.
   *
   * There are no method-specific errors.
   */
  getCapabilities(
    request: DeepPartial<GetCapabilitiesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ServerCapabilities>;
}

function bytesFromBase64(b64: string): Uint8Array {
  const bin = globalThis.atob(b64);
  const arr = new Uint8Array(bin.length);
  for (let i = 0; i < bin.length; ++i) {
    arr[i] = bin.charCodeAt(i);
  }
  return arr;
}

function base64FromBytes(arr: Uint8Array): string {
  const bin: string[] = [];
  arr.forEach((byte) => {
    bin.push(globalThis.String.fromCharCode(byte));
  });
  return globalThis.btoa(bin.join(""));
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = Math.trunc(date.getTime() / 1_000).toString();
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (globalThis.Number(t.seconds) || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
